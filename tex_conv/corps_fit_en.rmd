


Let us reconsider the example of the biomass model with a single entry (here dbh) of the power type:
\begin{equation}
B=aD^b(\#eq:p)
\end{equation}
We have already seen that this is a non-linear model as $B$ is non linearly dependent upon coefficients $a$ and $b$. But this model can be rendered linear by log transformation. Relation (\ref{p}) is equivalent to: $\ln(B)=\ln(a)+b\ln(D)$, which can be considered to be a linear regression of response variable $Y=\ln(B)$ against effect variable $X=\ln(D)$. We can therefore estimate coefficients $a$ and $b$ (or rather $\ln(a)$ and $b$) in power model (\ref{p}) by linear regression on log-transformed data. What about the residual error? If the linear regression on log-transformed data is appropriate, this means that $\varepsilon=\ln(B)-\ln(a)-b\ln(D)$ follows a centered normal distribution of constant standard deviation $\sigma$. If we return to the starting data and use exponential transformation (which is the inverse transformation to log transformation), the residual error here is a factor:
\[
B=aD^b\times\varepsilon'
\]
where $\varepsilon'=\exp(\varepsilon)$. Thus, we have moved from an additive error on log-transformed data to a multiplicative error on the starting data. Also, if $\varepsilon$ follows a centered normal distribution of standard deviation $\sigma$, then, by definition, $\varepsilon'=\exp(\varepsilon)$ follows a log-normal distribution of parameters 0 and $\sigma$:
\[
\varepsilon'\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{LN}(0,\ \sigma)
\]
Contrary to $\varepsilon$, which has a mean of zero, the mean of $\varepsilon'$ is not zero but:
$\mathrm{E}(\varepsilon')=\exp(\sigma^2/2)$. The implications of this will be considered in chapter \@ref(util).

We can draw two lessons from this example:
\begin{enumerate}
\item when we are faced by a non-linear relation between a response variable and one (or several) effect variables, a transformation may render this relation linear;
\item this transformation of the variable affects not only the form of the relation between the effect variable(s) and the response variable, but also the residual error.
\end{enumerate}
Concerning the first point, this variables transformation means that we have two approaches for fitting a non-linear model. The first, when faced with a non-linear relation between a response variable and effect variables, consists in looking for a transformation that renders this relation linear, and thereafter using the approach employed for the linear model. The second consists in fitting the non-linear model directly, as we shall see in section \ref{nlm}. Each approach has its advantages and its drawbacks. The linear model has the advantage of providing a relatively simple theoretical framework and, above all, the estimations of its coefficients have explicit expressions. The drawback is that the model linearization step introduces an additional difficulty, and the inverse transformation, if we are not careful, may produce prediction bias (we will return to this in chapter \@ref(util)). Also, not all models can be rendered linear. For example, no variables transformation can render the following model linear: $Y=a_0+a_1X+a_2\exp(a_3X)$.

Regarding the second point, we are now, therefore, obliged to distinguish the form of the relation between the response variable and the effect variables (we also speak of the mean model, i.e. the mean of the response variable $Y$), and the form of the model for the residual error (we also speak of the variance model, i.e. the variance of $Y$). This transformation of the variable affects both simultaneously. The art of transforming variables therefore lies in tackling the two simultaneously and thereby rendering the model linear with regard to its coefficients and stabilizing the variance of the residuals (i.e. rendering it constant).

#### Common variable transformations

Although theoretically there is no limit as to the variable transformations we can use, the transformations likely to concern volumes and biomasses are few in number. That most commonly employed to fit models is the log transformation. Given a power model:
\[
Y=aX_1^{b_1}X_2^{b_2}\times\ldots\times X_p^{b_p}\times\varepsilon
\]
the log transformation consists in replacing the variable $Y$ by its log: $Y'=\ln(Y)$, and each of the effect variables by their log: $X_j'=\ln(X_j)$. The resulting model corresponds to:
\begin{equation}
Y'=a'+b_1X_1'+b_2X_2'+\ldots+b_pX_p'+\varepsilon'(\#eq:rm2)
\end{equation}
where $\varepsilon'=\ln(\varepsilon)$. The inverse transformation is the exponential for all the effect and response variables. In terms of residual error, the log transformation is appropriate if $\varepsilon'$ follows a normal distribution, therefore if the error $\varepsilon$ is positive and multiplicative. It should be noted that log transformation poses a problem for variables that may take a zero value. In this case, the transformation $X'=\ln(X+1)$
is used rather than $X'=\ln(X)$ (or more generally,
$X'=\ln(X+\mathrm{constant})$ if $X$ can take a negative value, e.g. a dbh increment). By way of examples, the following biomass models:

\begin{eqnarray*}
B &=& aD^b\\ %
B &=& a(D^2H)^b\\ %
B &=& a\rho^{b_1}D^{b_2}H^{b_3}
\end{eqnarray*}
may be fitted by linear regression after log transformation of the data.

Given an exponential model:

\begin{equation}
Y=a\exp(b_1X_1+b_2X_2+\ldots+b_pX_p)\times\varepsilon(\#eq:expo)
\end{equation}
the appropriate transformation consists in replacing variable $Y$
by its log: $Y'=\ln(Y)$, and not transforming the effect variables: $X_j'=X_j$. 
The resulting model is identical to (\ref{rm2}). 
The inverse transformation is the exponential for the response variable and no change for the effect variables. In terms of residual error, this transformation is appropriate if $\varepsilon'$ follows a normal distribution, therefore if the error $\varepsilon$ is positive and multiplicative. It should be noted that, without loss of generality, we can reparameterize the coefficients of the exponential model (\ref{expo}) by applying $b'_j=\exp(b_j)$. Strictly identical writing of exponential model (\ref{expo}) therefore yields:
\[
Y=a{b'_1}^{X_1}{b'_2}^{X_2}\times\ldots\times{b'_p}^{X_p}\times\varepsilon
\]
For example, the following biomass model:
\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3\}
\]
may be fitted by linear regression after this type of variable transformation (with, in this example, $X_j=[\ln(D)]^j$).

The Box-Cox transformation generalizes the log transformation. It is in fact a family of transformations indexed by a parameter $\xi$. Given a variable $X$, its Box-Cox transform $X'_{\xi}$ corresponds to:
\[
X'_{\xi}=\left\{
\begin{array}{lcl}
(X^{\xi}-1)/\xi && (\xi\neq0)\\ %
\ln(X)=\lim_{\xi\rightarrow0}(X^{\xi}-1)/\xi && (\xi=0)
\end{array}
\right.
\]
The Box-Cox transformation can be used to convert the question of choosing a variable transformation into a question of estimating parameter $\xi$ [@hoeting99].

\subsubsection{A special variable transformation\label{apart}}

The usual variable transformations change the form of the relation between the response variable and the effect variable. When the cluster of points $(X_i,\ Y_i)$ formed by plotting the response variable against the effect variable is a straight line with heteroscedasticity, as shown in  Figure \@ref(fig:linh), then variable transformation needs to be employed to stabilize the variance of $Y$, though without affecting the linear nature of the relation between $X$ and $Y$. The particular case illustrated by \ref{linh} occurs fairly often when an allometric equation is fitted between two values that vary in a proportional manner [see for example @ngomanda12]. The linear nature of the relation between X and Y means that the model has the form:
\begin{equation}
Y=a+bX+\varepsilon(\#eq:rl2)
\end{equation}
but the heteroscedasticity indicates that the variance of$\varepsilon$ is not constant, and this prevents any fitting of a linear regression. A variable transformation in this case consists in replacing $Y$ by $Y'=Y/X$ and $X$ by $X'=1/X$. By dividing each member of (\ref{rl2}) by $X$, the post-variable transformation model becomes:
\begin{equation}
Y'=aX'+b+\varepsilon'(\#eq:rl3)
\end{equation}
where $\varepsilon'=\varepsilon/X$. The transformed model still corresponds to a linear relation, except that the y-intercept $a$ of the relation between $X$ and $Y$ has become the slope of the relation between $X'$ and $Y'$, and reciprocally, the slope $b$ of the relation between $X$ and $Y$ has become the y-intercept of the relation between $X'$ and $Y'$. Model (\ref{rl3}) may be fitted by simple linear regression if the variance of $\varepsilon'$ is constant. As 
$\mathrm{Var}(\varepsilon')=\sigma^2$ means
$\mathrm{Var}(\varepsilon)=\sigma^2X^2$, the variable transformation is appropriate if the standard deviation of
$\varepsilon$ is proportional to $X$.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.5\textwidth]{transfo}
\end{center}
\caption{Linear relation between an effect variable ($X$) and a response variable ($Y$), with an increase in the variability of $Y$ with an increase in $X$ (heteroscedasticity).\label{linh}}
\end{figure}

As model (\ref{rl3}) was fitted by simple linear regression, its sum of squares corresponds to:
\[
\mathrm{SSE}(a,\ b)=\sum_{i=1}^n(Y'_i-aX'_i-b)^2
=\sum_{i=1}^n(Y_i/X_i-a/X_i-b)^2=\sum_{i=1}^nX_i^{-2}(Y_i-a-bX_i)^2
\]
Here we recognize the expression for the sum of squares for a weighted regression using the weight $w_i=X_i^{-2}$.
Thus, the variable transformation $Y'=Y/X$ and $X'=1/X$ is strictly identical to a weighted regression of
weight $w=1/X^2$.

\begin{filrouge}{Linear regression between $B/D^2$ and $H$}{fH}%
We saw in red line \ref{frlpD2H} that a double-entry biomass model using dbh and height corresponds to: $B=a+bD^2H+\varepsilon$ where $\mathrm{Var}(\varepsilon)\propto
D^4$. By dividing each member of the equation by $D^2$, we obtain:
\[
B/D^2=a/D^2+bH+\varepsilon'
\]
where
\[
\mathrm{Var}(\varepsilon')=\sigma^2
\]
Thus, the regression of the response variable $Y=B/D^2$ against the two effect variables $X_1=1/D^2$ and $X_2=H$ in principle satisfies the hypotheses of multiple linear regression. This regression is fitted by the ordinary least squares method. Fitting of this multiple regression by the command:

\R{summary(lm((Btot/dbh\^{}2)$\sim$-1+I(1/dbh\^{}2)+heig,data=dat))}%
yields:

\Rout{\begin{tabular}{lrrrrl}%
              &  Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
I(1/dbh\^{}2) & 1.181e-03 &  2.288e-03 &   0.516 &    0.608 &    \\ %
heig          & 2.742e-05 &  1.527e-06 &  17.957 &   <2e-16 & ***\\ %
\end{tabular}}%
where it can be seen that the coefficient associated with $X_1=1/D^2$ is not significantly different from zero. If we now return to the starting data, this means simply that the y-intercept $a$ is not significantly different from zero, something we had already noted in red line \ref{frlpD2H}. Therefore, $X_1$ may be withdrawn and a simple linear regression may be fitted of $Y=B/D^2$ against $X_2=H$:

\R{with(dat,plot(heig,Btot/dbh\^{}2,xlab="Height (m)",ylab="Biomass/square of dbh\bidouille{\newline} (t/cm2)"))%
\\ m <- lm((Btot/dbh\^{}2)$\sim$-1+heig,data=dat)%
\\ summary(m)%
\\ plot(m,which=1:2)}%
The scatter plot of $B/D^2$ against $H$ is indeed a straight line with a variance of $B/D^2$ that is approximately constant ( Figure \@ref(fig:fBD2vH)). Fitting the simple linear regression yields:
\Rout{\begin{tabular}{lrrrrl}%
     &  Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
heig & 2.747e-05 &  1.511e-06 &   18.19 &   <2e-16 & ***\\ %
\end{tabular}}%
with $R^2$ = \decimal{0}{8897} and a residual standard deviation of \decimal{0}{0003513}~tonnes~cm$^{-2}$. The model is written: $B/D^2=\decimal{2}{747}\times10^{-5}H$, or if we return to the starting variables: $B=\decimal{2}{747}\times10^{-5}D^2H$. We now need to check that this model is strictly identical to the weighted regression of $B$ against $D^2H$ shown in red line \ref{frlpD2H} with weighting proportional to $1/D^4$. The plot of the residuals against the fitted values and the quantile-quantile plot of the residuals are shown in  Figure \@ref(fig:fHres).
\end{filrouge}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{fBD2vH}
\end{center}
\caption{Scatter plot of biomass divided by the square of the dbh (tonnes~cm$^{-2}$) against height (m) for the 42 trees measured in Ghana by @henry10.\label{fBD2vH}}
\end{figure}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fHres}
\caption[Residuals plotted against fitted values and quantile--quantile plot of the residuals of the simple linear regression of $B/D^2$ against $H$ fitted for the 42 trees measured by \textbf{} in Ghana]{Residuals plotted against fitted values (left) and quantile--quantile plot (right) of the residuals of the simple linear regression of $B/D^2$ against $H$ fitted for the 42 trees measured by \textbf{} in Ghana.\label{fHres}}
\end{figure}

\begin{filrouge}{Linear regression between $B/D^2$ and $1/D$}{finvD}%
We saw in red line \ref{fDpara} that a polynomial biomass model against dbh corresponded to: $B=a_0+a_1D+a_2D^2+\varepsilon$ where $\mathrm{Var}(\varepsilon)\propto
D^4$. By dividing each member of the equation by $D^2$, we obtain:
\[
B/D^2=a_0/D^2+a_1/D+a_2+\varepsilon'
\]
where
\[
\mathrm{Var}(\varepsilon')=\sigma^2
\]
Thus, the regression of the response variable $Y=B/D^2$ against the two effect variables $X_1=1/D^2$ and $X_2=1/D$ in principle satisfies the hypotheses of multiple linear regression. This regression is fitted by the ordinary least squares method. Fitting this multiple regression by the command:
\R{summary(lm((Btot/dbh\^{}2)$\sim$I(1/dbh\^{}2)+I(1/dbh),data=dat))}%
yields:
\Rout{\begin{tabular}{lrrrrl}%
              &  Estimate  & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)   &  1.215e-03 &  9.014e-05 &  13.478 & 2.93e-16 & ***\\ %
I(1/dbh\^{}2) &  1.127e-02 &  6.356e-03 &   1.772 &  0.08415 & .  \\ %
I(1/dbh)      & -7.297e-03 &  2.140e-03 &  -3.409 &  0.00153 & ** \\ %
\end{tabular}}%
where it can be seen that the coefficient associated with $X_1=1/D^2$ is not significantly different from zero. If we now return to the starting data, this means simply that the y-intercept $a_0$ is not significantly different from zero, something we had already noted in red line \ref{fDpara}. Therefore, $X_1$ may be withdrawn and a simple linear regression may be fitted of $Y=B/D^2$ against $X_2=1/D$:

\R{with(dat,plot(1/dbh,Btot/dbh\^{}2,xlab="1/Diameter (/cm)",ylab="Biomass/square
of \bidouille{\newline}dbh (t/cm2)"))%
\\ m <- lm((Btot/dbh\^{}2)$\sim$I(1/dbh),data=dat)%
\\ summary(m)%
\\ plot(m,which=1:2)}%
The scatter plot of $B/D^2$ against $1/D$ is approximately a straight line with a variance of $B/D^2$ that is approximately constant (Figure \@ref(fig:fBD2vD)). Fitting the simple linear regression yields:

\Rout{\begin{tabular}{lrrrrl}%
            &  Estimate  & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept) &  1.124e-03 & 7.599e-05  & 14.789  & < 2e-16  & ***\\ %
I(1/dbh)    & -3.840e-03 & 9.047e-04  & -4.245  & 0.000126 & ***\\ %
\end{tabular}}%
with $R^2=\decimal{0}{3106}$ and a residual standard deviation of \decimal{0}{0003985} tonnes~cm$^{-2}$. 
The model is written:
$B/D^2=\decimal{1}{124}\times10^{-3}-\decimal{3}{84}\times10^{-3}D^{-1}$,
or if we return to the starting variables:
$B=-\decimal{3}{84}\times10^{-3}D+\decimal{1}{124}\times10^{-3}D^2$.
We now need to check that this model is strictly identical to the polynomial regression of $B$ against $D$ shown in red line \ref{fDpara} with weighting proportional to $1/D^4$. The plot of the residuals against the fitted values and the quantile-quantile plot of the residuals are shown in  Figure \@ref(fig:finvDres).
\end{filrouge}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{fBD2vD}
\end{center}
\caption[Scatter plot of biomass divided by the square of the dbh against the inverse of the dbh for 42 trees measured in Ghana by @henry10]{Scatter plot of biomass divided by the square of the dbh (tonnes in~cm$^{-2}$) against the inverse of the dbh (cm$^{-1}$) for 42 trees measured in Ghana by @henry10.\label{fBD2vD}}
\end{figure}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{finvDres}
\caption[Residuals plotted against fitted values (left) and quantile--quantile plot (right) of the residuals of the simple linear regression of $B/D^2$ against $1/D$ fitted for the 42 trees measured by @henry10 in Ghana]{Residuals plotted against fitted values (left) and quantile--quantile plot (right) of the residuals of the simple linear regression of $B/D^2$ against $1/D$ fitted for the 42 trees measured by @henry10 in Ghana.\label{finvDres}}
\end{figure}

\section{Fitting a non-linear model\label{nlm}}

Let us now address the more general case of fitting a non-linear model. This model is written:
\[
Y=f(X_1,\ \ldots,\ X_p;\theta)+\varepsilon
\]
where $Y$ is the response variable, $X_1$, \ldots, $X_p$ are the effect variables, $\theta$ is the vector of all the model coefficients, $\varepsilon$ is the residual error, and $f$ is a function. If $f$ is linear in relation to the coefficients $\theta$, this brings us back to the previously studied linear model. We will henceforth no longer make any a priori hypotheses concerning the linearity of $f$ in relation to coefficients $\theta$. As previously, we assume that the residuals are independent and that they follow a centered normal distribution. By contrast, we do not make any a priori hypothesis concerning their variance. $\mathrm{E}(\varepsilon)=0$ means that $\mathrm{E}(Y)=f(X_1,\ \ldots,\ X_p;\theta)$. This is why we can say that $f$ defines the mean model (i.e. for $Y$). Let us write:
\[
\mathrm{Var}(\varepsilon)=g(X_1,\ \ldots,\ X_p;\vartheta)
\]
where $g$ is a function and $\vartheta$ a set of parameters.
As $\mathrm{Var}(Y)=\mathrm{Var}(\varepsilon)$, we can say that $g$ defines the variance model. Function $g$ may take various forms, but for biomass or volume data it is generally a power function of a variable that characterizes tree size (typically dbh). Without loss of generality, we can put forward that this effect variable is $X_1$, and therefore:
\[
g(X_1,\ \ldots,\ X_p;\vartheta)\equiv(kX_1^c)^2
\]
where $\vartheta\equiv(k,\ c)$, $k>0$ and $c\geq0$.

Interpreting the results of fitting a non-linear model is fundamentally the same as for the linear model. The difference between the linear model and the non-linear model, in addition to their properties, lies in the manner by which model coefficients are estimated. Two particular approaches are used: (*i*) exponent $c$ is fixed in advance; (*ii*) exponent $c$ is a parameter to be estimated in the same manner as the model's other parameters.

### Exponent known

Let us first consider the case where the exponent $c$ of the variance model is known in advance. Here, the least squares method can again be used to fit the model. The weighted sum of squares corresponds to:
\[
\mathrm{SSE}(\theta)=\sum_{i=1}^nw_i\ \varepsilon_i^2
=\sum_{i=1}^nw_i\ [Y_i-f(X_{i1},\ \ldots,\ X_{ip};\theta)]^2
\]
where the weights are inversely proportional to the variance of the residuals:
\[
w_i=\frac{1}{X_{i1}^{2c}}\propto\frac{1}{\mathrm{Var}(\varepsilon_i)}
\]
As previously, the estimator of the model's coefficients corresponds to the value of $\theta$ that minimizes the weighted sum of squares:
\[
\hat{\theta}=\arg\min_{\theta}\mathrm{SSE}(\theta)
=\arg\min_{\theta}\bigg\{\sum_{i=1}^n\frac{1}{X_{i1}^{2c}}
[Y_i-f(X_{i1},\ \ldots,\ X_{ip};\theta)]^2\bigg\}
\]
In the particular case where the residuals have a constant variance (i.e. $c=0$), the weighted least squares method is simplified to the ordinary least squares method (all weights $w_i$ are 1), but the principle behind the calculations remains the same. The estimator $\theta$ is obtained by resolving

\begin{equation}
\frac{\partial\mathrm{SSE}}{\partial\theta}(\hat{\theta})=0
(\#eq:der)
\end{equation}
with the constraint $(\partial^2\mathrm{SSE}/\partial\theta^2)>0$ to ensure that this is indeed a minimum, not a maximum. In the previous case of the linear model, resolving (\ref{der}) yielded an explicit expression for the estimator $\hat{\theta}$. This is not the case for the general case of the non-linear model: there is no explicit expression for $\hat{\theta}$. The sum of squares must therefore be minimized using a numerical algorithm. We will examine this point in depth in section \ref{algo}.

\subsubsection{*A priori* value for the exponent}

The *a priori* value of exponent $c$ is obtained in the non-linear case in the same manner as for the linear case (see page \pageref{chx}): by trial and error, by dividing $X_1$ into classes and estimating the variance of $Y$ for each class, or by minimizing Furnival's index (see p.\pageref{furni}).

\begin{filrouge}{Weighted non-linear regression between $B$ and $D$}{fnlsD}%
The graphical exploration (red lines \ref{feBvD} and \ref{feln}) showed that the relation between biomass $B$ and dbh $D$ was of the power type, with the variance of the biomass increasing with dbh:
\[
B=aD^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]
We saw in red line \ref{frlpD2H} that the conditional standard deviation of the biomass derived from the dbh was proportional to the square of the dbh: $c=2$. We can therefore fit a non-linear regression by the weighted least squares method using a weighting that is inversely proportional to $D^4$:

\R{start <- coef(lm(log(Btot)$\sim$I(log(dbh)),data=dat[dat\$Btot>0,]))%
\\ start[1] <- exp(start[1])
\\ names(start) <- c("a","b")
\\ m <- nls(Btot$\sim$a*dbh\^{}b,data=dat,start=start,weights=1/dat\$dbh\^{}4)%
\\ summary(m)}%
The non-linear regression is fitted using the \texttt{nls} command which calls on the start values of the coefficients. These start values are contained in the \texttt{start} object and are calculated by re-transforming the coefficients of the linear regression on the log-transformed data. Fitting the non-linear regression by the weighted least squares method gives:
\Rout{\begin{tabular}{lrrrrl}%
  &  Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
a & 2.492e-04 &  7.893e-05 &   3.157 &  0.00303 & ** \\ %
b & 2.346e+00 &  7.373e-02 &  31.824 &  < 2e-16 & ***\\ %
\end{tabular}}%
with a residual standard deviation
$k=\decimal{0}{0003598}$~tonnes~cm$^{-2}$. 
The model is therefore written: $B=\decimal{2}{492}\times10^{-4}D^{2.346}$. Let us now return to the linear regression fitted to log-transformed data (red line \ref{rllnBvD}) which was written: $\ln(B)=-\decimal{8}{42722}+\decimal{2}{36104}\ln(D)$.
If we return naively to the starting data by applying the exponential function (we will see in \S\ \ref{invtra} why this is naive), the model becomes: $B=\exp(-\decimal{8}{42722})\times
D^{2.36104}=\decimal{2}{188}\times10^{-4}D^{2.36104}$. The model fitted by non-linear regression and the model fitted by linear regression on log-transformed data are therefore very similar.
\end{filrouge}

\begin{filrouge}{Weighted non-linear regression between $B$ and $D^2H$}{fnlsD2H}%
We have already fitted a power model $B=a(D^2H)^b$ by simple linear regression on log-transformed data (red line \ref{rllnBvD2H}). Let us now fit this model directly by non-linear regression:
\[
B=a(D^2H)^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]
In order to take account of the heteroscedasticity, and considering that the conditional standard deviation of the biomass derived from the diameter is proportional to $D^2$ (red line \ref{frlpD2H}), we can fit this non-linear model by the weighted least squares method using a weighting inversely proportional to $D^4$:

\R{start <- coef(lm(log(Btot)$\sim$I(log(dbh\^{}2*heig)),data=dat[dat\$Btot>0,]))%
\\ start[1] <- exp(start[1])%
\\ names(start) <- c("a","b")%
\\ m <- nls(Btot$\sim$a*(dbh\^{}2*heig)\^{}b,data=dat,start=start,weights=1/dat\$dbh\^{}4)%
\\ summary(m)}%
As previously (red line \ref{fnlsD}), the \texttt{nls} command calls on start values for the coefficients and these are obtained from the coefficients of the multiple regression on log-transformed data. The result of the fitting is as follows:

\Rout{\begin{tabular}{lrrrrl}%
  &  Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
a & 7.885e-05 &  2.862e-05 &   2.755 &   0.0088 & ** \\ %
b & 9.154e-01 &  2.957e-02 &  30.953 &   <2e-16 & ***\\ %
\end{tabular}}%
with a residual standard deviation 
$k=\decimal{0}{0003325}$~tonnes~cm$^{-2}$. The model is therefore written:
$B=\decimal{7}{885}\times10^{-5}(D^2H)^{0.9154}$. Let us now return to the linear regression fitted to log-transformed data (red line \ref{rllnBvD2H}), which was written:
$\ln(B)=-\decimal{8}{99427}+\decimal{0}{87238}\ln(D^2H)$.
If we return naively to the starting data by applying the exponential function, this model becomes: 
$B=\exp(-\decimal{8}{99427})\times
D^{0.87238}=\decimal{1}{241}\times10^{-4}D^{0.87238}$. 
The model fitted by non-linear regression and the model fitted by linear regression on log-transformed data are therefore very similar.
\end{filrouge}

\begin{filrouge}{Weighted non-linear regression between $B$, $D$ and $H$}{fnlsDH}%
We have already fitted a power model $B=aD^{b_1}H^{b_2}$ by multiple linear regression on log-transformed data (red line \ref{flnDlnH}). Let us now fit this model directly by non-linear regression:
\[
B=aD^{b_1}H^{b_2}+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]
In order to take account of the heteroscedasticity, and considering that the conditional standard deviation of the biomass derived from dbh is proportional to $D^2$ (red line \ref{frlpD2H}), we can fit this non-linear model by the weighted least squares method using a weighting inversely proportional to $D^4$:
\R{start <- coef(lm(log(Btot)$\sim$I(log(dbh))+I(log(heig)),data=dat[dat\$Btot>0,]))%
\\ start[1] <- exp(start[1])%
\\ names(start) <- c("a","b1","b2")%
\\ m <- nls(Btot$\sim$a*dbh\^{}b1*heig\^{}b2,data=dat,start=start,weights=1/dat\$dbh\^{}4)%
\\ summary(m)}%
As previously (red line \ref{fnlsD}), the \texttt{nls} command calls on start values for the coefficients and these are obtained from the coefficients of the multiple regression on log-transformed data. The result of the fitting is as follows:
\Rout{\begin{tabular}{lrrrrl}%
   &  Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
a  & 1.003e-04 &  5.496e-05 &   1.824 &   0.0758 & .  \\ %
b1 & 1.923e+00 &  1.956e-01 &   9.833 & 4.12e-12 & ***\\ %
b2 & 7.435e-01 &  3.298e-01 &   2.254 &   0.0299 & *  \\ %
\end{tabular}}%
with a residual standard deviation $k=\decimal{0}{0003356}$~tonnes~cm$^{-2}$. The model is therefore written: $B=\decimal{1}{003}\times10^{-4}D^{1.923}H^{0.7435}$. The model is similar to that fitted by multiple regression on log-transformed data (red line \ref{flnDlnH}). But coefficient $a$ is estimated with less precision here than by the multiple regression on log-transformed data.
\end{filrouge}

### Estimating the exponent

Let us now consider the case where the exponent $c$ needs to be estimated at the same time as the model's other parameters. This case includes the linear regression with variance model that we mentioned in section \ref{lme}. In this case the least squares method is no longer valid. We are therefore obliged to use another fitting method: the maximum likelihood method. The likelihood of an observation ($X_{i1}$,
\ldots, $X_{ip}$, $Y_i$) is the probability density of observing ($X_{i1}$, \ldots, $X_{ip}$, $Y_i$) in the specified model. The probability density of a normal distribution of expectation $\mu$ and standard deviation $\sigma$ is:

\[
\phi(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{x-\mu}{\sigma}\bigg)^2\bigg]
\]
As $Y_i$ follows a normal distribution of expectation $f(X_{i1},\ \ldots,\ X_{ip};\theta)$ and standard deviation $kX_{i1}^c$, the likelihood of the $i$th observation is:

\[
\frac{1}{kX_{i1}^c\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\]
As the observations are independent, their joint likelihood is the product of the likelihoods of each observation. The likelihood of the sample of $n$ observations is therefore:
\begin{eqnarray}
\ell(\theta,\ k,\ c) &=& \prod_{i=1}^n
\frac{1}{kX_{i1}^c\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\label{vrais}
\\ &=& \frac{1}{(k\sqrt{2\pi})^n}\frac{1}{(\prod_{i=1}^nX_{i1})^c}
\exp\bigg[-\frac{1}{2}\sum_{i=1}^n\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\nonumber
\end{eqnarray}
This likelihood is considered to be a function of parameters $\theta$, $k$ and $c$.

The better the values of parameters $\theta$, $k$ and $c$ the higher the probability of the observations being obtained in the model corresponding to these parameter values. In other words, the best values for parameters $\theta$, $k$ and $c$ are those that maximize the likelihood of the observations. The corresponding estimator is by definition the maximum likelihood estimator, and is written:

\[
(\hat{\theta},\ \hat{k},\ \hat{c})=\arg\max_{(\theta,\ k,\ c)}\;
\ell(\theta,\ k,\ c)=\arg\max_{(\theta,\ k,\ c)}\;
\ln[\ell(\theta,\ k,\ c)]
\]
where the last equality stems from the fact that a function and its logarithm reach their maximum for the same values of their argument. The logarithm of the likelihood, which we call the log-likelihood and which we write $\mathcal{L}$, is easier to calculate than the likelihood, and therefore, for our calculations, it is the log-likelihood we will be seeking to maximize. In the present case, the log-likelihood is written:

\begin{eqnarray}
\mathcal{L}(\theta,\ k,\ c) &=&
\ln[\ell(\theta,\ k,\ c)]\label{ll}
\\ &=& -n\ln(k\sqrt{2\pi})-c\sum_{i=1}^n\ln(X_{i1})-\frac{1}{2}\sum_{i=1}^n\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\nonumber
\\ &=& -\frac{1}{2}\sum_{i=1}^n\bigg[\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2
+\ln(2\pi)+\ln(k^2X_i^{2c})\bigg]\nonumber
\end{eqnarray}
To obtain the maximum likelihood estimators of the parameters, we would need to calculate the partial derivatives of the log-likelihood with respect to these parameters, and look for the values where they cancel each other out (while ensuring that the second derivatives are indeed negative). In the general case, there is no analytical solution to this problem. In the same manner as previously for the sum of squares, we will need to use a numerical algorithm to maximize the log-likelihood.
We can show that the maximum likelihood method yields a coefficients estimator that is asymptotically (i.e. when the number $n$ of observations tends toward infinity) the best. We can also show that in the case of the linear model, the least squares estimator and the maximum likelihood estimator are the same.

\begin{filrouge}{Non-linear regression between $B$ and $D$ with variance model}{fnlmD}%

Let us look again at the non-linear regression between biomass and dbh (see red line \ref{fnlsD}), but this time consider that exponent $c$ in the variance model is a parameter that needs to be estimated like the others. The model is written in the same fashion as before (red line \ref{fnlsD}):

\[
B=aD^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
but is fitted by the maximum likelihood method:

\R{start <- coef(lm(log(Btot)$\sim$I(log(dbh)),data=dat[dat\$Btot>0,]))%
\\ start[1] <- exp(start[1])
\\ names(start) <- c("a","b")
\\ library(nlme)%
\\ m <- nlme(Btot$\sim$a*dbh\^{}b, data=cbind(dat,g="a"), fixed=a+b$\sim$1,
start=start, groups=$\sim$g, weights=varPower(form=$\sim$dbh))
\\ summary(m)}%
The model is fitted by the \texttt{nlme} command\footnote{The \texttt{nlme} command in fact serves to fit mixed effect non-linear models. The \texttt{nlreg} command is used to fit non-linear models with variance model, but we have obtained abnormal results with this command (version 3.1-96) which explains why we prefer to use the \texttt{nlme} command here, even though there is no mixed effect in the models considered here.} which, like the \texttt{nls} command (red line \ref{fnlsD}), requires \texttt{start} values for the coefficients. These start values are calculated as in red line \ref{fnlsD}. The result of the fitting is as follows:
\Rout{\begin{tabular}{lrrrrr}%
  &     Value &  Std.Error & DF &  t-value & p-value\\ %
a & 0.0002445 & 0.00007136 & 40 &  3.42568 &  0.0014\\ %
b & 2.3510500 & 0.06947401 & 40 & 33.84071 &  0.0000\\ %
\end{tabular}}%
with an estimated value of exponent $c=\decimal{2}{090814}$. This estimated value is very similar to that evaluated for the weighted non-linear regression ($c=2$, see in red line \ref{frlpD2H}). The fitted model is therefore: $B=\decimal{2}{445}\times10^{-4}D^{2.35105}$, which is very similar to the model fitted by weighted non-linear regression (red line \ref{fnlsD}).


\end{filrouge}

\begin{filrouge}{Non-linear regression between $B$ and $D^2H$ with variance model}{fnlmD2H}%
Let us look again at the non-linear regression between biomass and $D^2H$ (see red line \ref{fnlsD2H}), but this time consider that exponent $c$ in the variance model is a parameter that needs to be estimated like the others. The model is written in the same fashion as before (red line \ref{fnlsD2H}):
\[
B=a(D^2H)^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
but is fitted by the maximum likelihood method:
\R{start <- coef(lm(log(Btot)$\sim$I(log(dbh\^{}2*heig)),data=dat[dat\$Btot>0,]))%
\\ start[1] <- exp(start[1])
\\ names(start) <- c("a","b")
\\ library(nlme)%
\\ m <- nlme(Btot$\sim$a*(dbh\^{}2*heig)\^{}b,data=cbind(dat,g="a"),fixed=a+b$\sim$1,%
start=start,\bidouille{\newline}groups=$\sim$g,weights=varPower(form=$\sim$dbh))
\\ summary(m)}%
The model is fitted by the \texttt{nlme} command, which, like the \texttt{nls} command (red line \ref{fnlsD}), requires start values for the coefficients. These \texttt{start} values are calculated as in red line \ref{fnlsD}. The result of the fitting is as follows:

\Rout{\begin{tabular}{lrrrrr}%
  &     Value &   Std.Error & DF &  t-value & p-value\\ %
a & 0.0000819 & 0.000028528 & 40 &  2.87214 &  0.0065\\ %
b & 0.9122144 & 0.028627821 & 40 & 31.86461 &  0.0000\\ %
\end{tabular}}%
with an estimated value of exponent $c=\decimal{2}{042586}$. This estimated value is very similar to that evaluated for the weighted non-linear regression ($c=2$, see red line \ref{frlpD2H}). The fitted model is therefore: $B=\decimal{8}{19}\times10^{-5}(D^2H)^{0.9122144}$, which is very similar to the model fitted by weighted non-linear regression (red line \ref{fnlsD2H}).
\end{filrouge}

\begin{filrouge}{Non-linear regression between $B$, $D$ and $H$ with variance model}{fnlmDH}%
Let us look again at the non-linear regression between biomass, dbh and height (see red line \ref{fnlsDH}):
\[
B=aD^{b_1}H^{b_2}+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
but this time consider that exponent $c$ in the variance model is a parameter that needs to be estimated like the others. The fitting by maximum likelihood is as follows:
\R{library(nlme)%
\\ start <- coef(lm(log(Btot)$\sim$I(log(dbh))+I(log(heig)),data=dat[dat\$Btot>0,]))%
\\ start[1] <- exp(start[1])%
\\ names(start) <- c("a","b1","b2")%
\\ m <- nlme(Btot$\sim$a*dbh\^{}b1*heig\^{}b2,data=cbind(dat,g="a"),fixed=a+b1+b2$\sim$1,%
\bidouille{\newline}start=start,groups=$\sim$g,weights=varPower(form=$\sim$dbh))
\\ summary(m)}%
and, as previously, requires the provision of \texttt{start} values for the coefficients. The fitting yields:

\Rout{\begin{tabular}{lrrrrr}%
   &     Value & Std.Error & DF &  t-value & p-value\\ %
a  & 0.0001109 & 0.0000566 & 39 & 1.959869 &  0.0572\\ %
b1 & 1.9434876 & 0.1947994 & 39 & 9.976866 &  0.0000\\ %
b2 & 0.6926256 & 0.3211766 & 39 & 2.156526 &  0.0373\\ %
\end{tabular}}%
with an estimated value of exponent $c=\decimal{2}{055553}$. This estimated value is very similar to that evaluated for the weighted non-linear regression ($c=2$, see red line \ref{frlpD2H}). The fitted model is therefore: $B=\decimal{1}{109}\times10^{-4}D^{1.9434876}H^{0.6926256}$, which is very similar to the model fitted by weighted non-linear regression (red line \ref{fnlsDH}).
\end{filrouge}

\begin{filrouge}{Non-linear regression between $B$ and a polynomial of $\ln(D)$}{fexpDpol}%
Previously (red line \ref{flnDpol}), we used multiple regression to fit a model between $\ln(B)$ and a polynomial of $\ln(D)$. If we look again at the start variable, the model is written:
\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+\ldots+a_p[\ln(D)]^p\}+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
We will now directly fit this non-linear model by maximum likelihood (such that exponent $c$ is estimated at the same time as the model's other parameters). For a third-order polynomial, the fitting is obtained by:
\R{library(nlme)%
\\ start <- coef(lm(log(Btot)$\sim$I(log(dbh))+I(log(dbh)\^{}2)+I(log(dbh)\^{}3),%
data=dat[\bidouille{\newline}dat\$Btot>0,]))%
\\ start[1] <- exp(start[1])%
\\ names(start) <- paste("a",0:3,sep="")%
\\ m <- nlme(Btot$\sim$exp(a0+a1*log(dbh)+a2*log(dbh)\^{}2+a3*log(dbh)\^{}3),%
data=cbind(dat,\bidouille{\newline}g="a"),fixed=a0+a1+a2+a3$\sim$1,%
start=start,groups=$\sim$g,weights=varPower(form=$\sim$dbh))%
\\ summary(m)}%
and results in:
\Rout{\begin{tabular}{lrrrrr}%
   &     Value & Std.Error & DF &   t-value & p-value\\ %
a0 & -8.983801 & 2.2927006 & 38 & -3.918436 &  0.0004\\ %
a1 &  2.939020 & 2.1073819 & 38 &  1.394631 &  0.1712\\ %
a2 & -0.158585 & 0.6172529 & 38 & -0.256921 &  0.7986\\ %
a3 &  0.013461 & 0.0581339 & 38 &  0.231547 &  0.8181\\ %
\end{tabular}}%
with an estimated value of exponent $c=\decimal{2}{099938}$. This result is very similar to that obtained by multiple regression on log-transformed data (red line \ref{flnDpol}).
\end{filrouge}

\subsection{Numerical optimization\label{algo}}

If, in the case of a non-linear model, we are looking to minimize the sum of squares (when exponent $c$ is known) or maximize the log-likelihood (when exponent $c$ needs to be estimated) we need to use a numerical optimization algorithm. As maximizing the log-likelihood is equivalent to minimizing the opposite of the log-likelihood, we shall, in all that follows, consider only the problem of minimizing a function in a multidimensional space. A multitude of optimization algorithms have been developed \citep[chapitre 10]{press07} but the aim here is not to present a review of them all. What simply needs to be recognized at this stage is that these algorithms are iterative and require parameter start values. Based on this start value and each iteration, the algorithm shifts in the parameter space while looking to minimize the target function (i.e. the sum of squares or minus the log-likelihood). The target function may be represented as a hypersurface in the parameter space (Figure \@ref(fig:sp)). Each position in this space corresponds to a value of the parameters. A lump in this surface corresponds to a local maximum of the target function, whereas a dip in the surface corresponds to a local minimum. The aim is to determine the overall minimum, i.e. the deepest dip. The position of this dip corresponds to the estimated value of the parameters. If the algorithm gives the position of a dip that is not the deepest, the estimation of the parameters is false.

\begin{figure}[p]
\begin{center}
\textbf{A}\includegraphics[width=0.7\textwidth]{numopt1}
\\\textbf{B}\includegraphics[width=0.7\textwidth]{numopt2}
\end{center}
\caption[Representation of the target function]{Representation of the target function (*i.e.* the quantity to be minimized) as a surface in parameter space. Each position in this space corresponds to a value of the parameters. The successive values $\theta_1$, $\theta_2$, \ldots, for the parameters are obtained from a start value $\theta_0$ by descending the surface along the line with the steepest slope.(A) The surface has a single watershed. (B) The surface has several watersheds.\label{sp}}
\end{figure}

#### Descending algorithm

The simplest optimization algorithm consists in calculating successive positions, i.e. successive parameter values, by descending the surface defined by the target function along the line that has the steepest slope (Figure \@ref(fig:sp)A). This algorithm leads to one dip in the surface, but nothing says that this dip is the deepest for the surface may have several watersheds with several dips. Depending on the starting position, the algorithm will converge toward one dip or another (Figure \@ref(fig:sp)B). Also, even two starting positions very close together may be located on either side of a crest separating the two watersheds, and will therefore result in different dips, i.e. in different parameter estimations. The only case where this algorithm gives the correct parameter estimation regardless of starting value is when the surface has only a single dip, i.e. when the target function is convex. This in particular is the case for the linear model, but generally not for the non-linear model.

#### Improving algorithms in the case of local minima

More subtle algorithms have been developed than those descending along the steepest slope. For example, these may include the possibility to climb out of a dip into which the algorithm has temporarily converged in order to determine whether there might not be a deeper dip in the neighborhood. But no algorithm, even the most subtle, offers the certainty that it has converged to the deepest dip. Thus, any numerical optimization algorithm (*i*) may be trapped by a local minimum instead of converging to the overall minimum, and (*ii*) is sensitive to the indicated start position, which in part determines the final position toward which the algorithm will converge.

If we now return to the issue at hand, this means (*i*) fitting a non-linear model could yield erroneous parameter estimations, and (*ii*) selecting parameter start values for the algorithm is a sensitive issue. Herein lies the main drawback of fitting a non-linear model, and if it is to be avoided, the parameter start values must be carefully selected and, above all, several values must be tested.

#### Selecting parameter start values

When mean model $f$ can be transformed into a linear relation between the response variable $Y$ and the effect variables $X_1$, \ldots, $X_p$, a start value for the coefficients may be obtained by fitting a linear regression on the transformed variables without considering the possible heteroscedasticity of the residuals. Let us for example consider a power type biomass table:
\begin{equation}
B=aD^{b_1}H^{b_2}\rho^{b_3}+\varepsilon(\#eq:powea)
\end{equation}
where
\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\;kD^c)
\]
The power model for the expectation of $B$ can be rendered linear by log-transforming the variables: $\ln(B)=a'+b_1\ln(D)+b_2\ln(H)+b_3\ln(\rho)$. But this transformation is incompatible with the additivity of the errors in the model (\ref{powea}). In other words, the multiple regression of the response variable $\ln(B)$ against the effect variables $\ln(D)$, $\ln(H)$ and $\ln(\rho)$:
\begin{equation}
\ln(B)=a'+b_1\ln(D)+b_2\ln(H)+b_3\ln(\rho)+\varepsilon'(\#eq:poweb)
\end{equation}
where $\varepsilon'\sim\mathcal{N}(0,\ \sigma)$, is not a model equivalent to (\ref{powea}), even when the residuals $\varepsilon'$ of this model have a constant variance. Even though models (\ref{powea}) and (\ref{poweb}) are not mathematically equivalent, the coefficients of (\ref{poweb}) --- estimated by multiple regression --- may serve as start values for the numerical algorithm that estimates the coefficients of (\ref{powea}). If we write as $x^{(0)}$ the start value of parameter $x$ for the numerical optimization algorithm, we thus obtain:
\[
a^{(0)}=\exp(\hat{a}'),\quad b_i^{(0)}=\hat{b}_i,\quad
k^{(0)}=\hat{\sigma},\quad c^{(0)}=0
\]
Sometimes, the mean model cannot be rendered linear. An example of this is the following biomass table used for trees in a plantation [@saintandre05]:
\[
B=a+[b_0+b_1T+b_2\exp(-b_3T)]D^2H+\varepsilon
\]
where $T$ is the age of the plantation and $\varepsilon\sim\mathcal{N}(0,\ kD^c)$, which has a mean model that cannot be rendered linear. In this case, parameter start values must be selected empirically. For this current example we could for instance take:
\[
a^{(0)}=\hat{a},\quad b_0^{(0)}+b_2^{(0)}=\hat{b}_0,\quad
b_1^{(0)}=\hat{b}_1,\quad b_3^{(0)}=0,\quad
k^{(0)}=\hat{\sigma},\quad c^{(0)}=0
\]
where $\hat{a}$, $\hat{b}_0$, $\hat{b}_1$ and $\hat{\sigma}$ are estimated values for the coefficients and residual standard deviations of the multiple regression of $B$ against $D^2H$ and $D^2HT$.
Selecting parameter start values does not mean that several start values should not be tested. Therefore, when fitting a non-linear model with a numerical optimization algorithm, it is essential to test several parameter start values to ensure that the estimations are stable.

\section{Selecting variables and models\label{select}}

When we look to construct a volume or biomass table, the graphical exploration of the data (chapter \@ref(explo)) generally yields several possible model forms. We could fit all these potentially interesting models, but ultimately, which of all these fitted models should be recommended to the user? Selecting variables and selecting models aims to determine which is the "best" possible expression of the model among all those fitted.

### Selecting variables

Let us take the example of a biomass table we are looking to construct from a dataset that includes tree diameter (dbh) and height, and wood specific density. By working on log-transformed data, and given the variables they include, we may fit the following models:

\begin{eqnarray*}
\ln(B) &=& a_0+a_1\ln(D)+\varepsilon\\ %
\ln(B) &=& a_0+a_2\ln(H)+\varepsilon\\ %
\ln(B) &=& a_0+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_2\ln(H)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+a_3\ln(\rho)+\varepsilon
\end{eqnarray*}
The *complete* model (the last in the above list) is that which includes all the effect variables available. All the other models may be considered to be subsets of the complete model, but where certain effect variables have been employed and other set aside. Selecting the variables aims to choose --- from among the effect variables of the complete model --- those that should be kept and those that may be set aside as they contribute little to the prediction of the response variable. In other words, in this example, selecting the variables would consist in choosing the best model from among the seven models envisaged for $\ln(B)$.

Given that there are $p$ effect variables $X_1$, $X_2$, \ldots, $X_p$, there are $2^p-1$ models that include all or some of these effect variables. Selecting the variables consists in choosing the "best" combination of effect variables from all those available. This means firstly that we must have criterion that can be used to evaluate the quality of a model. We have already seen (p.\pageref{irm}) that $R^2$ is a poor criterion for evaluating the quality of one model in comparison with that of another as it increases automatically with the number of effect variables, and this regardless of whether these provide information useful for predicting the response variable or not. A better criterion for selecting effect variables is the residual variance estimator, which is linked to $R^2$ by the relation:
\[
\hat{\sigma}^2=\frac{n}{n-p-1}(1-R^2)\ S_Y^2
\]
where $S_Y^2$ is the empirical variance of the response variable.

Several methods may be used to select the best combination of effect variables. If $p$ is not too high, we can review all the $2^p-1$ possible models exhaustively. When $p$ is too high, a step by step method may be used to select the variables. Step-by-step methods proceed by the successive elimination or successive addition of effect variables. The descending method consists in eliminating the least significant of the $p$ variables. The regression is then recalculated and the process repeated until a stop criterion is satisfied (e.g. when all model coefficients are significantly different from zero). The ascending method proceeds in the opposite direction: we start with the best single-variable regression and add the variable that increases $R^2$ the most until the stop criterion is satisfied.

The so-called *stepwise* method is a further improvement upon the previous algorithm that consists in performing an additional Fisher's significance test at each step such as not to introduce a non-significant variable and possibly eliminate variables that have already been introduced but are no longer informative given the last variable selected. The algorithm stops when no more variables can be added or withdrawn. The different step-by-step selection methods do not necessarily give the same result, but the "stepwise" method would appear to be the best. They do not, however, safeguard from the untimely removal of variables that are actually significant, which may well bias the result. And in connection with this, it should be recalled that if we know (for biological reasons) why a variable should be included in a model (e.g. wood specific density), it is not because a statistical test declares it non-significant that it should be rejected (because of the test's type II error).

\begin{filrouge}{Selecting variables}{selvar}

Let us select the variables $\ln(D)$, $[\ln(D)]^2$, $[\ln(D)]^3$, $\ln(H)$ to predict the log of the biomass. The complete model is therefore:

\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(H)
+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
Variables are selected in R using the \texttt{step} command applied to the fitted complete model:
\R{m <- lm(log(Btot)$\sim$I(log(dbh))+I(log(dbh)\^{}2)+I(log(dbh)\^{}3)+%
I(log(heig)),data=dat[\bidouille{\newline}dat\$Btot>0,])%
\\ summary(step(m))}%
which yields:
\Rout{\begin{tabular}{lrrrrl}%
                 & Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)      & -6.50202 &    0.35999 & -18.062 &  < 2e-16 & ***\\ %
I(log(dbh)\^{}2) &  0.23756 &    0.01972 &  12.044 & 1.53e-14 & ***\\ %
I(log(heig))     &  1.01874 &    0.17950 &   5.675 & 1.59e-06 & ***\\ %
\end{tabular}}%
The variables selected are therefore  $[\ln(D)]^2$ and $\ln(H)$. The model finally retained is therefore:
$\ln(B)=-\decimal{6}{50202}+\decimal{0}{23756}[\ln(D)]^2+\decimal{1}{01874}\ln(H)$,
with a residual standard deviation of \decimal{0}{3994} and
$R^2=\decimal{0}{974}$.
\end{filrouge}

\subsection{Selecting models\label{selmod}}

Given two competitor models that predict the same response variable to within one transformation of a variable, which should we choose? Let us look at a few different cases before answering this question.

#### Nested models

The simplest case is where the two models to be compared are nested. A model is *nested* in another if the two predict the same response variable and if we can move from the second to the first by removing one or several effect variables. For example, the biomass table $B=a_0+a_1D+\varepsilon$ is nested in $B=a_0+a_1D+a_2D^2H+\varepsilon$ since we can move from the second to the first by deleting $D^2H$ from the effect variables. Likewise, the model $B=a_0+a_2D^2H+\varepsilon$ is nested in $B=a_0+a_1D+a_2D^2H+\varepsilon$ since we can move from the second to the first by deleting $D$ from the effect variables. By contrast, the model $B=a_0+a_1D+\varepsilon$ is not nested in $B=a_0+a_2D^2H+\varepsilon$.
Let $p$ be the number of effect variables in the complete model and $p'<p$ be the number of effect variables in the nested model. Without loss of generality, we can write the complete model as:
\begin{equation}
Y=f(X_1,\ \ldots,\ X_{p'},\ X_{p'+1},\ \ldots,\ X_p;
\theta_0,\ \theta_1)+\varepsilon(\#eq:embnl)
\end{equation}
where ($\theta_0$, $\theta_1$) is the vector of the coefficients associated with the complete model and $\theta_0$ is the vector of the coefficients associated with the nested model, which may be obtained by setting 
$\theta_1=\mathbf{0}$. In the particular case of the linear model, the complete model is obtained as the sum of the nested model and additional terms:
\begin{equation}
\underbrace{\underbrace{Y=a_0+a_1X_1+\ldots+a_{p'}X_{p'}}_{\mbox{\scriptsize
nested model}}+a_{p'+1}X_{p'+1}+\ldots+a_pX_p}_{\mbox{
\scriptsize complete model}}+\varepsilon(\#eq:emblm)
\end{equation}
where $\theta_0=(a_0,\ \ldots,\ a_{p'})$ and
$\theta_1=(a_{p'+1},\ \ldots,\ a_p)$.

In the case of nested models, a statistical test can be used to test one of the models against the other. The null hypothesis of this test is that $\theta_1=\mathbf{0}$, i.e. the additional terms are not significant, which can also be expressed as: the nested model is better than the complete model. If the p-value of this test proves to be below the significance level (typically 5\ \%), then the null hypothesis is rejected, i.e. the complete model is best. Conversely, if the p-value is above the significance threshold, the nested model is considered to be the best.

In the case of the linear model (\ref{emblm}), the test statistic is a ratio of the mean squares, which under the null hypothesis follows Fisher's distribution. This is the same type of test as that used to test the overall significance of a multiple regression, or that used in the "stepwise" method of selecting variables. In the general case of the non-linear model (\ref{embnl}), the test statistic is a likelihood ratio, such that $-2\log$(likelihood ratio) under the null hypothesis follows a $\chi^2$ distribution.

\begin{filrouge}{Testing nested models: $\ln(D)$}{fboite}%

In red line \ref{selvar} the variable $[\ln(D)]^2$ was selected with $\ln(H)$ as effect variable of $\ln(B)$ but not $\ln(D)$. Model $\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_4\ln(H)$, which includes the additional term $\ln(D)$, can be compared with the model $\ln(B)=a_0+a_2[\ln(D)]^2+a_4\ln(H)$ using the nested models test. In R, the \texttt{anova} command can be used to test a nested model, with the first argument being the nested model and the second being the complete model:
\R{comp <-
lm(log(Btot)$\sim$I(log(dbh))+I(log(dbh)\^{}2)+I(log(heig)),data=dat[dat\$Btot>0,])%
\\ nest <- lm(log(Btot)$\sim$I(log(dbh)\^{}2)+I(log(heig)),data=dat[dat\$Btot>0,])%
\\ anova(nest,comp)}%
The test gives the following result:
\Rout{\begin{tabular}{lrrrrrr}%
  & Res.Df &    RSS & Df & Sum of Sq &      F & Pr(>F)\\ %
1 &     38 & 6.0605 &    &           &        &       \\ %
2 &     37 & 5.8964 &  1 &   0.16407 & 1.0295 & 0.3169\\ %
\end{tabular}}%
The p-value is  \decimal{0}{3169}, therefore greater than 5\ \%. The nested model (without $\ln(D)$) is therefore selected rather than the complete model.
\end{filrouge}

\begin{filrouge}{Testing nested models: $\ln(H)$}{fboite2}%
The model $\ln(B)=-\decimal{8}{42722}+\decimal{2}{36104}\ln(D)$ was obtained in red line \ref{rllnBvD} whereas red line \ref{flnDlnH} gave the model $\ln(B)=-\decimal{8}{9050}+\decimal{1}{8654}\ln(D)+\decimal{0}{7083}\ln(H)$. As the first is nested in the second, we can test for which is the best. The command 
\R{comp <- lm(log(Btot)$\sim$I(log(dbh))+I(log(heig)),data=dat[dat\$Btot>0,])%
\\ nest <- lm(log(Btot)$\sim$I(log(dbh)),data=dat[dat\$Btot>0,])%
\\ anova(nest,comp)}%
yields:
\Rout{\begin{tabular}{lrrrrrrl}%
  & Res.Df &    RSS & Df & Sum of Sq &      F &   Pr(>F) &   \\ %
1 &     39 & 8.3236 &    &           &        &          &   \\ %
2 &     38 & 6.4014 &  1 &    1.9222 & 11.410 & 0.001698 & **\\ %
\end{tabular}}%
As the p-value is less than 5\ \%, the complete model (including $\ln(H)$ as effect variable) is selected rather than the nested model.

\end{filrouge}

#### Models with the same response variable

When we want to compare two models that have the same response variable but are not nested, we can no longer use a statistical test. For example, we cannot use the above-mentioned test to compare $B=a_0+a_1D+\varepsilon$ and $B=a_0+a_2D^2H+\varepsilon$. In this case, we must use an information criterion [@bozdogan87; @burnham02; @burnham04]. There are several, suited to different contexts. The most widespread are the "Bayesian information criterion", or BIC, and above all the @akaike74 information criterion (or AIC). The AIC is expressed as:
\[
\mathrm{AIC}=-2\ln\ell(\hat{\theta})+2q
\]
where $\ell(\hat{\theta})$ if the model's likelihood, i.e. the likelihood of the sample for the values estimated from model parameters (see equation \ref{vrais}), and $q$ is the number of free parameters estimated. In particular, in the case of a multiple regression against $p$ effect variables, $q=p+1$ (i.e. the $p$ coefficients associated with the $p$ effect variables
plus the y-intercept). The coefficient $-2$ in front of the log-likelihood in the AIC expression is identical to that in the test statistic on the likelihood ratio in the case of nested models. Given two models with the same number of parameters, the best model is that with the highest likelihood, therefore that with the smallest AIC. At equal likelihoods, the best model is that with the fewest parameters (in accordance with the principle of Occam's razor), and therefore is once more that with the smallest AIC. When all is said and done, the best model is that with the smallest value of AIC.

The BIC is similar in expression to the AIC, but with a term that penalizes more strongly the number of parameters:
\[
\mathrm{BIC}=-2\ln\ell(\hat{\theta})+q\ln(n)
\]
where $n$ is the number of observations. Here again, the best model is that with the smallest value of BIC. When fitting volume or biomass tables, AIC is used rather than BIC as a model selection criterion.

\begin{filrouge}{Selecting models with $B$ as response variable}{selB}%

The following models with B as response variable were fitted:

\begin{itemize}
\item red line \ref{fDpara} or \ref{finvD}: $B=-\decimal{3}{840}\times10^{-3}D+\decimal{1}{124}\times10^{-3}D^2$
\item red line \ref{fDD2var}: $B=-\decimal{3}{319456}\times10^{-3}D+\decimal{1}{067068}\times10^{-3}D^2$
\item red line \ref{fnlsD}: $B=\decimal{2}{492}\times10^{-4}D^{2.346}$
\item red line \ref{fnlmD}: $B=\decimal{2}{445}\times10^{-4}D^{2.35105}$
\item red line \ref{frlpD2H} ou \ref{fH}: $B=\decimal{2}{747}\times10^{-5}D^2H$
\item red line \ref{fD2Hvar}: $B=\decimal{2}{740688}\times10^{-5}D^2H$
\item red line \ref{fnlsD2H}: $B=\decimal{7}{885}\times10^{-5}(D^2H)^{0.9154}$
\item red line \ref{fnlmD2H}: $B=\decimal{8}{19}\times10^{-5}(D^2H)^{0.9122144}$
\item red line \ref{fnlsDH}: $B=\decimal{1}{003}\times10^{-4}D^{1.923}H^{0.7435}$
\item red line \ref{fnlmDH}: $B=\decimal{1}{109}\times10^{-4}D^{1.9434876}H^{0.6926256}$
\end{itemize}
The models in red lines \ref{fDpara}, \ref{fDD2var}, \ref{frlpD2H} and \ref{fD2Hvar} are fitted by linear regression while the others are fitted by non-linear regression. These models have five distinct forms, with two fitting methods for each: using a weighted regression by the weighted least squares method (red lines \ref{fDpara}, \ref{fnlsD}, \ref{frlpD2H}, \ref{fnlsD2H} and \ref{fnlsDH}) or using a regression with variance model by the maximum likelihood method (red lines \ref{fDD2var}, \ref{fnlmD}, \ref{fD2Hvar}, \ref{fnlmD2H}
and \ref{fnlmDH}). The predictions made by these different models are shown in  Figure \@ref(fig:fpredB). Let \texttt{m} be one of the fitted models with dbh as the only entry. A plot of the predictions made by this model may be obtained as follows:
\R{with(dat,plot(dbh,Btot,xlab="Dbh(cm)",ylab="Biomass (t)"))%
\\ D <- seq(par("usr")[1],par("usr")[2],length=200)%
\\ lines(D,predict(m,newdata=data.frame(dbh=D)),col="red")}%
For a model \texttt{m} that has dbh and height as entries, its predictions may be obtained as follows:
\R{D <- seq(0,180,length=20)%
\\ H <- seq(0,61,length=20)%
\\ B <- matrix(predict(m,newdata=expand.grid(dbh=D,height=H)),length(D))}%
and a plot of the biomass response surface against diameter and height may be obtained by:
\R{M <- persp(D,H,B,xlab="Dbh (cm)",ylab="Height (m)",%
zlab="Biomass (t)",\bidouille{\newline}ticktype="detailed")
\\ points(trans3d(dat\$dbh,dat\$heig,dat\$Btot,M))}%
Given a fitted model \texttt{m}, its AIC may be calculated by the command: 
\R{AIC(m)}%
AIC values for the 10 models listed above are given in Table \ref{fAICD}. This table illustrates a problem that arises with several statistical software packages, including R: when we maximize the log-likelihood (\ref{ll}), the constants (such as $-n\ln(2\pi)/2$) no longer play any role. The constant we use to calculate the log-likelihood, and by consequence AIC, is therefore a matter of convention, and different constants are used depending on the calculation. Thus, in Table \ref{fAICD}, we can see that the values of AIC in models fitted by the \texttt{nls} command are substantially higher than those in the other models: it is not that these models are substantially worse than the others, it is simply that the \texttt{nls} command uses a constant different from the others to calculate the log-likelihood. Therefore, with R, it should be remembered that the values of AIC should be compared only for models that have been fitted using the same command.
In our present case, if we compare the two models that were fitted with the \texttt{lm} command, the best (i.e. that with the smallest AIC) is that which has $D^2H$ as effect variable (red line \ref{frlpD2H}). If we compare the five models fitted with the \texttt{nlme} command, the best is again that with $D^2H$ as effect variable (red line \ref{fD2Hvar}). And if we compare the three models fitted with the \texttt{nls} command, the best is yet again that with $D^2H$ as effect variable (red line \ref{fnlsD2H}). It may therefore be concluded that whatever fitting method is used, the biomass table that uses $D^2H$ as effect variable is the best.
\end{filrouge}

\begin{figure}[htbp]
\begin{center}
{\sffamily\bfseries A}\ifincludegraphics{height=6cm}{faicD1}%
{\sffamily\bfseries B}\ifincludegraphics{height=6cm}{faicD2}\par\bigskip%
{\sffamily\bfseries C}\includegraphics[width=0.49\textwidth]{faicD3}%
{\sffamily\bfseries D}\includegraphics[width=0.49\textwidth]{faicD4}%
\end{center}
\caption[Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by
@henry10]{Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by
@henry10.The data are shown as points. (A)
Tables with dbh as only entry, corresponding to red lines \ref{fDpara} (\ifelsecoul{red}{solid lines}), \ref{fDD2var} (\ifelsecoul{green}{dashed line}), \ref{fnlsD}
(\ifelsecoul{blue}{dotted line}) and \ref{fnlmD}
(\ifelsecoul{magenta}{dashes and dots}). (B) Tables with $D^2H$
as the only effect variable, corresponding to red lines \ref{frlpD2H} (\ifelsecoul{red}{solid line}), \ref{fD2Hvar} (\ifelsecoul{green}{dashed line}), \ref{fnlsD2H}
(\ifelsecoul{blue}{dotted line}) and \ref{fnlmD2H}
(\ifelsecoul{magenta}{dashes and dots}). (C) Table corresponds to red line \ref{fnlsDH}. (D) Table corresponds to red line \ref{fnlmDH}.\label{fpredB}}
\end{figure}

\begin{table}
\caption[AIC values for 10 biomass tables fitted to data from 42 trees measured in Ghana by @henry10]{AIC values for 10 biomass tables fitted to data from 42 trees measured in Ghana by @henry10. These 10 tables directly predict biomass.\label{fAICD}}

\begin{center}
\begin{tabular}{|rlllr|}\hline
Red           & Entry    & Fitting$^*$& Command       & AIC                     \\ %
line          &          & method     & R             &                         \\\hline%
\ref{fDpara}  & $D$      & WLS        & \texttt{lm}   & \decimal{76}{71133}     \\ %
\ref{fDD2var} & $D$      & ML         & \texttt{nlme} & \decimal{83}{09157}     \\ %
\ref{fnlsD}   & $D$      & WLS        & \texttt{nls}  & \decimal{24\ 809}{75727}\\ %
\ref{fnlmD}   & $D$      & ML         & \texttt{nlme} & \decimal{75}{00927}     \\ %
\ref{frlpD2H} & $D^2H$   & WLS        & \texttt{lm}   & \decimal{65}{15002}     \\ %
\ref{fD2Hvar} & $D^2H$   & ML         & \texttt{nlme} & \decimal{69}{09644}     \\ %
\ref{fnlsD2H} & $D^2H$   & WLS        & \texttt{nls}  & \decimal{24\ 797}{53706}\\ %
\ref{fnlmD2H} & $D^2H$   & ML         & \texttt{nlme} & \decimal{69}{24482}     \\ %
\ref{fnlsDH}  & $D$, $H$ & WLS        & \texttt{nls}  & \decimal{24\ 802}{91248}\\ %
\ref{fnlmDH}  & $D$, $H$ & ML         & \texttt{nlme} & \decimal{76}{80204}     \\ %
\hline
\end{tabular}
\end{center}
{\footnotesize$^*$WLS = weighted least squares, ML = maximum likelihood}
\end{table}

\begin{filrouge}{Selecting models with $\ln(B)$ as response variable}{sellnB}%
The following models with $\ln(B)$ as response variable were fitted:
\begin{itemize}
\item red line \ref{rllnBvD} or \ref{flnDpol}: $\ln(B)=-\decimal{8}{42722}+\decimal{2}{36104}\ln(D)$
\item red line \ref{rllnBvD2H}: $\ln(B)=-\decimal{8}{99427}+\decimal{0}{87238}\ln(D^2H)$
\item red line \ref{flnDlnH}: $\ln(B)=-\decimal{8}{9050}+\decimal{1}{8654}\ln(D)+\decimal{0}{7083}\ln(H)$
\item red line \ref{selvar}: $\ln(B)=-\decimal{6}{50202}+\decimal{0}{23756}[\ln(D)]^2+\decimal{1}{01874}\ln(H)$
\end{itemize}
All these models were fitted using linear regression by the ordinary least squares method. A plot of the predictions on a log scale for model \texttt{m} with dbh as only entry may be obtained by the following command:
\R{with(dat,plot(dbh,Btot,xlab="Dbh (cm)",ylab="Biomass (t)",log="xy"))%
\\ D <- 10\^{}par("usr")[1:2]%
\\ lines(D,exp(predict(m1,newdata=data.frame(dbh=D))))}%
For a model that uses both dbh and height as entries, a plot on a log scale may be obtained by the command:
\R{D <- exp(seq(log(1),log(180),length=20))%
\\ H <- exp(seq(log(1),log(61),length=20))%
\\ B <- matrix(predict(m,newdata=expand.grid(dbh=D,heig=H)),length(D))%
\\ M <- persp(log(D),log(H),B,xlab="log(Diameter) (cm)",ylab="log(height) (m)",%
zlab=\bidouille{\newline}"log(Biomass) (t)",ticktype="detailed")%
\\ points(trans3d(log(dat\$dbh),log(dat\$heig),log(dat\$Btot),M))}%
Predictions of $\ln(B)$ by the four models are given in  Figure \@ref(fig:fpredlnB). Given a fitted model \texttt{m}, its AIC may be calculated by the command:
\R{AIC(m)}%
Table \ref{fAIClnD} gives AIC values for the four models. As all four models were fitted by the same \texttt{lm} command, the AIC values are directly comparable. The best model, i.e. that with the smallest AIC, is the fourth (red line \ref{selvar}). It should also be noted that an AIC-based classification of these models is entirely consistent with the results of the nested models tests performed previously (red lines \ref{fboite} and \ref{fboite2}).
\end{filrouge}

\begin{figure}[htbp]
\begin{center}
{\sffamily\bfseries A}\includegraphics[height=6cm]{faiclnD1}%
{\sffamily\bfseries B}\includegraphics[height=6cm]{faiclnD2}\par\bigskip
{\sffamily\bfseries C}\includegraphics[width=0.49\textwidth]{faiclnD3}%
{\sffamily\bfseries D}\includegraphics[width=0.49\textwidth]{faiclnD4}
\end{center}
\caption[Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by @henry10]{Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by @henry10. The data are shown as points. (A) Model in red line \ref{rllnBvD}. (B) Model in red line \ref{rllnBvD2H}. (C) Model in red line \ref{flnDlnH}. (D) Model in red line \ref{selvar}.\label{fpredlnB}}
\end{figure}

\begin{table}
\caption[AIC values for four biomass tables fitted to data from 42 trees measured in Ghana by 
@henry10]{AIC values for four biomass tables fitted to data from 42 trees measured in Ghana by 
@henry10. These four tables predict the log of the biomass and are all fitted using linear regression by the ordinary least squares (OLS) method.
\label{fAIClnD}}
\begin{center}
\begin{tabular}{|rlllr|}\hline
Red             & Entry    & Fitting    & Command     & AIC                \\ %
line            &          & method     & R           &                    \\\hline%
\ref{rllnBvD}   & $D$      & OLS        & \texttt{lm} & \decimal{56}{97923}\\ %
\ref{rllnBvD2H} & $D^2H$   & OLS        & \texttt{lm} & \decimal{46}{87780}\\ %
\ref{flnDlnH}   & $D$, $H$ & OLS        & \texttt{lm} & \decimal{48}{21367}\\ %
\ref{selvar}    & $D$, $H$ & OLS        & \texttt{lm} & \decimal{45}{96998}\\ %
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Models with different response variables\label{furni}}

The more general case is when we want to compare two models that do not have the same response variable because one is a transform of the other. For example, the models $B=aD^b+\varepsilon$ and $\ln(B)=a+b\ln(D)+\varepsilon$ both predict biomass, but the response variable is $B$ in one case and $\ln(B)$ in the other. In this case, we cannot use the AIC or BIC to compare the models. By contrast, @furnival61 index can in this case be used to compare the models. The model with the smallest Furnival's index is considered to be the best [@parresol99].

Furnival's index is defined only for a model whose residual error $\varepsilon$ has a variance that is assumed to be constant: $\mbox{Var}(\varepsilon)=\sigma^2$. By contrast, no constraint is imposed on the form of the transformation of the variable linking the modeled response variable $Y$ to the variable of interest (volume or biomass). Let us consider the case of a biomass model (that can immediately be transposed into a volume model) and let $\psi$ be this variable transformation: $Y=\psi(B)$. Furnival's index is defined by:
\[
F=\frac{\hat{\sigma}}{\sqrt[n]{\prod_{i=1}^n\psi'(B_i)}}
=\exp\Big(-\frac{1}{n}\sum_{i=1}^n\ln[\psi'(B_i)]\Big)\ \hat{\sigma}
\]
where $\hat{\sigma}$ if the estimation of the residual standard deviation of the fitted model and $B_i$ is the biomass of the $i$th tree measured. When there is no transformation of variables, $\psi$ is the identity function and Furnival's index $F$ is then equal to the residual standard deviation $\hat{\sigma}$. The most common variable transformation is the log transformation:
$\psi(B)=\ln(B)$ and $\psi'(B)=1/B$, in which case Furnival's index is:

\[
F_{\ln}=\hat{\sigma}\sqrt[n]{\textstyle\prod_{i=1}^nB_i}
=\exp\Big(\frac{1}{n}\sum_{i=1}^n\ln(B_i)\Big)\ \hat{\sigma}
\]

For linear regressions where the residual variance is assumed to be proportional to a power of an effect variable $X_1$, a trick can nevertheless be used to define Furnival's index. The linear regression:

\begin{equation}
Y=a_0+a_1X_1+a_2X_2+\ldots+a_pX_p+\varepsilon(\#eq:F1)
\end{equation}
where $\mbox{Var}(\varepsilon)=(kX_1^c)^2$; is strictly identical to the linear regression (see p.\pageref{apart}):
\begin{equation}
Y'=a_0X_1^{-c}+a_1X_1^{1-c}+a_2X_2X_1^{-c}+\ldots+
a_pX_pX_1^{-c}+\varepsilon'(\#eq:F2)
\end{equation}
where $Y'=YX_1^{-c}$, $\varepsilon'=\varepsilon X_1^{-c}$ and
$\mbox{Var}(\varepsilon')=k^2$. As model (\ref{F2}) has constant residual variance, its Furnival's index is defined.
By extension, we can define the Furnival index of model (\ref{F1}) as being the Furnival index of model (\ref{F2}). If $Y=\psi(B)$, then $Y'=X_1^{-c}\psi(B)$, such that Furnival's index is:

\[
F=\frac{\hat{k}}{\sqrt[n]{\prod_{i=1}^nX_{i1}^{-c}\psi'(B_i)}}
=\exp\Big(\frac{1}{n}\sum_{i=1}^n\{c\ln(X_{i1})-\ln[\psi'(B_i)]\}
\Big)\ \hat{k}
\]
Therefore, Furnival's index can be used to select the value of exponent $c$ in a weighted regression (see p.\pageref{chx}).

### Choosing a fitting method

Let us return to the method used to fit a volume or biomass model. Many solutions may be available to fit a model. Let us for instance consider the biomass model:
\[
B=a\rho^{b_1}D^{b_2}H^{b_3}+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ kD^c)
\]
This model could be adjusted as a non-linear model (*i*) by the weighted least squares method ($c$ fixed in advance) or (*ii*) by the maximum likelihood method ($c$ not fixed in advance). If we apply a log transformation to the data, we could (*iii*) fit the multiple regression:
\[
\ln(B)=a'+b_1\ln(\rho)+b_2\ln(D)+b_3\ln(H)+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ \sigma)
\]
Thus, for a given model that predicts biomass as a power of effect variables, we have three possible fitting methods. Obviously, methods (*i*)--(*ii*) and (*iii*) are based on different hypotheses concerning the structure of the residual errors: additive error with regard to $B$ in cases (*i*) and (*ii*), multiplicative error with regard to $B$ in case (*iii*). But both these error types reflect data heteroscedasticity, and therefore fitting methods (*i*), (*ii*) and (*iii*) all have a chance of being valid.

As another example, let us consider the biomass table:
\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)\}+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ kD^c)
\]
Here again, we can (*i*) fit a non-linear model by the least squares method (specifying $c$ in advance), (*ii*) fit a non-linear model by the maximum likelihood method (estimating $c$), or (*ii*) fit a multiple regression on log-transformed data:
\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ \sigma)
\]
Here again, the structure of the errors is not the same in the three cases, but all can correctly reflect the heteroscedasticity of the biomass.
In most cases, the different fitting methods give very similar results in terms of predictions. Should any doubt persist regarding the most appropriate fitting method, then model selection methods may be employed to decide. But in practice the choice of a particular fitting method results rather from the importance given to the respective advantages and drawbacks of each method. Multiple regression has the drawback of imposing constraints on the form of the residuals, and has less flexibility for the mean model. By contrast, it has the advantage of offering an explicit expression for the estimators of the model's coefficients: there is therefore no risk of having erroneous estimators for the coefficients. The non-linear model has the advantage of not imposing any constraints on the mean model or the variance model. Its drawback is that it does not have an explicit expression for parameter estimators: there is therefore the risk of having erroneous parameter estimators.

\begin{filrouge}{Power model fitting methods}{fmeth}%
We have already taken a look at three methods for fitting the power model $B=aD^b$:
\begin{enumerate}
\item using a simple linear regression on log-transformed data (red line \ref{rllnBvD}):
$\ln(B)=-\decimal{8}{42722}\bidouille{\newline}%
+\decimal{2}{36104}\ln(D)$, if we "naively" apply the exponential inverse transformation;
\item using a weighted non-linear regression (red line \ref{fnlsD}):
$B=\decimal{2}{492}\times10^{-4}D^{2.346}$;
\item using a non-linear regression with variance model (red line \ref{fnlmD}): $B=\decimal{2}{445}\bidouille{\newline}%
\times10^{-4}D^{2.35105}$.
\end{enumerate}
The predictions given by these three fittings of the same model are illustrated in  Figure \@ref(fig:fselm). The differences can be seen to be minimal and well below prediction precision, as we will see later (\S\ \ref{BVpred}).
\end{filrouge}

\begin{figure}[htb]
\begin{center}
\ifincludegraphics{width=\textwidth}{fselm}
\end{center}
\caption[Biomass predictions by the same power table fitted in three different ways to data from 42 trees measured in Ghana by @henry10]{Biomass predictions by the same power table fitted in three different ways to data from 42 trees measured in Ghana by @henry10. The data are shown as points. The \ifelsecoul{red}{solid} line is the linear regression on log-transformed data (red line \ref{rllnBvD}). The \ifelsecoul{green line}{dashed line} (practically superimposed by the \ifelsecoul{blue line}{dotted line}) is the fitting by weighted non-linear regression (red line \ref{fnlsD}). The \ifelsecoul{blue color}{dotted line} shows the fitting by non-linear regression with variance model (red line \ref{fnlmD}). (A) No data transformation. (B) On a log scale.\label{fselm}}
\end{figure}%

## Stratification and aggregation factors

Up until now we have considered that the dataset used to fit the volume or biomass model is homogeneous. In reality, the dataset may be drawn from measurements made under different conditions, or may have been generated by merging several separate datasets. Covariables are generally used to describe this dataset heterogeneity. For example, a covariable may indicate the type of forest in which the measurements were made (deciduous, semi-deciduous, evergreen, etc.), the type of soil, or year planted (if a plantation), etc. A crucial covariable for multispecific data is tree species. Initially, all these covariables that can explain the heterogeneity of a dataset were considered as qualitative variables (or factors). The various modalities of these factors define strata, and a well constituted dataset is one that is sampled in relation to previously identified strata (see \S\ \ref{stratif}). How can these qualitative covariables be taken into account in a volume or biomass model? Is it valid to analyze the dataset in an overall manner? Or should we analyze the data subsets corresponding to each stratum separately? These are the questions we will be addressing here (\S\ \ref{stdat}).

Also, biomass measurements are made separately for each compartment in the tree (see chapter \@ref(ter)). Therefore, in addition to an estimation of its total biomass, we also have, for each tree in the sample, an estimation of the biomass of its foliage, the biomass of its truck, of its large branches, of its small branches, etc. How can we take account of these different compartments when establishing biomass models? We will also be addressing this question (\S\ \ref{cmpt}).

\subsection{Stratifying data\label{stdat}}

Let us now consider that there are qualitative covariables that stratify the dataset into $S$ strata. As each stratum corresponds to a cross between the modalities of the qualitative covariables (in the context of experimental plans we speak of *treatment* rather than stratum) we will not consider each covariable separately. For example, if one covariable indicates the type of forest with three modalities (let us say: deciduous forest, semi-deciduous forest and evergreen forest) and another covariable indicates the type of soil with three modalities (let us say: sandy soil, clay soil, loamy soil), the cross between these two covariables gives $S=3\times 3=9$ strata (deciduous forest on sandy soil, deciduous forest on clay soil, etc.). We are not looking to analyze the forest type effect separately, or the soil type effect separately. Also, if certain combinations of the covariable modalities are not represented in the dataset, this reduces the number of strata. For example, if there are no evergreen forests on loamy soil, the number of strata $S=8$ instead of 9.

Therefore, when stratifying a dataset, one strategy consists in fitting a model separately for each stratum. In the case of a multiple regression, this would be written:
\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon_s
\]
where
\[
\varepsilon_s\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma_s)
\]
where ($Y_s$, $X_{1s}$, \ldots, $X_{ps}$) designates an observation relative to stratum $s$, for $s=1$, \ldots, $S$. There are now $S\times(p+1)$ coefficients to be estimated. An alternative strategy consists in analyzing the dataset as a whole, by fitting a model such as:
\begin{equation}
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
(\#eq:ancov)
\end{equation}
where
\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma)
\]
This model differs only in the structure of the error. This type of model is called an analysis of *covariance*. It assumes that all the residuals have the same variance, not only within each stratum, but also from one stratum to another. An analysis of covariance can test whether there is a stratum effect on the response variable, alone or in interaction with each of the effect variables $X_1$, \ldots, $X_p$. Testing the main effect of the stratification is equivalent to testing the null hypothesis $a_{01}=a_{02}=\ldots=a_{0S}$. The test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher's distribution. Testing the effect of the interaction between the stratification and the $j$th effect variable is equivalent to testing the null hypothesis $a_{j1}=a_{j2}=\ldots=a_{jS}$. As previously, the test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher's distribution.

The advantage of testing these effects is that each time one proves not to be significant, the $S$ coefficients $a_{j1}$, $a_{j2}$, \ldots, $a_{jS}$ to be estimated can be replaced by a single common coefficient $a_j$. Let us imagine, for example, that in analysis of covariance (\ref{ancov}), the main effect of the stratum is not significant, and neither is the interaction between the stratum and the first $p'$ effect variables (where $p'<p$). In this case the model to be fitted is:
\[
Y_s=a_0+a_1X_{1s}+\ldots+a_{p'}X_{p's}+a_{p'+1,s}X_{p'+1,s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]
where $\varepsilon\sim\mathcal{N}(0,\ \sigma)$. This model now includes "only" $p'+1+(p-p')S$ coefficients to be estimated, instead of $(p+1)S$ coefficients if we fit the model separately for each stratum. As all the observations serve to estimate common coefficients $a_0$, \ldots, $a_{p'}$, these are estimated more precisely than if we fit the model separately for each stratum.

This principle of an analysis of covariance can be immediately extended to the case of a non-linear model. Here again, we can test whether or not the coefficients are significantly different between the strata, and if necessary estimate a common coefficient for all the strata.

\begin{filrouge}{Specific biomass model}{fmspe}
In red line \ref{rllnBvD2H}, we used simple linear regression to fit a power model with $D^2H$ as effect variable to log-transformed data: $\ln(B)=a+b\ln(D^2H)$. We can now include species-related information in this model to test whether the coefficients $a$ and $b$ differ from one species to another. The model corresponds to an analysis of covariance:
\[
\ln(B_s)=a_s+b_s\ln(D_s^2H_s)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
where index $s$ designates the species. This model is fitted by the command:
\R{m <- lm(log(Btot)$\sim$species*I(log(dbh\^{}2*heig)),data=dat[dat\$Btot>0,])}%
To test whether the coefficients $a$ and $b$ differ from one species to another, we can use the command:
\R{anova(m)}%
which yields:
\Rout{\begin{tabular}{lrrrrrl}%
                             & Df &  Sum Sq & Mean Sq &   F value &    Pr(>F) &    \\ %
species                      & 15 & 117.667 &   7.844 &   98.4396 & 1.647e-13 & ***\\ %
I(log(dbh\^{}2*heig))        &  1 & 112.689 & 112.689 & 1414.1228 & < 2.2e-16 & ***\\ %
species:I(log(dbh\^{}2*heig))&  7 &   0.942 &   0.135 &    1.6879 &    0.1785 &    \\ %
Residuals                    & 17 &   1.355 &   0.080 &           &           &    \\ %
\end{tabular}}%
The first line in the table tests for a species effect, i.e. whether the y-intercept $a_s$ differs from one species to another. The null hypothesis of this test is that there is no difference between species: $a_1=a_2=\ldots=a_S$, where $S=16$ is the number of species. The test statistic is given in the "F value" column. As the p-value of the test is less than 5\ \%, it may be concluded that the y-intercept of the model is significantly different from one species to another. The second line in the table tests for an effect of the $D^2H$, variable, i.e. whether the mean slope associated with this variable is significantly different from zero. The third line in the table tests whether the slope-species interaction is significant, i.e. whether slope $b_s$ differs from one species to another. The null hypothesis is that there is no difference between species: $b_1=b_2=\ldots=b_S$. The p-value here is 0.1785, therefore greater than 5\ \%: there is therefore no significant slope difference between the species.

This leads us to fit the following model:

\begin{equation}
\ln(B_s)=a_s+b\ln(D_s^2H_s)+\varepsilon(\#eq:mspe)
\end{equation}
which considers that slope $b$ is the same for all species. The relevant command is:
\R{m <- lm(log(Btot)$\sim$species+I(log(dbh\^{}2*heig)),data=dat[dat\$Btot>0,])%
\\ anova(m)}%
and yields:
\Rout{\begin{tabular}{lrrrrrl}%
                      & Df &  Sum Sq & Mean Sq & F value &    Pr(>F) &    \\ %
species                & 15 & 117.667 &   7.844 &   81.99 & < 2.2e-16 & ***\\ %
I(log(dbh\^{}2*heig)) &  1 & 112.689 & 112.689 & 1177.81 & < 2.2e-16 & ***\\ %
Residuals             & 24 &   2.296 &   0.096 &         &           &    \\ %
\end{tabular}}%
Model coefficients may be obtained by the command:
\R{summary(m)}%
which yields:
\Rout{\begin{tabular}{lrrrrl}%
                                 & Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)                      & -9.00359 &    0.45144 & -19.944 &   <2e-16 & ***\\ %
speciesAubrevillea kerstingii    & -0.54634 &    0.43784 &  -1.248 &   0.2241 &    \\ %
speciesCecropia peltata          & -0.77688 &    0.36261 &  -2.142 &   0.0425 & *  \\ %
speciesCeiba pentandra           & -0.70841 &    0.38048 &  -1.862 &   0.0749 & .  \\ %
speciesCola nitida               & -0.46428 &    0.44476 &  -1.044 &   0.3069 &    \\ %
speciesDaniellia thurifera       &  0.04685 &    0.46413 &   0.101 &   0.9204 &    \\ %
speciesDialium aubrevilliei      & -0.15626 &    0.43757 &  -0.357 &   0.7241 &    \\ %
speciesDrypetes chevalieri       &  0.04953 &    0.45395 &   0.109 &   0.9140 &    \\ %
speciesGarcinia epunctata        &  1.09645 &    0.47318 &   2.317 &   0.0293 & *  \\ %
speciesGuarea cedrata            & -0.45255 &    0.38460 &  -1.177 &   0.2509 &    \\ %
speciesHeritiera utilis          & -0.26865 &    0.32663 &  -0.822 &   0.4189 &    \\ %
speciesNauclea diderrichii       & -0.55464 &    0.35759 &  -1.551 &   0.1340 &    \\ %
speciesNesogordonia papaverifera & -0.47817 &    0.44335 &  -1.079 &   0.2915 &    \\ %
speciesPiptadeniastrum africanum & -0.17956 &    0.35718 &  -0.503 &   0.6197 &    \\ %
speciesStrombosia glaucescens    &  0.06333 &    0.39597 &   0.160 &   0.8743 &    \\ %
speciesTieghemella heckelii      & -0.09104 &    0.33908 &  -0.268 &   0.7906 &    \\ %
I(log(dbh\^{}2*heig))            &  0.89985 &    0.02622 &  34.319 &   <2e-16 & ***\\ %
\end{tabular}}%
The last line in this table gives the value of the slope:
$b=\decimal{0}{89985}$. The other lines give y-intercept values for the 16 species. By convention, R proceeds in the following manner when specifying these values: the first line in the table gives the y-intercept for the first species in alphabetical order. As the first species in alphabetical order is *Afzelia bella*, the y-intercept for *Afzelia bella* is therefore $a_1=-\decimal{9}{00359}$. The subsequent lines give the *difference* $a_s-a_1$ between the y-intercepts of the species indicated and the y-intercept of *Afzelia bella*. Thus, the y-intercept of *Aubrevillea kerstingii* is:
$a_2=a_1-\decimal{0}{54634}=-\decimal{9}{00359}-\decimal{0}{54634}=
-\decimal{9}{54993}$. Finally, the expression for the specific table corresponds to:
\[
\ln(B)=\decimal{0}{89985}\ln(D^2H)-\left\{
\begin{array}{lcl}
\decimal{9}{00359} && \mbox{for *Afzelia bella*}            \\ %
\decimal{9}{54993} && \mbox{for *Aubrevillea kerstingii*}   \\ %
\decimal{9}{78047} && \mbox{for *Cecropia peltata*}         \\ %
\decimal{9}{71200} && \mbox{for *Ceiba pentandra*}          \\ %
\decimal{9}{46786} && \mbox{for *Cola nitida*}              \\ %
\decimal{8}{95674} && \mbox{for *Daniellia thurifera*}      \\ %
\decimal{9}{15985} && \mbox{for *Dialium aubrevilliei*}     \\ %
\decimal{8}{95406} && \mbox{for *Drypetes chevalieri*}      \\ %
\decimal{7}{90713} && \mbox{for *Garcinia epunctata*}       \\ %
\decimal{9}{45614} && \mbox{for *Guarea cedrata*}           \\ %
\decimal{9}{27223} && \mbox{for *Heritiera utilis*}         \\ %
\decimal{9}{55823} && \mbox{for *Nauclea diderrichii*}      \\ %
\decimal{9}{48176} && \mbox{for *Nesogordonia papaverifera*}\\ %
\decimal{9}{18315} && \mbox{for *Piptadeniastrum africanum*}\\ %
\decimal{8}{94026} && \mbox{for *Strombosia glaucescens*}   \\ %
\decimal{9}{09462} && \mbox{for *Tieghemella heckelii*}     \\ %
\end{array}
\right.
\]
For a given diameter and height, the species with the highest biomass is *Garcinia epunctata* while that with the lowest biomass is *Cecropia peltata*. The model's residual standard deviation is $\hat{\sigma}=\decimal{0}{3093}$ and $R^2=\decimal{0}{9901}$.
\end{filrouge}

#### Case of a numerical covariable

Up until now we have considered that the covariables defining the stratification are qualitative factors. But in certain cases these covariables can also be interpreted as numerical variables. For instance, let us consider a biomass model for plantations [@saintandre05]. The year the plantation was set up (or, which comes to the same thing, the age of the trees), may be used as a stratification covariable. This year or age may be seen indifferently as a qualitative variable (cohort of trees of the same age) or as a numerical value. More generally, any numerical variable may be seen as a qualitative variable if it is divided into classes. In the case of age, we could also consider plantations aged 0 to 5 years as a stratum, plantations aged 5 to 10 years as another stratum, plantations aged 10 to 20 years as a third stratum, etc.. The advantage of dividing covariable $Z$ into classes and considering it as a qualitative variable is that this allows us to model the relation between $Z$ and response variable $Y$ without any *a priori* constraint on the form of this relation. By contrast, if we consider $Z$ to be a numerical variable, we are obliged *a priori* to set the form of the relation between $Y$ and $Z$ (linear relation, polynomial relation, exponential relation, power relation, etc.). The disadvantage of dividing $Z$ into classes and considering this covariable as being qualitative is that the division introduces an element of randomness. Also, the covariance model that uses classes of $Z$ (qualitative covariables) will generally have more parameters that need to be estimated than the model that considers $Z$ to be a numerical covariable.

It is customary in modeling to play on this dual interpretation of numerical variables. When covariable $Z$ is numerical (e.g. tree age), it is advisable to proceed in two steps (as explained in \S\ \ref{plus}):
\begin{enumerate}
\item consider $Z$ as a qualitative variable (if necessary after division into classes) and fit a covariance model which will provide a picture of the form of the relation between Z and the model's coefficients;
\item model this relation using an appropriate expression then return to the fitting of a linear or non-linear model, but considering $Z$ to be a numerical variable.
\end{enumerate}
If we return to the example of the age of the trees in a plantation: let us assume that age $Z$ has been divided into $S$ age classes. The first step consists typically of an analysis of covariance (assuming that the model has been rendered linear):
\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]
where $s=1$, \ldots, $S$. Let $Z_s$ be the median age of age class $s$. We then draw scatter plots of $a_{0s}$ against $Z_s$, $a_{1s}$ against
$Z_s$, \ldots, $a_{ps}$ against $Z_s$ and for each scatter plot we look for the form of the relation that fits the plot. Let us imagine that $a_{0s}$ varies in a linear manner with $Z_s$, that $a_{1s}$ varies in a exponential manner with $Z_s$, that $a_{2s}$ varies in a power manner with $Z_s$, and that the coefficients $a_{3s}$ to $a_{ps}$ do not vary in relation to $Z_s$ (which besides can be formally tested). In this particular case we will then, as the second step, fit the following non-linear model:
\[
Y=\underbrace{b_0+b_1Z}_{a_{0s}}+\underbrace{b_2\exp(-b_3Z)}_{a_{1s}}X_1
+\underbrace{b_4Z^{b_5}}_{a_{2s}}X_2+a_3X_3+\ldots+a_pX_p+\varepsilon
\]
where age $Z$ is now considered as a numerical variable. Such a model involving a numerical effect covariable is called a *parameterized* model (here by age).

*Ordinal* covariables warrant a special remark. An ordinal variable is a qualitative variable that establishes an order. For example, the month of the year is a qualitative variable that establishes a chronological order. The type of soil along a soil fertility gradient is also an ordinal variable. Ordinal variables are generally treated as fully fledged qualitative variables, but in this case the order information they contain is lost. An alternative consists in numbering the ordered modalities of the ordinal variable using integers, then considering the ordinal variable as a numerical variable. For example, in the case of the months of the year, we can set January = 1, February = 2, etc. This approach is meaningful only if the differences between the integers correctly reflect the differences between the modalities of the ordinal variable. For instance, if we set 1 = January 2011 up to 12 = December 2011, we will set 1 = January 2012 if the response is cyclically seasonal, whereas we will set 13 = January 2012 if the response presents as a continuous trend. In the case of three soil types along a fertility gradient, we will set 1 = poorest soil, 2 = soil of intermediate fertility, and 3 = richest soil if we consider the fertility difference between two soils induces a response that is proportional to this difference, but we will set 1 = poorest soil, 4 = soil of intermediate fertility, and 9 = richest soil if we consider that the response is proportional to the square of the fertility difference.

#### Special case of the species

In the case of multispecific datasets, the species is a stratification covariable that warrants special attention. If the dataset includes only a few species (less than about 10), and there are sufficient observations per species (see \S\ \ref{size}), then species may be considered to be a stratification covariable like any other. The model in this case would be divided into $S$ specific models, or the models could be grouped together based on the allometric resemblance of the species.

If the dataset contains many species, or if only a few observations are available for certain species, it is difficult to treat the species as a stratification covariable. A solution in this case consists in using species *functional traits*. Functional traits are defined here, a little abusively, as the numerical variables that characterize the species [@diaz97; @rosch97; @lavorel02; see \citealp{violle07} for a more rigorous definition]. The most widely used trait in biomass models is wood density. If we decide to use functional traits to represent species, these traits intervene as effect variables in the model in the same way as the effect variables that characterize the tree, for example its dbh or height. The single-entry (dbh) monospecific biomass model of the power type, which in its linear form may be written:
\[
\ln(B)=a_0+a_1\ln(D)+\varepsilon
\]
will thus, in the multispecific case, become a double-entry biomass model:
\[
\ln(B)=a_0+a_1\ln(D)+a_2\ln(\rho)+\varepsilon
\]
if we decide to use wood density $\rho$ to represent the specific effect.

\begin{filrouge}{Specific wood density-dependent biomass model}{fdens}%
In red line \ref{fmspe}, species-related information was taken into account in the model $\ln(B)=a+b\ln(D^2H)$ through a qualitative covariable. We can now capture this information through specific wood density $\rho$. The fitted model is therefore:
\begin{equation}
\ln(B)=a_0+a_1\ln(D^2H)+a_2\ln(\rho)+\varepsilon(\#eq:dens)
\end{equation}
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
As wood density in the dataset was measured for each individual, we must start by calculating mean wood density for each species:
\R{dm <- tapply(dat\$dens,dat\$species,mean)%
\\ dat <- cbind(dat,dmoy=dm[as.character(dat\$species)])}%
The \texttt{dat} dataset now contains an additional variable \texttt{dmoy} that gives specific wood density. The model is fitted by the command:
\R{m <- lm(log(Btot)$\sim$I(log(dbh\^{}2*heig))+I(log(dmoy)),data=dat[dat\$Btot>0,])%
\\ summary(m)}%
which yields:
\Rout{\begin{tabular}{lrrrrl}%
                      & Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)           & -8.38900 &    0.26452 & -31.714 &  < 2e-16 & ***\\ %
I(log(dbh\^{}2*heig)) &  0.85715 &    0.02031 &  42.205 &  < 2e-16 & ***\\ %
I(log(dmoy))          &  0.72864 &    0.17720 &   4.112 & 0.000202 & ***\\ %
\end{tabular}}%
with a residual standard deviation of \decimal{0}{3442} and $R^2=\decimal{0}{9806}$. The model is therefore written:
$\ln(B)=-\decimal{8}{38900}+\decimal{0}{85715}\ln(D^2H)+\decimal{0}{72864}\ln(\rho)$.
Is it better to take account of the species through wood density, as we have just done, or by constructing specific models as in red line \ref{fmspe}? To answer this question we need to compare model (\ref{mspe}) to the model (\ref{dens}) using the AIC:
\R{AIC(m)}%
which yields $\mathrm{AIC}=\decimal{34}{17859}$ for specific model (\ref{mspe}) and $\mathrm{AIC}=\decimal{33}{78733}$ for the model (\ref{dens}) that uses wood density. The latter therefore is slightly better, but the difference in AIC is nevertheless minor.
\end{filrouge}

To better take account of wood density variations within a tree, it is possible to analyze inter- and intra-specific variations rather than use a mean density based on the hypothesis that wood density is the same from the pith to the bark and from the top to the bottom of trees (see chapter \@ref(biol)). Wood density can be modeled by taking account of factors such as species, functional group, tree dimensions, and radial or vertical position in the tree. An initial comparison may be made using Friedman's analysis of variance test, followed by Tuckey's highly significant difference (HSD) test. These tests will indicate the variables that most influence wood density. This wood density may then be modeled based on these variables [@henry10].

\begin{filrouge}{Individual wood density-dependent biomass model}{fidens}%
In red line \ref{fdens}, wood density $\rho$ was defined for the species by calculating the mean of individual densities for trees in the same species. Let us now fit a biomass model based on individual wood density measurements in order to take account of inter-individual variations in wood density in a given species. The fitted model is:
\[
\ln(B)=a_0+a_1\ln(D)+a_2\ln(\rho)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
where $\rho$ unlike in red line \ref{fdens}, is here the *individual* measurement of wood density. The model is fitted by the command:
\R{m <- lm(log(Btot)$\sim$I(log(dbh))+I(log(dens)),data=dat[dat\$Btot>0,])%
\\ summary(m)}%
which yields:
\Rout{\begin{tabular}{lrrrrl}%
             & Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)  & -7.76644 &    0.20618 & -37.668 &  < 2e-16 & ***\\ %
I(log(dbh))  &  2.35272 &    0.04812 &  48.889 &  < 2e-16 & ***\\ %
I(log(dens)) &  1.00717 &    0.14053 &   7.167 & 1.46e-08 & ***\\ %
\end{tabular}}%
with a residual standard deviation of \decimal{0}{3052} and $R^2=\decimal{0}{9848}$. The model is written:
$\ln(B)=-\decimal{7}{76644}+\decimal{2}{35272}\ln(D)+\decimal{1}{00717}\ln(\rho)$.
According to this model, biomass is dependent upon individual wood density by the term $\rho^{1.00717}$, i.e. practically $\rho$. By way of a comparison, model (\ref{dens}) was dependent upon specific wood density by the term $\rho^{0.72864}$. From a biological standpoint, the exponent 1.00717 is more satisfactory than the exponent 0.72864 since it means that the biomass is the product of a volume (that depends solely on tree dimensions) and a density. The difference between the two exponents may be attributed to inter-individual variations in wood density within the species. But the model based on individual wood density has little practical usefulness as it would require a wood density measurement in every tree whose biomass we are seeking to predict.
\end{filrouge}

\subsection{Tree compartments\label{cmpt}}

Tree biomass is determined separately for each compartment in the tree (stump, trunk, large branches, small branches, leaves, etc.). Total above-ground biomass is the sum of these compartments. The approach already presented for fitting a model could be followed for each compartment separately. In this case we would construct a model for leaf biomass, a model for the biomass of the large branches, etc.. This approach integrates dataset stratification. Thus, we could initially fit a model for each compartment and each stratum; as a second step, and depending on the differences found between strata, we could aggregate the strata and/or parameterize the model in order to construct a model by compartment for all the strata. But the approach would not end there. We could also continue the data integration in order to move toward a smaller number of more integrating models.

#### Compartment additivity

As total above-ground biomass is the sum of the biomasses of the different compartments, it could be imagined that the best model for predicting above-ground biomass is the sum of the models predicting the biomass of each compartment. In fact, because of the correlations between the biomasses of the different compartment, this is not the case [@cunia84; @cunia85b; @parresol99]. Also, certain model families are not stable by addition. This in particular is the case for power models: the sum of two power functions is not a power function. If we have fitted a power model for each compartment:
\begin{eqnarray*}
B^{\mathrm{stump}} &=& a_1D^{b_1}
\\ B^{\mathrm{trunk}} &=& a_2D^{b_2}
\\ B^{\mathrm{large\ branches}} &=& a_3D^{b_3}
\\ B^{\mathrm{small\ branches}} &=& a_4D^{b_4}
\\ B^{\mathrm{leaves}} &=& a_5D^{b_5}
\end{eqnarray*}
The sum $B^{\mbox{\scriptsize above-ground}}=B^{\mathrm{stump}}
+B^{\mathrm{trunk}}+B^{\mathrm{large\ branches}}
+B^{\mathrm{small\ branches}}+B^{\mathrm{leaves}}$ =
$\sum_{m=1}^5a_m$ $D^{b_m}$ is not a power function of dbh. By contrast, polynomial models are stable by addition.

#### Fitting a multivariate model

In order to take account of the correlations between the biomasses of the different compartments, we can fit the models relative to the different compartments in a simultaneous manner rather than separately. This last step in model integration requires us to redefine the response variable. As we are looking to predict simultaneously the biomasses of different compartments, we are no longer dealing with a response variable but a *response vector* $\mathbf{Y}$. The length of this vector is equal to the number $M$ of compartments. For example, if the response variable is the biomass,
\[
\mathbf{Y}=\left[
\begin{array}{l}
B^{\mbox{\scriptsize above-ground}}\\ %
B^{\mathrm{stump}}\\ %
B^{\mathrm{trunk}}\\ %
B^{\mathrm{large\ branches}}\\ %
B^{\mathrm{small\ branches}}\\ %
B^{\mathrm{leaves}}
\end{array}
\right]
\]
If the response variable is the log of the biomass,
\[
\mathbf{Y}=\left[
\begin{array}{l}
\ln(B^{\mbox{\scriptsize above-ground}})\\ %
\ln(B^{\mathrm{stump}})\\ %
\ln(B^{\mathrm{trunk}})\\ %
\ln(B^{\mathrm{large\ branches}})\\ %
\ln(B^{\mathrm{small\ branches}})\\ %
\ln(B^{\mathrm{leaves}})
\end{array}
\right]
\]
Let $Y_m$ be the response variable of the $m$th compartment
(where $m=1$, \ldots, $M$). Without loss of generality, we can consider that all the compartments have the same set $X_1$, $X_2$, \ldots, $X_p$ of effect variables (if a variable is not included in the prediction of a compartment, the corresponding coefficient can simply be set at zero). A model that predicts a response vector rather than a response variable is a multivariate model. An observation used to fit a multivariate model consists of a vector ($Y_1$, \ldots, $Y_M$, $X_1$, \ldots, $X_p$) of length $M+p$. The residual of a multivariate model is a vector $\boldsymbol{\varepsilon}$ of length $M$, equal to the difference between the observed response vector and the predicted response vector.

The expression of an $M$-variate model differs from the $M$ univariate models corresponding to the different compartments only by the structure of the residual error; the structure of the mean model does not change. Let us consider the general case of a non-linear model. If the $M$ univariate models are:
\begin{equation}
Y_m=f_m(X_1,\ \ldots,\ X_p;\theta_m)+\varepsilon_m(\#eq:uvar)
\end{equation}
for $m=1$, \ldots, $M$, then the multivariate model is written:
\[
\mathbf{Y}=\mathbf{F}(X_1,\ \ldots,\ X_p;\boldsymbol{\theta})+
\boldsymbol{\varepsilon}
\]
where $\mathbf{Y}={}^{\mathrm{t}}{[Y_1,\ \ldots,\ Y_M]}$,
$\boldsymbol{\theta}={}^{\mathrm{t}}{[\theta_1,\ \ldots,\ \theta_M]}$, and
\begin{equation}
\mathbf{F}(X_1,\ \ldots,\ X_p;\boldsymbol{\theta})=\left[
\begin{array}{c}
f_1(X_1,\ \ldots,\ X_p;\theta_1)
\\\vdots\\
f_m(X_1,\ \ldots,\ X_p;\theta_m)
\\\vdots\\
f_M(X_1,\ \ldots,\ X_p;\theta_M)
\end{array}
\right](\#eq:mvar)
\end{equation}
The residual vector $\boldsymbol{\varepsilon}$ now follows a centered multinormal distribution, with a variance-covariance matrix of:
\[
\mathrm{Var}(\boldsymbol{\varepsilon})\equiv\boldsymbol{\Sigma}
=\left[
\begin{array}{cccc}
\sigma_1^2 & \zeta_{12} & \cdots        & \zeta_{1M}\\ %
\zeta_{21} & \sigma_2^2 & \ddots        & \vdots\\ %
\vdots     & \ddots     & \ddots        & \zeta_{M-1,M}\\ %
\zeta_{M1} & \cdots     & \zeta_{M,M-1} & \sigma_M^2
\end{array}
\right]
\]
Matrix $\boldsymbol{\Sigma}$ is symmetrical with $M$ lines and $M$ columns, such that $\sigma_m^2=\mathrm{Var}(\varepsilon_m)$ is the residual variance of the biomass of the $m$th compartment and $\zeta_{ml}=\zeta_{lm}$ is the residual covariance between the biomass of the $m$th compartment and that of the $l$th compartment. Like in the univariate case, two residuals corresponding to two different observations are assumed to be independent: $\boldsymbol{\varepsilon}_i$ is independent of $\boldsymbol{\varepsilon}_j$ pour $i\neq j$. The difference arises from the fact that the different compartments are no longer assumed to be independent one from another.

A multivariate model such as (\ref{mvar}) is fitted based on the same principles as univariate models (\ref{uvar}). If the variance-covariance matrix $\boldsymbol{\Sigma}$ is diagonal (i.e. $\zeta_{ml}=0$, $\forall m$, $l$), then the fitting of the multivariate model (\ref{mvar}) is equivalent to the separate fitting of the $M$ univariate models (\ref{uvar}). In the case of a linear model, estimated values for coefficients $\theta_1$, $\theta_2$, \ldots, $\theta_M$ resulting from fitting the $M$-variate linear model are the same as the values obtained by the separate fitting of the $M$ univariate linear models (on condition that the same effect variables $X_1$, \ldots, $X_p$ are used in all cases) \citep[chapter 3]{muller06}. But the significance tests associated with the coefficients do not give the same results in the two cases. If the different compartments are sufficiently correlated one with the other, the simultaneous fitting of all the compartments by the multivariate model (\ref{mvar}) will yield a more precise estimation of model coefficients, and therefore more precise biomass predictions.

#### Harmonizing a model

In certain cases, particularly energy wood, we are looking to predict the dry biomass of the trunk at different cross-cut diameters. For instance, we want to predict simultaneously the total biomass $B$ of the trunk, the biomass $B_{7}$ of the trunk to the small-end diameter of 7~cm, and the biomass $B_{10}$ of the trunk to the small-end diameter of 10~cm. We could consider the entire trunk, the trunk to a cross-cut of 7~cm, and the trunk to a cross-cut of 10~cm as three different compartments and apply the same fitting principles as presented in the previous section. In fact, the problem is more complex because, unlike the trunk and leaf compartments which are separate, the compartments defined by different cross-cut diameters are nested one within the others: $B=B_{7}+$ biomass of the section to a small-end diameter of 7~cm, and $B_{7}=B_{10}+$ biomass of the section from the 10~cm to the 7~cm small-end cross-cuts. Thus, the multivariate model that predicts vector ($B$, $B_7$, $B_{10}$) must ensure that $B>B_{7}>B_{10}$ across the entire valid range of the model. The process consisting of constraining the multivariate model in such a manner that it predicts the biomasses of the different compartments while checking the logic of their nesting is called model *harmonization* [@parresol99]. @jacobs80 and @cunia85 have put forward solutions to this problem in the form of equations relating the coefficients in the models used for the different compartments. In this case we must fit an $M$-variate model (if there are $M$ cross-cut diameters) while ensuring that the coefficients $\theta_1$, \ldots, $\theta_M$ corresponding to the $M$ cross-cut diameters satisfy a number of equations linking them together. When the coefficients in the multivariate model are estimated by maximum likelihood, their numerical estimation is in fact a problem of constrained optimization.

If predicting the volume or biomass of a stem, an alternative to the volume or biomass model is to use stem profile integration [@parresol89; @parresol99]. Let $P(h)$ be a stem profile, i.e. a plot of trunk transversal section area against height $h$ from the ground ($h$ also represents the length when a stem is followed from its large to its small end) [@maguire96; @dean06; @metcalf09]. If the section through the stem is roughly circular, the diameter of the tree at height $h$ may be calculated as: $D(h)=\sqrt{4P(h)/\pi}$. The biomass of the trunk to a cross-cut diameter $D$ is calculated by integrating the stem profile from the ground ($h=0$) to height $P^{-1}(\frac{\pi}{4}D^2)$ corresponds to this diameter:
\[
B_D=\int_0^{P^{-1}(\frac{\pi}{4}D^2)}\rho(h)\;P(h)\ \mathrm{d}h
\]
where $\rho(h)$ is wood density at height $h$. Stem volume to cross-cut diameter $D$ is calculated in the same manner, except that $\rho$ is replaced by 1. The stem profile approach has the advantage that model harmonization here is automatic. But it is an approach that is conceptually different from volume and biomass models, with specific fitting problems [@fang99; @parresol99], and is outside the scope of this guide. It should be noted that when dealing with very large trees, for which it is almost impossible to measure biomass directly, the stem profile approach is a relevant alternative [@vanpelt01; @dean03; @dean03b; @dean06; @sillett10].

