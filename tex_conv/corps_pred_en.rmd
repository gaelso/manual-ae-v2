




If we want to predict the volume or biomass of a stand using biomass tables, we cannot measure model entries for all the trees in the stand, only for a sample. The volume or biomass of the trees in this sample will be calculated using the model, then extrapolated to the entire stand. Predicting the volume or biomass of a stand therefore includes two sources of variability: one related to the model's individual prediction, the other to the sampling of the trees in the stand. If we rigorously take account of these two sources of variability when making predictions on a stand scale, this will cause complex double sampling problems as mentioned in sections \ref{peup} and \ref{sampeup} [@parresol99].

The problem is slightly less complex when the sample of trees used to construct the model is independent of the sample of trees in which the entries were measured. In this case we can consider that the prediction error stemming from the model is independent of the sampling error. Let us assume that $n$ sampling plots of unit area $A$ were selected in a stand whose total area is $\mathcal{A}$. Let $N_i$ be the number of trees found in the 
$i$th plot ($i=1$, \ldots, $n$) and let
$X_{ij1}$, \ldots, $X_{ijp}$ be the $p$ effect variables measured on the $j$th tree of the $i$th
plot ($j=1$, \ldots, $N_i$). @cunia65; @cunia87c considered the particular case where biomass is predicted by multiple regression based on the $p$ effect variables. The estimation of stand biomass is in this case:
\begin{eqnarray*}
\hat{B} &=&
\frac{\mathcal{A}}{n}\sum_{i=1}^n\frac{1}{A}\sum_{j=1}^{N_i}
(\hat{a}_0+\hat{a}_1X_{ij1}+\ldots+\hat{a}_pX_{ijp})
\\ &=& \hat{a}_0\Big(\frac{\mathcal{A}}{nA}\sum_{i=1}^nN_i\Big)
+\hat{a}_1\Big(\frac{\mathcal{A}}{nA}\sum_{i=1}^n\sum_{j=1}^{N_i}X_{ij1}\Big)
+\ldots+\hat{a}_p\Big(\frac{\mathcal{A}}{nA}\sum_{i=1}^n\sum_{j=1}^{N_i}X_{ijp}\Big)
\end{eqnarray*}
where $\hat{a}_0$, \ldots, $\hat{a}_p$ are the estimated coefficients of the regression. If 
$X^{\star}_0=(\mathcal{A}/nA)\sum_{i=1}^nN_i$ and for each $k=1$,
\ldots, $p$,
\[
X^{\star}_k=\frac{\mathcal{A}}{nA}\sum_{i=1}^n\sum_{j=1}^{N_i}X_{ijk}
\]
The estimated biomass of the stand is therefore
\[
\hat{B}=\hat{a}_0X^{\star}_0+\hat{a}_1X^{\star}_1+\ldots+\hat{a}_pX^{\star}_p
\]
Interestingly, the variability of 
$\hat{\mathbf{a}}={}^{\mathrm{t}}[\hat{a}_0,\ \ldots,\ \hat{a}_p]}$ depends entirely on model fitting, not on stand sampling, while on the other hand, the variability of 
$\mathbf{x}={}^{\mathrm{t}}[X^{\star}_0,\ \ldots,\ X^{\star}_p]}$ depends entirely on the sampling, not on the model. Given that these two errors are independent,
\[
\mathrm{E}(\hat{B})=\mathrm{E}({}^{\mathrm{t}}\hat{\mathbf{a}}}\mathbf{x})
={}^{\mathrm{t}}\mathrm{E}(\hat{\mathbf{a}})}\ \mathrm{E}(\mathbf{x})
\]
and
\[
\mathrm{Var}(\hat{B})={}^{\mathrm{t}}\mathbf{a}}\boldsymbol{\Sigma}_{\mathbf{x}}
\mathbf{a}+{}^{\mathrm{t}}\mathbf{x}}\boldsymbol{\Sigma}_{\mathbf{a}}\mathbf{x}
\]
where $\boldsymbol{\Sigma}_{\mathbf{a}}$ is the
$(p+1)\times(p+1)$ variance-covariance matrix of the model's coefficients while $\boldsymbol{\Sigma}_{\mathbf{x}}$ is the
$(p+1)\times(p+1)$ variance-covariance matrix of the sample of 
$\mathbf{x}$. The first matrix is derived from model fitting whereas the second is derived from stand sampling. Thus, the error for the biomass prediction of a stand is the sum of these two terms, one of which is related to the prediction error of the model and the other to stand sampling error.

More generally, the principle is exactly the same as when we considered on page \pageref{errmes}, an uncertainty related to the measurement of the effect variables $X_1$, \ldots, $X_p$. A measurement error is not of the same nature as a sampling error. But from a mathematical standpoint, the calculations are the same: in both cases this means considering that the effect variables $X_1$, \ldots, $X_p$
are random, not fixed. We may therefore, in this general case, use a Monte Carlo method to estimate stand biomass. The pseudo-algorithm of this Monte Carlo method is the same as that used previously (see p.\pageref{psa}):
\begin{enumerate}
\item For $k$ from $1$ to $Q$, where $Q$ is the number of Monte Carlo iterations:
    \begin{enumerate}
    \item for $i$ from 1 to $p$, draw $\hat{X}_i^{(k)}$ following a distribution corresponding to the variability of the stand sample (this distribution depends on the type of sampling conducted, the size and number of plots inventoried, etc.);
    \item draw a vector $\hat{\theta}^{(k)}$ following a multinormal distribution of mean $\hat{\theta}$ and variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$;
    \item calculate the prediction 
    $\hat{Y}^{(k)}=f(\hat{X}_1^{(k)},\ \ldots,\ \hat{X}_p^{(k)};
    \hat{\theta}^{(k)})$.
    \end{enumerate}
\item The confidence interval of the prediction is the empirical confidence interval of the $Q$ values $\hat{Y}^{(1)}$, \ldots,
$\hat{Y}^{(Q)}$.
\end{enumerate}

\section{Expanding and converting volume and biomass models\label{BEF}}

It may happen that we have a model which predicts a variable that is not exactly the one we need, but is closely related to it. For example, we may have a model that predicts the dry biomass of the trunk, but we are looking to predict the total above-ground biomass of the tree. Or, we may have a model that predicts the volume of the trunk, but we are looking to predict its dry biomass. Rather than deciding not to use a model simply because it does not give us exactly what we want, we can use it by default and adjust it using a factor. We can use 
*conversion* factors to convert volume to biomass (and 
*vice versa*), *expansion* factors to extrapolate a part to the whole, or combinations of the two. Using this approach 
@henry11 put forward three methods to obtain the total biomass:
\begin{itemize}
\item The biomass of the trunk is the product of its volume and specific wood density $\rho$;
\item above-ground biomass is the product of trunk biomass and a biomass expansion factor (BEF);
\item above-ground biomass is the product of trunk volume and a biomass expansion and conversion factor (BCEF = BEF$\times\rho$).
\end{itemize}
Tables of these different conversion and expansion factors are available. These values are often very variable as they implicitly include difference sources of variability. Although the default table may be very precise, we often loose the benefit of this precision when using an expansion or correction factor as the prediction error cumulates all the sources of error involved in its calculation.

If we have a table that uses height as entry value, but do not have any height information, we may use a corollary model that predicts height from the entry values we do have (typically a height-dbh relation). Like for conversion and expansion factors, this introduces an additional source of error.

\section{Arbitrating between different models\label{arbi}}

If we want to predict the volume or biomass of given trees, we often have a choice between different models. For instance, several models may have been fitted in different places for a given species. Or we may have a local model and a pan-tropical model. Arbitrating between the different models available is not always an easy task [@henry11].
For example, should we choose a specific, local model fitted using little data (and therefore unbiased *a priori* but with high prediction variability) or a pan-tropical, multispecific model fitted with a host of data (therefore potentially biased but with low prediction variability)? This shows how many criteria are potentially involved when making a choice: model quality (the range of its validity, its capacity to extrapolate predictions, etc.), its specificity (with local, monospecific models at one extreme and pan-tropical, multispecific models at the other), the size of the dataset used for model fitting (and therefore implicitly the variability of its predictions). Arbitrating between different models should not be confused with model selection, as described in section \@ref(selmod) for when selecting models their coefficients are not yet known, and when we estimate these coefficients we are looking for the model that best fits the data. When arbitrating between models we are dealing with models already fitted and whose coefficients are known.

Arbitration between different models often takes place without any biomass or volume data. But the case we will focus on here is when we have a reference dataset $\mathcal{S}_n$, with $n$ observations of the response variable (volume of biomass) and the effect variables.

### Comparing using validation criteria

When we have a dataset 
$\mathcal{S}_n$, we can compare the different models available on the basis of the validation criteria described in section \@ref(Ival), by using $\mathcal{S}_n$ as the validation dataset. Given that the models do not necessarily have the same number $p$ of parameters, and in line with the parsimony principle, we should prefer the validation criteria that depend on $p$ in such a manner as to penalize those models with many parameters.

When comparing a particular, supposedly "best" candidate model with different competitors, we can compare its predictions with those of its competitors. To do this we look to see whether or not the predictions made by its competitors are contained within the confidence interval at a significance level of $\alpha$ of the candidate model.

### Choosing a model

A model may be chosen with respect to a "true"
model $f$ that although unknown may be assumed to exist. Let $M$ be the number of models available. Let us note as 
$\hat{f}_m$ the function of the $p$ effect variables that predicts volume or biomass based on the $m$th model. This function is random as it depends on coefficients that are estimated and therefore have their own distribution. The distribution of 
$\hat{f}_m$ therefore describes the variability of the predictions based on the model $m$, as described in section 
\ref{BVpred}. The $M$ models may have very different forms: the $\hat{f}_1$ model may correspond to a power function, the $\hat{f}_2$ model to a polynomial function, etc. We will also assume that there is a function $f$ of the $p$ effect variables that describes the
"true" relation between the response variable (volume or biomass) and these effect variables. We do not know this 
"true" relation. We do not know what form it has. But each of the $M$ models may be considered to be an approximation of the "true" relation $f$.

In the models selection theory [@massart07],
the difference between the relation $f$ and a model $\hat{f}_m$ is quantified by a function $\gamma$ that we call the *loss* function. For example, the loss function may be the norm $L^2$ of the difference between $f$ and $\hat{f}_m$:
\[
\gamma(f,\ \hat{f}_m)=\int_{x_1}\ldots\int_{x_p}
[f(x_1,\ \ldots,\ x_p)-\hat{f}_m(x_1,\ \ldots,\ x_p)]^2
\ \mathrm{d}x_1\ldots\mathrm{d}x_p
\]
The expectation of the loss with respect to the distribution of $\hat{f}_m$ is called the *risk* (written $R$) i.e. when integrating on the variability of model predictions:
\[
R=\mathrm{E}[\gamma(f,\ \hat{f}_m)]
\]
The best of the $M$ models available is that which minimizes the risk. The problem is that we do not know the true relation $f$, therefore, this "best" model is also unknown. In the models selection theory this best model is called an *oracle*. The model finally chosen will be that such that the risk of the oracle is bounded for a vast family of functions $f$. Intuitively, the model chosen is that where the difference between it and the "true" relation is limited whatever this "true" relation may be (within the limits of a range of realistic possibilities). We will not go into further detail here concerning this topic as it is outside the scope of this guide.

### Bayesian model averaging

Rather than choose one model from the $M$ available, with the danger of not choosing the "best", an alternative consists in combining the $M$ competitor models to form a new model. This is called "Bayesian model averaging" and although it has been very widely used for weather forecasting models [@raftery05; @furrer07; @berliner08; @smith09] it is little used for forestry models [@li08; @picard12].
Let
$\mathcal{S}_n=\{(Y_i,\ X_{i1},\ \ldots,\ X_{ip}),\ i=1,\ldots,p\}$
be a reference dataset with $n$ observations of the response variable $Y$ and the $p$ effect variables. Bayesian model averaging considers that the distribution of the response variable $Y$ is a mixture of $M$ distributions:
\[
g(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ g_m(Y|X_1,\ \ldots,\ X_p)
\]
where $g$ is the distribution density of $Y$, $g_m$ is the conditional distribution density of $Y$ with model $m$ being the "best", and $w_m$ is the weight of the $m$th model in the mixture, that we may interpret as the *a posteriori* probability that the 
$m$th model is the "best". The *a posteriori* $w_m$ probabilities are reflections of the quality of the fitting of the models to the data, and their sum is one:
$\sum_{m=1}^Mw_m=1$.

In the same manner as for the model selection mentioned in the previous section, Bayesian model averaging assumes that there is a "true" (though unknown) relation between the response variable and the $p$ effect variables, and that each model differs from this "true" relation according to a normal distribution of standard deviation $\sigma_m$. In other words, density $g_m$
is the density of the normal distribution of mean 
$f_m(x_1,\ \ldots,\ x_p)$ and standard deviation $\sigma_m$, where $f_m$ is the function of $p$ variables corresponding to the $m$th
model. Thus,
\[
g(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ \phi(Y;f_m(x_1,\ \ldots,\ x_p),\;
\sigma_m)
\]
where $\phi(\cdot;\mu,\ \sigma)$ is the probability density of the normal distribution of expectation $\mu$ and standard deviation $\sigma$. The model 
$f_{\mathrm{mean}}$ resulting from the combination of the $M$ competitor models is defined as the expectation of the mixture model, i.e.:
\[
f_{\mathrm{mean}}(X_1,\ \ldots,\ X_p)=\mbox{E}(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ 
f_m(X_1,\ \ldots,\ X_p)
\]
Thus, the model resulting from the combination of the $M$ competitor models is a weighted mean of these $M$ models, with the weight of model $m$ being the *a posteriori* probability that model $m$ is the best. We can also calculate the variance of the predictions made by model $f_{\mathrm{mean}}$
that is a combination of the $M$ competitor models:
\begin{eqnarray*}
\mbox{Var}(Y|X_1,\ \ldots,\ X_p) &=& \sum_{m=1}^Mw_m\Big[
f_m(X_1,\ \ldots,\ X_p)-\sum_{l=1}^Mw_l\ f_l(X_1,\ \ldots,\ X_p)\Big]^2
\\ && +\sum_{m=1}^Mw_m\ \sigma_m^2
\end{eqnarray*}
The first term corresponds to between-model variance and expresses prediction variability from one model to another; the second term corresponds to within-model variance, and expresses the conditional error of the prediction, with this model being the best.

If we want to use model $f_{\mathrm{mean}}$ instead of the $M$ models $f_1$, \ldots, $f_M$, we now have to estimate the weights
$w_1$, \ldots, $w_M$ and the within-model standard deviations $\sigma_1$,
\ldots, $\sigma_M$. These $2M$ parameters are estimated from reference dataset $\mathcal{S}_n$ using an EM algorithm [@dempster77; @mclachlan08]. The EM algorithm introduces latent variables $z_{im}$ such that $z_{im}$ is the *a posteriori* probability that model $m$ is the best model for observation $i$ of $\mathcal{S}_n$. The latent variables $z_{im}$ take values of between 0 and 1.
The EM algorithm is iterative and alternates between two steps at each iteration: step E (as in "expectation") and step M (as "maximisation"). The EM algorithms is as follows:
\begin{enumerate}
\item Choose start values $w^{(0)}_1$, \ldots, $w^{(0)}_M$,
$\sigma^{(0)}_1$, \ldots, $\sigma^{(0)}_M$ for the $2M$ parameters to be estimated.
\item Alternate the two steps:
    \begin{enumerate}
    \item step E: calculate the value of $z_{im}$ at iteration 
    $j$ sbased on the values of the parameters at iteration $j-1$:
    \[
    z_{im}^{(j)}=\frac{w_m^{(j-1)}\ \phi[Y_i;f_m(X_{i1},\ \ldots,\ X_{ip}),
    \ \sigma_m^{(j-1)}]}{\sum_{k=1}^Mw_k^{(j-1)}\ \phi[Y_i;f_k(X_{i1},\ \ldots,\ X_{ip}),
    \ \sigma_k^{(j-1)}]}
    \]
    \item step M: estimate the parameters at iteration $j$ using current values of $z_{im}$ as weight, i.e.:
    \begin{eqnarray*}
    w_m^{(j)} &=& \frac{1}{n}\sum_{i=1}^nz_{im}^{(j)}\\ %
    {\sigma_m^{(j)}}^2 &=&
    \frac{\sum_{i=1}^nz_{im}^{(j)}[Y_i-f_m(X_{i1},\ \ldots,\ X_{ip})]^2}
    {\sum_{i=1}^nz_{im}^{(j)}}
    \end{eqnarray*}
    \end{enumerate}
\item[] such as $\sum_{m=1}^M|w_m^{(j)}-w_m^{(j-1)}|+
\sum_{m=1}^M|\sigma_m^{(j)}-\sigma_m^{(j-1)}|$ is larger than a fixed infitesimal threshold (for example $10^{-6}$).
\item The estimated value of $w_m$ is $w_m^{(j)}$ and the estimated value of $\sigma_m$ is $\sigma_m^{(j)}$.
\end{enumerate}
