

## Predicting the volume or biomass of a tree {#BVpred}


Making a prediction based on model $f$ consists in using the known values of the effect variables $X_1,\ \ldots,\ X_p$, to calculate the predicted value $\hat{Y}$. A prediction does not end with the calculation of
\[
\hat{Y}=f(X_1,\ \ldots,\ X_p;\hat{\theta})
\]
The estimator $\hat{\theta}$ of model parameters is a random vector whose distribution stems from the distribution of the observations used to fit the model. Any model prediction $\hat{Y}$ is therefore itself a random variable whose distribution stems from the distribution of the observations used to fit the model. If we are to express this intrinsic variability of the prediction, we must associate it with an uncertainty indicator such as the standard deviation of the prediction or its 95\ \% confidence interval.

Several confidence intervals are available depending on whether we are predicting the volume or biomass of a tree selected at random in the stand, or the volume or biomass of the mean tree in the stand. We will now describe in detail the analytical expressions of these confidence intervals for a linear model (`r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(plm)), then for a non-linear model (`r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(pnls)). We will then present expressions for these confidence intervals that are similar but simpler to compute (`r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(papp), before focusing on the case of transformed variables (`r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(invtra)).



### Prediction: linear model {#plm}


#### Prediction by simple linear regression {-}

Let $\hat{a}$ be the estimated y-intercept of a linear regression, and $\hat{b}$ be its estimated slope. The prediction of $\hat{Y}$ the response variable may be written in two different ways:
\begin{eqnarray}
\hat{Y}&=&\hat{a}+\hat{b}X(\#eq:Ypeup)\\ %
\hat{Y}&=&\hat{a}+\hat{b}X+\varepsilon(\#eq:Yind)
\end{eqnarray}
In both cases the expectation of $\hat{Y}$ is the same as $\mathrm{E}(\varepsilon)=0$. By contrast, the variance of $\hat{Y}$
is not the same: it is higher in the second than in the first. These two equations may be interpreted as follows. Let us assume that effect variable $X$ is dbh and that the response variable $Y$ is the biomass. The number of trees in the whole forest with a given dbh $X$ (to within measurements errors) is immeasurably huge. If we were able to measure the biomass of all these trees with the same dbh, we would obtain variable values oscillating around a certain mean value. If we seek to predict this mean biomass (i.e. the mean of all the trees with dbh $X$), then equation \@ref(eq:Ypeup) of the prediction is valid. By contrast, if we seek to predict the biomass of a tree selected at random from all the trees with dbh $X$, then equation \@ref(eq:Yind) of the prediction is valid. The variability of the prediction is greater for \@ref(eq:Yind) than for \@ref(eq:Ypeup) because here the variability of the mean biomass prediction is supplemented by the differences in biomass between the trees.

This means that two methods can be used to calculate a confidence interval for a prediction. There is a confidence interval for the prediction of the mean of $Y$, and a confidence interval for the prediction of an individual selected at random from the population in which the mean of $Y$ was calculated. The second confidence interval is wider than the first.

In the case of a simple linear regression, it can be shown [@saporta90, p.373--374] that the confidence interval at a significance level of $\alpha$ for predicting the mean \@ref(eq:Ypeup) is:
\begin{equation}
\hat{a}+\hat{b}X\pm
t_{n-2}\ \hat{\sigma}\sqrt{\frac{1}{n}+\frac{(X-\bar{X})^2}{nS_X^2}}
(\#eq:icpeup)
\end{equation}
while the confidence interval at a significance level of $\alpha$ for predicting a tree selected at random \@ref(eq:Yind) is:
\begin{equation}
\hat{a}+\hat{b}X\pm
t_{n-2}\ \hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(X-\bar{X})^2}{nS_X^2}}
(\#eq:icind)
\end{equation}
where $t_{n-2}$ is the quantile $1-\alpha/2$ of a Student's distribution with $n-2$ degrees of freedom, $\bar{X}=(\sum_{i=1}^nX_i)/n$ is the mean of the observed values of $X$ in the dataset that served to fit the model, and $S_X^2=[\sum_{i=1}^n(X_i-\bar{X})^2]/n$ is the empirical variance of the observed values of $X$ in the dataset that served to fit the model.

These expressions call for several remarks. The first is that the difference between the bounds of confidence interval \@ref(eq:icind) for a tree selected at random and the bounds of confidence interval \@ref(eq:icpeup) for the mean tree is approximately $t_{n-2}\hat{\sigma}$. This difference reflects the difference between equations \@ref(eq:Yind) and \@ref(eq:Ypeup), which is due to the residual term $\varepsilon$ whose estimated standard deviation is $\hat{\sigma}$.

The second is that the width of the confidence interval is not constant, but varies with $X$. The confidence interval is narrowest when $X=\bar{X}$ but widens as $X$ moves away from $\bar{X}$.

The third remark is that in order to calculate the confidence interval of a prediction based on a linear regression, we must be in possession of --- if not the original data that served to fit the model --- at least the mean $\bar{X}$ of the effect variable and its empirical standard deviation $S_X$. If the original data that served to fit the model are not available, and the values of $\bar{X}$ and $S_X$ have not been documented, we cannot accurately calculate the confidence interval.


::::::{.filrouge data-latex=""}

(@eq-ficlnD)

:::{.exercise #ficlnD}
(ref:ficlnD)
:::

Let us return to the simple linear regression between $\ln(B)$ and $\ln(D)$ that was fitted in red line \@ref(exr:rllnBvD). Let
`m` be the object containing the fitted model (see red line \@ref(exr:rllnBvD)). The confidence intervals may be calculated using the `predict` command. For example, for a tree with a dbh of 20~cm, the 95\ \% confidence interval for the mean tree is obtained by the command:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ I(log(dbh)), data = dat[dat$Btot > 0,]) ## red line 7
predict(m, newdata = data.frame(dbh = 20), interval = "confidence", level = 0.95)
```

which yields:

```{r}
conf <- predict(m, newdata = data.frame(dbh = 20), interval = "confidence", level = 0.95)
conf
```

Thus, the fitted model is $\ln(B)=$ `r conf[1]` with a 95\ \% confidence interval of `r conf[2]` to `r conf[3]`. For a tree with a dbh of 20\ cm selected at random, the confidence interval may be obtained by the command:

```{r, echo=T, results='hide'}
predict(
    m, newdata = data.frame(dbh = 20), interval = "prediction", level = 0.95
    )
```

which yields:

```{r}
pred <- predict(m,newdata=data.frame(dbh=20),interval="prediction",level=0.95)
pred
```

Figure \@ref(fig:icD) shows the confidence intervals for the entire range of the data.

::::::


```{r icD, out.width=one_col, fig.cap="(ref:icd)"}

display_fig("iclnD")

```


#### Prediction by multiple regression {-}

The principles laid down in the case of the linear regression also apply to the multiple regression. The confidence interval can be expressed in two different ways: one for the prediction of the mean tree, the other for the prediction of a tree selected at random.

In the case of a multiple regression with estimated coefficients $\hat{\mathbf{a}}={}^{\mathrm{t}}[\hat{a}_0,\ \hat{a}_1,\ \hat{a}_2,\ \ldots,\ \hat{a}_p]$, the predicted value $\hat{Y}$ of the response variable for a tree whose effect variables are $\mathbf{x}={}^{\mathrm{t}}[1,\ X_1,\ X_2,\ \ldots, X_p]$, is:
\[
\hat{Y}={}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}
\]
and the confidence interval (with a significance level of $\alpha$) of this prediction is [@saporta90, p.387]:

- for the prediction of the mean tree:
\begin{equation}
{}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}\pm t_{n-p-1}\ \hat{\sigma}
\sqrt{{}^{\mathrm{t}}{\mathbf{x}}({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\mathbf{x}}
(\#eq:iclm0)
\end{equation}

- for the prediction of a tree selected at random:
\begin{equation}
{}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}\pm t_{n-p-1}\ \hat{\sigma}
\sqrt{1+{}^{\mathrm{t}}{\mathbf{x}}({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\mathbf{x}}
(\#eq:iclm1)
\end{equation}

where $\mathbf{X}$ is is the design matrix constructed using the data that served to fit the multiple regression. In order to calculate the confidence interval of the predictions, we need to be in possession of --- if not the original data that served to fit the model --- at least the matrix $({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}$. It should be noted that the variance of the prediction in the case \@ref(eq:iclm1) of a tree selected at random is made up of two terms: a term $\hat{\sigma}^2$ representing the residual error and a term $\hat{\sigma}^2\;{}^{\mathrm{t}}{\mathbf{x}}({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\mathbf{x}$ representing the variability induced by the estimation of the model's coefficients. When estimating the mean tree, the first term disappears, only the second remains.


::::::{.filrouge data-latex=""}

(@eq-ficlnDH)

:::{.exercise #ficlnDH}
(ref:ficlnDH)
:::

Let us return to the multiple linear regression between $\ln(B)$, $\ln(D)$ and $\ln(H)$ that was fitted in red line \@ref(exr:flnDlnH). Let `m` be the object containing the fitted model (see red line \@ref(exr:flnDlnH)). The confidence intervals may be calculated using the `predict` command. For example, for a tree with a dbh of 20~cm and a height of 20\ m, the 95\ \% confidence interval for the mean tree is obtained by the command:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ I(log(dbh)) + I(log(heig)), data = dat[dat$Btot > 0,])
predict(
    m, newdata = data.frame(dbh = 20, heig = 20), 
    interval = "confidence", level = 0.95
    )
```

which yields:

```{r}
conf <- predict(m,newdata=data.frame(dbh=20,heig=20),interval="confidence",level=0.95)
conf
```

Thus, the model predicts $\ln(B)=$ `r conf[1]` with a 95\ \% confidence interval of `r conf[2]` to `r conf[3]`. For a tree with a dbh of 20\ cm  and a height of 20\ m selected at random, the confidence interval is obtained by the command:

```{r, echo=T, results='hide'}
predict(
    m, newdata = data.frame(dbh = 20, heig = 20), 
    interval = "prediction", level = 0.95
    )
```

which yields:

```{r}
pred <- predict(m,newdata=data.frame(dbh=20,heig=20),interval="prediction",level=0.95)
pred
```

::::::



### Prediction: case of a non-linear model {#pnls}

In the general case of a non-linear model as defined by
\[
Y=f(X_1,\ \ldots,\ X_p;\theta)+\varepsilon
\]
where
\[
\varepsilon\mathop{\sim}\mathcal{N}(0,\;kX_1^c)
\]
unlike the case of the linear model, there is no exact explicit expression for the confidence intervals of the predictions. But the $\delta$-method can be used to obtain an approximate (and asymptotically exact) expression of these confidence intervals [@serfling80]. As before, there are two confidence intervals:

- a confidence interval for the prediction of the mean tree:
\begin{equation}
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm t_{n-q}
\sqrt{{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\;
[\mathrm{d}_{\theta}f(\hat{\theta})]}(\#eq:icnl0)
\end{equation}

- a confidence interval for the prediction of a tree selected at random:
\begin{equation}
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm t_{n-q}
\sqrt{\hat{k}^2X_1^{2\hat{c}}+
{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\;
[\mathrm{d}_{\theta}f(\hat{\theta})]}(\#eq:icnl1)
\end{equation}

where $q$ is the number of coefficients in the model (i.e. the length of vector $\theta$), $\mathrm{d}_{\theta}f(\hat{\theta})$ is the value in $\theta=\hat{\theta}$ of the differential of $f$ respect to model coefficients, and $\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}$ is an estimation in $\theta=\hat{\theta}$ of the variance-covariance matrix $\boldsymbol{\Sigma}_{\theta}$ of the estimator of $\theta$. The differential of $f$ with respect to model coefficients is the vector of length $q$:
\[
\mathrm{d}_{\theta}f(\theta)={}^{\mathrm{t}}{\bigg[\bigg(\frac{\partial
f(X_1,\ \ldots,\ X_p;\theta)}{\partial\theta_1}\bigg),\; \ldots,\;
\bigg(\frac{\partial
f(X_1,\ \ldots,\ X_p;\theta)}{\partial\theta_q}\bigg)\bigg]}
\]
where $\theta_i$ is the $i$th element of vector $\theta$. If using the estimator of the maximum likelihood of $\theta$, we can show that asymptotically, when $n\rightarrow\infty$ [@saporta90, p.301]:
\[
\boldsymbol{\Sigma}_{\theta}\ \mathop{\sim}_{n\rightarrow\infty}\
\mathbf{I}_n(\theta)^{-1}=\frac{1}{n}\; \mathbf{I}_1(\theta)^{-1}
\]
where $\mathbf{I}_n(\theta)$ is the Fisher information matrix provided by a sample of size $n$ on the vector of parameters $\theta$. This Fisher information matrix has $q$ lines and $q$ columns and is calculated from the second derivative of the log-likelihood of the sample:
\[
\mathbf{I}_n(\theta)=-\mathrm{E}\bigg[\frac{\partial^2\mathcal{L}(\theta)}
{\partial\theta^2}\bigg]
\]
An approximate estimation of the variance-covariance matrix of the parameters is therefore:
\[
\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}=-\bigg[\bigg(\frac{\partial^2\mathcal{L}(\theta)}
{\partial\theta^2}\bigg)\bigg|_{\theta=\hat{\theta}}\bigg]^{-1}
\]
In practice, the algorithm that numerically optimizes the log-likelihood of the sample at the same time gives a numerical estimation of the second derivative $(\partial^2\mathcal{L}/\partial\theta^2)$. This provides an immediate numerical estimation of $\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}$.

As before, the variance of the predictions in the case \@ref(eq:icnl1) of a tree selected at random is made up of two terms: a term $(\hat{k}X_1^{\hat{c}})^2$ representing the residual error and a term 
${}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\;[\mathrm{d}_{\theta}f(\hat{\theta})]$ representing the variability induced by the estimation of the model's coefficients. When estimating the mean tree, the first term disappears, only the second remains.



### Approximated confidence intervals {#papp}

The exact calculation of prediction confidence intervals requires information (matrix $\mathbf{X}$ for a linear model, variance-covariance matrix $\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}$ for a non-linear model) that is only rarely given in papers on volume or biomass tables. Generally, these papers provide only the number $n$ of observations used to fit the model and the residual standard deviation $\hat{\sigma}$ (linear model) or $\hat{k}$ and $\hat{c}$ (non-linear model). Sometimes, even this basic information about the fitting is missing. If $\mathbf{X}$ (linear model) or 
$\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}$ (non-linear model) is not provided, it is impossible to use the formulas given above to calculate confidence intervals. In this case we must use an approximate method.


#### Residual error alone {-}

Very often, only the residual standard deviation $\hat{\sigma}$ (linear model) or $\hat{k}$ and $\hat{c}$ (non-linear model) is given. In this case, an approximate confidence interval with a significance level of $\alpha$ may be determined:

- in the case of a linear regression:
\begin{equation}
(a_0+a_1X_1+\ldots+a_pX_p)\pm q_{1-\alpha/2}\ \hat{\sigma}
(\#eq:iclm2)
\end{equation}

- in the case of a non-linear model:
\begin{equation}
f(X_1,\ \ldots,\ X_p;\theta)\pm
q_{1-\alpha/2}\ \hat{k}X_1^{\hat{c}} (\#eq:icnl2)
\end{equation}

where $q_{1-\alpha/2}$ is the quantile $1-\alpha/2$ of the standard normal distribution. This confidence interval is a direct retranscription of the relation  $Y=a_0+a_1X_1+\ldots+a_pX_p+\varepsilon$ with $\varepsilon\sim\mathcal{N}(0,\ \hat{\sigma})$ (linear case) or $Y=f(X_1,\ \ldots,\ X_p;\theta)+\varepsilon$ with $\varepsilon\sim\mathcal{N}(0,\ \hat{k}X_1^{\hat{c}})$ (non-linear case), where we have deliberately written the model's coefficients without a circumflex to underline that these are fixed values. These relations therefore assume implicatively that the model's coefficients are exactly known and that the only source of variability is the residual error. In other words, these approximate confidence intervals may be interpreted as follows: these confidence intervals \@ref(eq:iclm2) (linear case) and \@ref(eq:icnl2) (non-linear case) are those that would be obtained for the prediction of a tree selected at *random* if sample size was *infinite*. This is confirmed when $n\rightarrow\infty$, $t_{n-p-1}$ tends toward $q_{1-\alpha/2}$ and the matrix $({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}$ in \@ref(eq:iclm1) tends toward the null matrix (whose coefficients are all zero). Thus, confidence interval \@ref(eq:iclm2) is indeed the bound of confidence interval \@ref(eq:iclm1) when $n\rightarrow\infty$. The same may be said of \@ref(eq:icnl1) and \@ref(eq:icnl2).


#### Confidence interval for the mean tree {-}

If an estimation $\hat{\boldsymbol{\Sigma}}$ is given of the variance-covariance matrix of the parameters, a confidence interval at a significance level of $\alpha$ for the prediction of the mean tree corresponds to:

- for a linear model:
\begin{equation}
(\hat{a}_0+\hat{a}_1X_1+\ldots+\hat{a}_pX_p)\pm
q_{1-\alpha/2}\sqrt{{}^{\mathrm{t}}{\mathbf{x}}\hat{\boldsymbol{\Sigma}}\mathbf{x}}
(\#eq:iclm3)
\end{equation}
where $\mathbf{x}$ is the vector ${}^{\mathrm{t}}{[X_1,\ \ldots,\ X_p]}$,

- for a non-linear model:
\begin{equation}
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm q_{1-\alpha/2}
\sqrt{{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}\;
[\mathrm{d}_{\theta}f(\hat{\theta})]}(\#eq:icnl3)
\end{equation}

These confidence intervals consider that all the prediction's variability stems from the estimation of the model's coefficients. Also, these confidence intervals are a direct retranscription of the fact that the model's coefficients follow a multinormal distribution whose mean is their true value and that has a variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$ for in the linear case, if $\hat{\mathbf{a}}={}^{\mathrm{t}}{[\hat{a}_1,\ \ldots,\ \hat{a}_p]}$ follows a multinormal distribution of mean ${}^{\mathrm{t}}{[a_1,\ \ldots,\ a_p]}$ and variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$, then the linear combination ${}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}$ follows a normal distribution of mean ${}^{\mathrm{t}}{\mathbf{x}}\ \mathbf{a}$ and variance ${}^{\mathrm{t}}{\mathbf{x}}\hat{\boldsymbol{\Sigma}}\mathbf{x}$ [@saporta90, p.85].

In the case of a linear model, we can show that the variance-covariance matrix of the estimator of the model's coefficients is [@saporta90, p.380]: $\boldsymbol{\Sigma}=\sigma^2({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}$. Thus, an estimation of this variance-covariance matrix is: $\hat{\boldsymbol{\Sigma}}=\hat{\sigma}^2({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}$. By integrating this expression in \@ref(eq:iclm3), we create an expression similar to \@ref(eq:iclm0). Similarly, in the non-linear case, confidence interval \@ref(eq:icnl3) is an approximation of \@ref(eq:icnl0).

In the non-linear case \@ref(eq:icnl3), if we want to avoid calculating the partial derivatives of $f$, we could use the Monte Carlo method. This method is based on a simulation that consists in making $Q$ samplings of coefficients $\theta$ in accordance with a multinormal distribution of mean $\hat{\theta}$ and variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$, calculating the prediction for each of the simulated values, then calculating the empirical confidence interval of these $Q$ predictions. This method is known in the literature as providing "population prediction intervals" [@bolker08; @paine12]. The pseudo-algorithm is as follows:

1. For $k$ of $1$ to $Q$:
        a. draw a vector $\hat{\theta}^{(k)}$ following a multinormal distribution of mean $\hat{\theta}$ and variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$;
        b. calculate the prediction $\hat{Y}^{(k)}=f(X_1,\ \ldots,\ X_p;\hat{\theta}^{(k)})$.
1. The confidence interval of the prediction is the empirical confidence interval of the $Q$ values $\hat{Y}^{(1)},\ \ldots,\ \hat{Y}^{(Q)}$.

Very often we do not know the variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$, but we do have an estimation of the standard deviations of the coefficients. Let $\mathrm{Var}(\hat{a}_i)=\Sigma_i$ (linear case) or $\mathrm{Var}(\hat{\theta}_i)=\Sigma_i$ (non-linear case) be the variance of the model's $i$th coefficient. In this case we will ignore the correlation between the model's coefficients and will approximate the variance-covariance matrix of the coefficients by a diagonal matrix:
\[
\hat{\boldsymbol{\Sigma}}\approx\left[
\begin{array}{ccc}
\hat{\Sigma}_1 &        & \mathbf{0}\\ %
               & \ddots & \\ %
\mathbf{0}     &        & \hat{\Sigma}_p\\ %
\end{array}
\right]
\]


#### Confidence interval for a tree selected at random {-}

The error resulting from the estimation of the model's coefficients, as described in the last section, can be cumulated with the residual error described in the section before last, in order to construct a prediction confidence interval for a tree selected at random. These are the variances of the predictions that are added one to the other. The confidence interval with a significance level of $\alpha$ is therefore:

- for a linear model:
\[
(\hat{a}_0+\hat{a}_1X_1+\ldots+\hat{a}_pX_p)\pm
q_{1-\alpha/2}\sqrt{\hat{\sigma}^2+{}^{\mathrm{t}}{\mathbf{x}}\hat{\boldsymbol{\Sigma}}\mathbf{x}}
\]
which is an approximation of \@ref(eq:iclm1),

- for a non-linear model:
\[
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm q_{1-\alpha/2}
\sqrt{\hat{k}^2X_1^{2\hat{c}}+{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;
\hat{\boldsymbol{\Sigma}}\; [\mathrm{d}_{\theta}f(\hat{\theta})]}
\]
which is an approximation of \@ref(eq:icnl1).

As before, if we want to avoid a great many calculations, we could employ the Monte Carlo method, using the following pseudo-algorithm:

1. For $k$ of $1$ to $Q$:
        a. draw a vector $\hat{\theta}^{(k)}$ following a multinormal distribution of mean $\hat{\theta}$ and variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$;
        a. draw a residual $\hat{\varepsilon}^{(k)}$ following a centered normal distribution of standard deviation $\hat{\sigma}$ (linear case) or $\hat{k}X_1^{\hat{c}}$ (non-linear case);
        a. calculate the prediction $\hat{Y}^{(k)}=f(X_1,\ \ldots,\ X_p;\hat{\theta}^{(k)})+\hat{\varepsilon}^{(k)}$.

1. The confidence interval of the prediction is the empirical confidence interval of the $Q$ values $\hat{Y}^{(1)},\ \ldots,\ \hat{Y}^{(Q)}$.


#### Confidence interval with measurement uncertainties {.unnumbered #errmes}

Fitting volume and biomass models assumes that the effect variables $X_1,\ \ldots,\ X_p$ are known exactly. In actual fact, this hypothesis is only an approximation as these variables are measured and are therefore subject to measurement error. Warning: measurement error should not be confused with the residual error of the response variable: the first is related to the measuring instrument and in principle may be rendered as small as we wish by using increasingly precise measuring instruments. The second reflects the intrinsic biological variability between individuals. We can take account of the impact of measurement error on the prediction by including it in the prediction confidence interval. Thus, the effect variables $X_1,\ \ldots,\ X_p$
are no longer considered as fixed, but as being contained within a distribution. Typically, to predict the volume or biomass of a tree with characteristics $X_1,\ \ldots,\ X_p$, we will consider that the $i$th characteristic is distributed according to a normal distribution of mean $X_i$ and standard deviation $\tau_i$. Typically, if $X_i$ is a dbh, then $\tau_i$ will be 3--5~mm; if $X_i$ is a height, $\tau_i$ will be 3\ \% of $X_i$ for $X_i\leq15$~m and 1~m for $X_i>15$~m.

It is difficult to calculate an explicit expression for the prediction confidence interval when the effect variables are considered to be random since this means we must calculate the variances of the products of these random variables, some of which are correlated one with the other. The $\delta$-method offers an approximate analytical solution [@serfling80]. Or, more simply, we can again use a Monte Carlo method. In this case, the pseudo-algorithm becomes:`r ifelse(book_format == "latex", " \\label{psa}", "")`

1. For $k$ of $1$ to $Q$:
        a. for $i$ of 1 to $p$, draw $\hat{X}_i^{(k)}$ following a normal distribution of mean $X_i$ and standard deviation  $\tau_i$;
        a. draw a vector $\hat{\theta}^{(k)}$ following a multinormal distribution of mean $\hat{\theta}$ and variance-covariance matrix $\hat{\boldsymbol{\Sigma}}$;
        a. draw a residual $\hat{\varepsilon}^{(k)}$ following a centered normal distribution of standard deviation $\hat{\sigma}$ (linear case) or $\hat{k}X_1^{\hat{c}}$ (non-linear case);
        a. calculate the prediction $\hat{Y}^{(k)}=f(\hat{X}_1^{(k)},\ \ldots,\ \hat{X}_p^{(k)}; \hat{\theta}^{(k)})+\hat{\varepsilon}^{(k)}$.

1. The confidence interval of the prediction is the empirical confidence interval of the $Q$ values $\hat{Y}^{(1)},\ \ldots,\ \hat{Y}^{(Q)}$.

This confidence interval corresponds to the prediction for a tree selected at random. To obtain the confidence interval for the mean tree, simply apply the same pseudo-algorithm but replaced step (c) by:

1. \  
        a. ($\ldots$)
        a. ($\ldots$)
        a. with $\hat{\varepsilon}^{(k)}=0$;
        a. ($\ldots$)



### Inverse variables transformation {#invtra}

We saw in section \@ref(trans) how a variable transformation can render linear a model that initially did not satisfy the hypotheses of the linear model. Variable transformation affects both the mean and the residual error. The same may be said of the inverse transformation, with implications on the calculation of prediction expectation. The log transformation is that most commonly used, but other types of transformation are also available.


#### Log transformation {-}

Let us first consider the case of the log transformation of volume or biomass values, which is by far the most common case for volume and biomass models. Let us assume that a log transformation has been applied to biomass $B$ to fit a linear model using effect variables $X_1,\ \ldots,\ X_p$:
\begin{equation}
Y=\ln(B)=a_0+a_1X_1+\ldots+a_pX_p+\varepsilon(\#eq:pre)
\end{equation}
where
\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma)
\]
This is equivalent to saying that $\ln(B)$ follows a normal distribution of mean $a_0+a_1X_1+\ldots+a_pX_p$ and standard deviation $\sigma$ or of saying, by definition, that $B$ follows a log-normal distribution of parameters $a_0+a_1X_1+\ldots+a_pX_p$ and $\sigma$. The expectation of this log-normal distribution is:
\[
\mbox{E}(B)=\exp\Big(a_0+a_1X_1+\ldots+a_pX_p+\frac{\sigma^2}{2}\Big)
\]
Compared to the inverse model of \@ref(eq:pre) which corresponds to $B=\exp(a_0+a_1X_1+\ldots+a_pX_p)$, the inverse transformation of the residual error introduces bias into the prediction which can be corrected by multiplying the prediction 
$\exp(a_0+a_1X_1+\ldots+a_pX_p)$ by an correction factor [@parresol99]:
\begin{equation}
\mathrm{CF}=\exp\Big(\frac{\hat{\sigma}^2}{2}\Big)(\#eq:CF)
\end{equation}
We must however take care as the biomass models reported in the literature, and fitted after log transformation of the biomass, sometimes include this factor and sometimes do not.

If the decimal $\log_{10}$ has been used for variable transformation rather than the natural log, the correction factor is:
\[
\mathrm{CF}=\exp\Big[\frac{(\hat{\sigma}\ln 10)^2}{2}\Big]\approx
\exp\Big(\frac{\hat{\sigma}^2}{0.3772}\Big)
\]


::::::{.filrouge data-latex=""}

(@eq-CFB)

:::{.exercise #CFB}
(ref:CFB)
:::

Let us return to the biomass model in red line \@ref(exr:fdens) fitted by multiple regression on log-transformed data:
\[
\ln(B)=-8.38900+0.85715\ln(D^2H)+0.72864\ln(\rho)
\]
If we consider the starting data but use the exponential function (without taking account of the correction factor), we obtain an under-estimated prediction: $B=\exp(-8.38900)\times(D^H)^{0.85715}\rho^{0.72864}=2.274\times10^{-4}(D^2H)^{0.85715}\rho^{0.72864}$.
Let `m` be the object containing the fitted model (red line \@ref(exr:fdens). The adjustment factor $\mathrm{CF}=\exp(\hat{\sigma}^2/2)$ is obtained by the command:

```{r, echo=T, results='hide'}
dm <- tapply(dat$dens, dat$species, mean)
dat <- cbind(dat, dmoy = dm[as.character(dat$species)])
m <- lm(
    formula = log(Btot) ~ I(log(dbh^2 * heig)) + I(log(dmoy)), 
    data = dat[dat$Btot> 0 ,]
    )
exp(summary(m)$sigma^2 / 2)
```

and here is `r exp(summary(m)$sigma^2/2)`. The correct model is therefore: $B=2.412\times10^{-4}(D^2H)^{0.85715}\rho^{0.72864}$.

::::::


#### Any transformation {-}

In the general case, let $\psi$ be a variable transformation of the biomass (or volume) such that the response variable $Y=\psi(B)$ may be predicted by a linear regression against the effect variables $X_1,\ \ldots,\ X_p$. We shall assume that $\psi$ is derivable and invertible. As $\psi(B)$ follows a normal distribution of mean $a_0+a_1X_1+\ldots+a_pX_p$ and standard deviation $\sigma$, $B=\psi^{-1}[\psi(B)]$ has an expectation of [@saporta90, p.26]:
\begin{equation}
\mathrm{E}(B)=\int\psi^{-1}(x)\;\phi(x)\;\mathrm{d}x(\#eq:EB)
\end{equation}
where $\phi$ is the probability density of the normal distribution of mean $a_0+a_1X_1+\ldots+a_pX_p$ and standard deviation $\sigma$. This expectation is generally different from $\psi^{-1}(a_0+a_1X_1+\ldots +a_pX_p)$: the variable transformation induces prediction bias when we return to the starting variable by inverse transformation. The drawback of formula \@ref(eq:EB) is that it requires the calculation of an integral.

When the residual standard deviation $\sigma$ is small, the $\delta$-method [@serfling80] provides an approximate expression of this prediction bias:
\begin{eqnarray*}
\mathrm{E}(\mathrm{B}) &\simeq&
\psi^{-1}[\mathrm{E}(Y)]+\frac{1}{2}\mathrm{Var}(Y)\;
(\psi^{-1})"[\mathrm{E}(Y)]
\\ &\simeq& \psi^{-1}(a_0+a_1X_1+\ldots+a_pX_p)+\frac{\sigma^2}{2}\;
(\psi^{-1})"(a_0+a_1X_1+\ldots+a_pX_p)
\end{eqnarray*}


#### "Smearing" estimation {-}

The "smearing" estimation method is a nonparametric method used to correct prediction bias when an inverse transformation is used on the response variable of a linear model [@duan83; @taylor86; @manning01]. Given that we can rewrite biomass (or volume) expectation equation \@ref(eq:EB) as:
\begin{eqnarray*}
\mathrm{E}(B) &=&
\int\psi^{-1}(x)\;\phi_0(x-a_0-a_1X1-\ldots-a_pX_p)\;\mathrm{d}x
\\ &=&
\int\psi^{-1}(x+a_0+a_1X1+\ldots+a_pX_p)\;\mathrm{d}\Phi_0(x)
\end{eqnarray*}
where $\phi_0$ (respectively $\Phi_0$) is the probability density (respectively the distribution function) of the centered normal distribution of standard deviation $\sigma$, the smearing method consists in replacing $\Phi_0$ by the empirical distribution function of the residuals from model fitting, i.e.:
\begin{eqnarray*}
B_{\mathrm{smearing}} &=&
\int\psi^{-1}(x+a_0+a_1X_1+\ldots+a_pX_p)\times\frac{1}{n}\sum_{i=1}^n\delta
(x-\hat{\varepsilon}_i)\ \mathrm{d}x
\\ &=& \frac{1}{n}\sum_{i=1}^n\psi^{-1}(a_0+a_1X_1+\ldots+a_pX_p+
\hat{\varepsilon}_i)
\end{eqnarray*}
where $\delta$ is Dirac's function with zero mass and $\hat{\varepsilon}_i$ is the residual of the fitted model for the $i$th
observation. This method of correcting the prediction bias has the advantage of being both very general and easy to calculate. It has the drawback that the residuals $\hat{\varepsilon}_i$ of model fitting need to be known. This is not a problem when you yourself fit a model to the data, but is a problem when using published volume or biomass tables that are not accompanied by the residuals.

In the particular case of the log transformation, $\psi^{-1}$ is the exponential function, and therefore the smearing estimation of the biomass corresponds to: $\exp(a_0+a_1X_1+\ldots+a_pX_p)\times\mathrm{CF}_{\mathrm{smearing}}$, where the smearing correction factor is:
\[
\mathrm{CF}_{\mathrm{smearing}}=\frac{1}{n}\sum_{i=1}^n\exp(\hat{\varepsilon}_i)
\]
Given that $\hat{\sigma}^2=(\sum_{i=1}^n\hat{\varepsilon}_i^2)/(n-p-1)$, the smearing correction factor is different from correction factor \@ref(eq:CF). But, on condition that $\hat{\sigma}\rightarrow0$, the two adjustment coefficients are equivalent.


::::::{.filrouge data-latex=""}

(@eq-smear)

:::{.exercise #smear}
(ref:smear)
:::

Let us again return to the biomass model in red line \@ref(exr:fdens) fitted by multiple regression on log-transformed data:
\[
\ln(B)=-8.38900+0.85715\ln(D^2H)+0.72864\ln(\rho)
\]
The smearing correction factor may be obtained by the command:

```{r, echo=T, results='hide'}
mean(exp(residuals(m)))
```

where `m` is the object containing the fitted model, and in this example is `r mean(exp(residuals(m)))`. By way of a comparison, the previously calculated correction factor (red line \@ref(exr:CFB)) was 1.061035.

::::::

