

## Stratification and aggregation factors


Up until now we have considered that the dataset used to fit the volume or biomass model is homogeneous. In reality, the dataset may be drawn from measurements made under different conditions, or may have been generated by merging several separate datasets. Covariables are generally used to describe this dataset heterogeneity. For example, a covariable may indicate the type of forest in which the measurements were made (deciduous, semi-deciduous, evergreen, etc.), the type of soil, or year planted (if a plantation), etc. A crucial covariable for multispecific data is tree species. Initially, all these covariables that can explain the heterogeneity of a dataset were considered as qualitative variables (or factors). The various modalities of these factors define strata, and a well constituted dataset is one that is sampled in relation to previously identified strata (see `r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(stratif)). How can these qualitative covariables be taken into account in a volume or biomass model? Is it valid to analyze the dataset in an overall manner? Or should we analyze the data subsets corresponding to each stratum separately? These are the questions we will be addressing here (`r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(stdat)).

Also, biomass measurements are made separately for each compartment in the tree (see chapter \@ref(ter)). Therefore, in addition to an estimation of its total biomass, we also have, for each tree in the sample, an estimation of the biomass of its foliage, the biomass of its truck, of its large branches, of its small branches, etc. How can we take account of these different compartments when establishing biomass models? We will also be addressing this question (`r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(cmpt)).



### Stratifying data {#stdat}

Let us now consider that there are qualitative covariables that stratify the dataset into $S$ strata. As each stratum corresponds to a cross between the modalities of the qualitative covariables (in the context of experimental plans we speak of *treatment* rather than stratum) we will not consider each covariable separately. For example, if one covariable indicates the type of forest with three modalities (let us say: deciduous forest, semi-deciduous forest and evergreen forest) and another covariable indicates the type of soil with three modalities (let us say: sandy soil, clay soil, loamy soil), the cross between these two covariables gives $S=3\times 3=9$ strata (deciduous forest on sandy soil, deciduous forest on clay soil, etc.). We are not looking to analyze the forest type effect separately, or the soil type effect separately. Also, if certain combinations of the covariable modalities are not represented in the dataset, this reduces the number of strata. For example, if there are no evergreen forests on loamy soil, the number of strata $S=8$ instead of 9.

Therefore, when stratifying a dataset, one strategy consists in fitting a model separately for each stratum. In the case of a multiple regression, this would be written:
\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon_s
\]
where
\[
\varepsilon_s\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma_s)
\]
where ($Y_s$, $X_{1s}$, $\ldots$, $X_{ps}$) designates an observation relative to stratum $s$, for $s=1$, $\ldots$, $S$. There are now $S\times(p+1)$ coefficients to be estimated. An alternative strategy consists in analyzing the dataset as a whole, by fitting a model such as:
\begin{equation}
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
(\#eq:ancov)
\end{equation}
where
\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma)
\]
This model differs only in the structure of the error. This type of model is called an analysis of *covariance*. It assumes that all the residuals have the same variance, not only within each stratum, but also from one stratum to another. An analysis of covariance can test whether there is a stratum effect on the response variable, alone or in interaction with each of the effect variables $X_1$, $\ldots$, $X_p$. Testing the main effect of the stratification is equivalent to testing the null hypothesis $a_{01}=a_{02}=\ldots=a_{0S}$. The test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher's distribution. Testing the effect of the interaction between the stratification and the $j$th effect variable is equivalent to testing the null hypothesis $a_{j1}=a_{j2}=\ldots=a_{jS}$. As previously, the test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher's distribution.

The advantage of testing these effects is that each time one proves not to be significant, the $S$ coefficients $a_{j1}$, $a_{j2}$, $\ldots$, $a_{jS}$ to be estimated can be replaced by a single common coefficient $a_j$. Let us imagine, for example, that in analysis of covariance \@ref(eq:ancov), the main effect of the stratum is not significant, and neither is the interaction between the stratum and the first $p'$ effect variables (where $p'<p$). In this case the model to be fitted is:
\[
Y_s=a_0+a_1X_{1s}+\ldots+a_{p'}X_{p's}+a_{p'+1,s}X_{p'+1,s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]
where $\varepsilon\sim\mathcal{N}(0,\ \sigma)$. This model now includes "only" $p'+1+(p-p')S$ coefficients to be estimated, instead of $(p+1)S$ coefficients if we fit the model separately for each stratum. As all the observations serve to estimate common coefficients $a_0$, $\ldots$, $a_{p'}$, these are estimated more precisely than if we fit the model separately for each stratum.

This principle of an analysis of covariance can be immediately extended to the case of a non-linear model. Here again, we can test whether or not the coefficients are significantly different between the strata, and if necessary estimate a common coefficient for all the strata.


::::::{.filrouge data-latex=""}

(@eq-fmspe)

:::{.exercise #mfspe}
(ref:mfspe)
:::

In red line \@ref(exr:rllnBvD2H), we used simple linear regression to fit a power model with $D^2H$ as effect variable to log-transformed data: $\ln(B)=a+b\ln(D^2H)$. We can now include species-related information in this model to test whether the coefficients $a$ and $b$ differ from one species to another. The model corresponds to an analysis of covariance:
\[
\ln(B_s)=a_s+b_s\ln(D_s^2H_s)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
where index $s$ designates the species. This model is fitted by the command:

```{r, echo=T, results='hide'}
m <- lm(
  formula = log(Btot) ~ species * I(log(dbh^2 * heig)), 
  data = dat[dat$Btot > 0,]
  )
```

To test whether the coefficients $a$ and $b$ differ from one species to another, we can use the command:

```{r, echo=T, results='hide'}
anova(m)
```

which yields:

```{r}
res <- anova(m)
attr(res, "heading") <- NULL
print(res, signif.legend = FALSE)
```

The first line in the table tests for a species effect, i.e. whether the y-intercept $a_s$ differs from one species to another. The null hypothesis of this test is that there is no difference between species: $a_1=a_2=\ldots=a_S$, where $S=16$ is the number of species. The test statistic is given in the "F value" column. As the p-value of the test is less than 5\ \%, it may be concluded that the y-intercept of the model is significantly different from one species to another. The second line in the table tests for an effect of the $D^2H$, variable, i.e. whether the mean slope associated with this variable is significantly different from zero. The third line in the table tests whether the slope-species interaction is significant, i.e. whether slope $b_s$ differs from one species to another. The null hypothesis is that there is no difference between species: $b_1=b_2=\ldots=b_S$. The p-value here is 0.1785, therefore greater than 5\ \%: there is therefore no significant slope difference between the species.

This leads us to fit the following model:
\begin{equation}
\ln(B_s)=a_s+b\ln(D_s^2H_s)+\varepsilon(\#eq:mspe)
\end{equation}
which considers that slope $b$ is the same for all species. The relevant command is:

```{r, echo=T, results='hide'}
m <- lm(
  formula = log(Btot) ~ species + I(log(dbh^2 * heig)),
  data = dat[dat$Btot > 0,]
  )
anova(m)
```

and yields:

```{r}
#printCoefmat(anova(m), signif.legend = F, na.print = "", digits = 4)
res <- anova(m)
attr(res, "heading") <- NULL
print(res, signif.legend = FALSE)
```

Model coefficients may be obtained by the command:

```{r, echo=T, results='hide'}
summary(m)
```

which yields:

```{r}
printCoefmat(summary(m)$coef, digits = 3, signif.stars = TRUE, signif.legend = FALSE)
```

The last line in this table gives the value of the slope: $b=0.89985$. The other lines give y-intercept values for the 16 species. By convention, R proceeds in the following manner when specifying these values: the first line in the table gives the y-intercept for the first species in alphabetical order. As the first species in alphabetical order is *Afzelia bella*, the y-intercept for *Afzelia bella* is therefore $a_1=-9.00359$. The subsequent lines give the *difference* $a_s-a_1$ between the y-intercepts of the species indicated and the y-intercept of *Afzelia bella*. Thus, the y-intercept of *Aubrevillea kerstingii* is: $a_2=a_1-0.54634=-9.00359-0.54634=-9.54993$. Finally, the expression for the specific table corresponds to:
\[
\ln(B)=0.89985\ln(D^2H)-\left\{
\begin{array}{lcl}
9.00359 && \textrm{for } \textit{Afzelia bella}   \\ %
9.54993 && \textrm{for } \textit{Aubrevillea kerstingii}   \\ %
9.78047 && \textrm{for } \textit{Cecropia peltata}         \\ %
9.71200 && \textrm{for } \textit{Ceiba pentandra}          \\ %
9.46786 && \textrm{for } \textit{Cola nitida}              \\ %
8.95674 && \textrm{for } \textit{Daniellia thurifera}      \\ %
9.15985 && \textrm{for } \textit{Dialium aubrevilliei}     \\ %
8.95406 && \textrm{for } \textit{Drypetes chevalieri}      \\ %
7.90713 && \textrm{for } \textit{Garcinia epunctata}       \\ %
9.45614 && \textrm{for } \textit{Guarea cedrata}           \\ %
9.27223 && \textrm{for } \textit{Heritiera utilis}         \\ %
9.55823 && \textrm{for } \textit{Nauclea diderrichii}      \\ %
9.48176 && \textrm{for } \textit{Nesogordonia papaverifera}\\ %
9.18315 && \textrm{for } \textit{Piptadeniastrum africanum}\\ %
8.94026 && \textrm{for } \textit{Strombosia glaucescens}   \\ %
9.09462 && \textrm{for } \textit{Tieghemella heckelii}     \\ %
\end{array}
\right.
\]

For a given diameter and height, the species with the highest biomass is *Garcinia epunctata* while that with the lowest biomass is *Cecropia peltata*. The model's residual standard deviation is $\hat{\sigma}=0.3093$ and $R^2=0.9901$.

::::::


#### Case of a numerical covariable {-}

Up until now we have considered that the covariables defining the stratification are qualitative factors. But in certain cases these covariables can also be interpreted as numerical variables. For instance, let us consider a biomass model for plantations [@saintandre05]. The year the plantation was set up (or, which comes to the same thing, the age of the trees), may be used as a stratification covariable. This year or age may be seen indifferently as a qualitative variable (cohort of trees of the same age) or as a numerical value. More generally, any numerical variable may be seen as a qualitative variable if it is divided into classes. In the case of age, we could also consider plantations aged 0 to 5 years as a stratum, plantations aged 5 to 10 years as another stratum, plantations aged 10 to 20 years as a third stratum, etc.. The advantage of dividing covariable $Z$ into classes and considering it as a qualitative variable is that this allows us to model the relation between $Z$ and response variable $Y$ without any *a priori* constraint on the form of this relation. By contrast, if we consider $Z$ to be a numerical variable, we are obliged *a priori* to set the form of the relation between $Y$ and $Z$ (linear relation, polynomial relation, exponential relation, power relation, etc.). The disadvantage of dividing $Z$ into classes and considering this covariable as being qualitative is that the division introduces an element of randomness. Also, the covariance model that uses classes of $Z$ (qualitative covariables) will generally have more parameters that need to be estimated than the model that considers $Z$ to be a numerical covariable.

It is customary in modeling to play on this dual interpretation of numerical variables. When covariable $Z$ is numerical (e.g. tree age), it is advisable to proceed in two steps (as explained in `r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(plus)):

1. consider $Z$ as a qualitative variable (if necessary after division into classes) and fit a covariance model which will provide a picture of the form of the relation between Z and the model's coefficients;

2. model this relation using an appropriate expression then return to the fitting of a linear or non-linear model, but considering $Z$ to be a numerical variable.

If we return to the example of the age of the trees in a plantation: let us assume that age $Z$ has been divided into $S$ age classes. The first step consists typically of an analysis of covariance (assuming that the model has been rendered linear):
\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]
where $s=1$, $\ldots$, $S$. Let $Z_s$ be the median age of age class $s$. We then draw scatter plots of $a_{0s}$ against $Z_s$, $a_{1s}$ against $Z_s$, $\ldots$, $a_{ps}$ against $Z_s$ and for each scatter plot we look for the form of the relation that fits the plot. Let us imagine that $a_{0s}$ varies in a linear manner with $Z_s$, that $a_{1s}$ varies in a exponential manner with $Z_s$, that $a_{2s}$ varies in a power manner with $Z_s$, and that the coefficients $a_{3s}$ to $a_{ps}$ do not vary in relation to $Z_s$ (which besides can be formally tested). In this particular case we will then, as the second step, fit the following non-linear model:
\[
Y=\underbrace{b_0+b_1Z}_{a_{0s}}+\underbrace{b_2\exp(-b_3Z)}_{a_{1s}}X_1
+\underbrace{b_4Z^{b_5}}_{a_{2s}}X_2+a_3X_3+\ldots+a_pX_p+\varepsilon
\]
where age $Z$ is now considered as a numerical variable. Such a model involving a numerical effect covariable is called a *parameterized* model (here by age).

*Ordinal* covariables warrant a special remark. An ordinal variable is a qualitative variable that establishes an order. For example, the month of the year is a qualitative variable that establishes a chronological order. The type of soil along a soil fertility gradient is also an ordinal variable. Ordinal variables are generally treated as fully fledged qualitative variables, but in this case the order information they contain is lost. An alternative consists in numbering the ordered modalities of the ordinal variable using integers, then considering the ordinal variable as a numerical variable. For example, in the case of the months of the year, we can set January = 1, February = 2, etc. This approach is meaningful only if the differences between the integers correctly reflect the differences between the modalities of the ordinal variable. For instance, if we set 1 = January 2011 up to 12 = December 2011, we will set 1 = January 2012 if the response is cyclically seasonal, whereas we will set 13 = January 2012 if the response presents as a continuous trend. In the case of three soil types along a fertility gradient, we will set 1 = poorest soil, 2 = soil of intermediate fertility, and 3 = richest soil if we consider the fertility difference between two soils induces a response that is proportional to this difference, but we will set 1 = poorest soil, 4 = soil of intermediate fertility, and 9 = richest soil if we consider that the response is proportional to the square of the fertility difference.


#### Special case of the species {-}

In the case of multispecific datasets, the species is a stratification covariable that warrants special attention. If the dataset includes only a few species (less than about 10), and there are sufficient observations per species (see `r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(size)), then species may be considered to be a stratification covariable like any other. The model in this case would be divided into $S$ specific models, or the models could be grouped together based on the allometric resemblance of the species.

If the dataset contains many species, or if only a few observations are available for certain species, it is difficult to treat the species as a stratification covariable. A solution in this case consists in using species *functional traits*. Functional traits are defined here, a little abusively, as the numerical variables that characterize the species [@diaz97; @rosch97; @lavorel02; see @violle07 for a more rigorous definition]. The most widely used trait in biomass models is wood density. If we decide to use functional traits to represent species, these traits intervene as effect variables in the model in the same way as the effect variables that characterize the tree, for example its dbh or height. The single-entry (dbh) monospecific biomass model of the power type, which in its linear form may be written:
\[
\ln(B)=a_0+a_1\ln(D)+\varepsilon
\]
will thus, in the multispecific case, become a double-entry biomass model:
\[
\ln(B)=a_0+a_1\ln(D)+a_2\ln(\rho)+\varepsilon
\]
if we decide to use wood density $\rho$ to represent the specific effect.


::::::{.filrouge data-latex=""}

(@eq-fdens)

:::{.exercise #fdens}
(ref:fdens)
:::

In red line \@ref(exr:fmspe), species-related information was taken into account in the model $\ln(B)=a+b\ln(D^2H)$ through a qualitative covariable. We can now capture this information through specific wood density $\rho$. The fitted model is therefore:
\begin{equation}
\ln(B)=a_0+a_1\ln(D^2H)+a_2\ln(\rho)+\varepsilon(\#eq:dens)
\end{equation}
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
As wood density in the dataset was measured for each individual, we must start by calculating mean wood density for each species:

```{r, echo=T, results='hide'}
dm <- tapply(dat$dens, dat$species, mean)
dat2 <- cbind(dat, dmoy = dm[as.character(dat$species)])
```

The `dat2` dataset now contains an additional variable `dmoy` that gives specific wood density. The model is fitted by the command:

```{r, echo=T, results='hide'}
m <- lm(
  formula = log(Btot) ~ I(log(dbh^2 * heig)) + I(log(dmoy)), 
  data = dat2[dat2$Btot > 0,]
  )
summary(m)
```

which yields:
```{r}
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

with a residual standard deviation of `r print(summary(m)$sigma, digits = 4)` and $R^2=$ `r print(summary(m)$r.squared, digits = 4)`. The model is therefore written: $\ln(B)=-8.38900+0.85715\ln(D^2H)+0.72864\ln(\rho)$. Is it better to take account of the species through wood density, as we have just done, or by constructing specific models as in red line \@ref(exr:fmspe)? To answer this question we need to compare model \@ref(eq:mspe) to the model \@ref(eq:dens) using the AIC:

```{r, echo=T, eval=F}
AIC(m)
```

which yields $\mathrm{AIC}=34.17859$ for specific model \@ref(eq:mspe) and $\mathrm{AIC}=33.78733$ for the model \@ref(eq:dens) that uses wood density. The latter therefore is slightly better, but the difference in AIC is nevertheless minor.

::::::


To better take account of wood density variations within a tree, it is possible to analyze inter- and intra-specific variations rather than use a mean density based on the hypothesis that wood density is the same from the pith to the bark and from the top to the bottom of trees (see chapter \@ref(biol)). Wood density can be modeled by taking account of factors such as species, functional group, tree dimensions, and radial or vertical position in the tree. An initial comparison may be made using Friedman's analysis of variance test, followed by Tuckey's highly significant difference (HSD) test. These tests will indicate the variables that most influence wood density. This wood density may then be modeled based on these variables [@henry10].


::::::{.filrouge data-latex=""}

(@eq-fidens)

:::{.exercise #fidens}
(ref:fidens)
:::

In red line \@ref(exr:fdens), wood density $\rho$ was defined for the species by calculating the mean of individual densities for trees in the same species. Let us now fit a biomass model based on individual wood density measurements in order to take account of inter-individual variations in wood density in a given species. The fitted model is:
\[
\ln(B)=a_0+a_1\ln(D)+a_2\ln(\rho)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
where $\rho$ unlike in red line \@ref(exr:fdens), is here the *individual* measurement of wood density. The model is fitted by the command:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ I(log(dbh)) + I(log(dens)), data = dat[dat$Btot > 0,])
summary(m)
```

which yields:

```{r}
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

with a residual standard deviation of `r print(summary(m)$sigma, digits = 4)` and $R^2=$ `r print(summary(m)$r.squared, digits = 4)`. The model is written: $\ln(B)=-7.76644+2.35272\ln(D)+1.00717\ln(\rho)$. According to this model, biomass is dependent upon individual wood density by the term $\rho^{1.00717}$, i.e. practically $\rho$. By way of a comparison, model \@ref(eq:dens) was dependent upon specific wood density by the term $\rho^{0.72864}$. From a biological standpoint, the exponent 1.00717 is more satisfactory than the exponent 0.72864 since it means that the biomass is the product of a volume (that depends solely on tree dimensions) and a density. The difference between the two exponents may be attributed to inter-individual variations in wood density within the species. But the model based on individual wood density has little practical usefulness as it would require a wood density measurement in every tree whose biomass we are seeking to predict.

::::::



### Tree compartments {#cmpt}

Tree biomass is determined separately for each compartment in the tree (stump, trunk, large branches, small branches, leaves, etc.). Total above-ground biomass is the sum of these compartments. The approach already presented for fitting a model could be followed for each compartment separately. In this case we would construct a model for leaf biomass, a model for the biomass of the large branches, etc.. This approach integrates dataset stratification. Thus, we could initially fit a model for each compartment and each stratum; as a second step, and depending on the differences found between strata, we could aggregate the strata and/or parameterize the model in order to construct a model by compartment for all the strata. But the approach would not end there. We could also continue the data integration in order to move toward a smaller number of more integrating models.


#### Compartment additivity {-}

As total above-ground biomass is the sum of the biomasses of the different compartments, it could be imagined that the best model for predicting above-ground biomass is the sum of the models predicting the biomass of each compartment. In fact, because of the correlations between the biomasses of the different compartment, this is not the case [@cunia84; @cunia85b; @parresol99]. Also, certain model families are not stable by addition. This in particular is the case for power models: the sum of two power functions is not a power function. If we have fitted a power model for each compartment:
\begin{eqnarray*}
B^{\mathrm{stump}} &=& a_1D^{b_1}
\\ B^{\mathrm{trunk}} &=& a_2D^{b_2}
\\ B^{\mathrm{large\ branches}} &=& a_3D^{b_3}
\\ B^{\mathrm{small\ branches}} &=& a_4D^{b_4}
\\ B^{\mathrm{leaves}} &=& a_5D^{b_5}
\end{eqnarray*}
The sum $B^{\textrm{above-ground}}=B^{\mathrm{stump}} +B^{\mathrm{trunk}}+B^{\mathrm{large\ branches}} +B^{\mathrm{small\ branches}}+B^{\mathrm{leaves}}$ = $\sum_{m=1}^5a_m$ $D^{b_m}$ is not a power function of dbh. By contrast, polynomial models are stable by addition.


#### Fitting a multivariate model {-}

In order to take account of the correlations between the biomasses of the different compartments, we can fit the models relative to the different compartments in a simultaneous manner rather than separately. This last step in model integration requires us to redefine the response variable. As we are looking to predict simultaneously the biomasses of different compartments, we are no longer dealing with a response variable but a *response vector* $\mathbf{Y}$. The length of this vector is equal to the number $M$ of compartments. For example, if the response variable is the biomass,
\[
\mathbf{Y}=\left[
\begin{array}{l}
B^{\textrm{above-ground}}\\ %
B^{\mathrm{stump}}\\ %
B^{\mathrm{trunk}}\\ %
B^{\mathrm{large\ branches}}\\ %
B^{\mathrm{small\ branches}}\\ %
B^{\mathrm{leaves}}
\end{array}
\right]
\]
If the response variable is the log of the biomass,
\[
\mathbf{Y}=\left[
\begin{array}{l}
\ln(B^{\textrm{above-ground}})\\ %
\ln(B^{\mathrm{stump}})\\ %
\ln(B^{\mathrm{trunk}})\\ %
\ln(B^{\mathrm{large\ branches}})\\ %
\ln(B^{\mathrm{small\ branches}})\\ %
\ln(B^{\mathrm{leaves}})
\end{array}
\right]
\]
Let $Y_m$ be the response variable of the $m$th compartment (where $m=1$, $\ldots$, $M$). Without loss of generality, we can consider that all the compartments have the same set $X_1$, $X_2$, $\ldots$, $X_p$ of effect variables (if a variable is not included in the prediction of a compartment, the corresponding coefficient can simply be set at zero). A model that predicts a response vector rather than a response variable is a multivariate model. An observation used to fit a multivariate model consists of a vector ($Y_1$, $\ldots$, $Y_M$, $X_1$, $\ldots$, $X_p$) of length $M+p$. The residual of a multivariate model is a vector $\boldsymbol{\varepsilon}$ of length $M$, equal to the difference between the observed response vector and the predicted response vector.

The expression of an $M$-variate model differs from the $M$ univariate models corresponding to the different compartments only by the structure of the residual error; the structure of the mean model does not change. Let us consider the general case of a non-linear model. If the $M$ univariate models are:
\begin{equation}
Y_m=f_m(X_1,\ \ldots,\ X_p;\theta_m)+\varepsilon_m(\#eq:uvar)
\end{equation}
for $m=1$, $\ldots$, $M$, then the multivariate model is written:
\[
\mathbf{Y}=\mathbf{F}(X_1,\ \ldots,\ X_p;\boldsymbol{\theta})+
\boldsymbol{\varepsilon}
\]
where $\mathbf{Y}={}^{\mathrm{t}}{[Y_1,\ \ldots,\ Y_M]}$, $\boldsymbol{\theta}={}^{\mathrm{t}}{[\theta_1,\ \ldots,\ \theta_M]}$, and
\begin{equation}
\mathbf{F}(X_1,\ \ldots,\ X_p;\boldsymbol{\theta})=\left[
\begin{array}{c}
f_1(X_1,\ \ldots,\ X_p;\theta_1)
\\\vdots\\
f_m(X_1,\ \ldots,\ X_p;\theta_m)
\\\vdots\\
f_M(X_1,\ \ldots,\ X_p;\theta_M)
\end{array}
\right](\#eq:mvar)
\end{equation}
The residual vector $\boldsymbol{\varepsilon}$ now follows a centered multinormal distribution, with a variance-covariance matrix of:
\[
\mathrm{Var}(\boldsymbol{\varepsilon})\equiv\boldsymbol{\Sigma}
=\left[
\begin{array}{cccc}
\sigma_1^2 & \zeta_{12} & \cdots        & \zeta_{1M}\\ %
\zeta_{21} & \sigma_2^2 & \ddots        & \vdots\\ %
\vdots     & \ddots     & \ddots        & \zeta_{M-1,M}\\ %
\zeta_{M1} & \cdots     & \zeta_{M,M-1} & \sigma_M^2
\end{array}
\right]
\]
Matrix $\boldsymbol{\Sigma}$ is symmetrical with $M$ lines and $M$ columns, such that $\sigma_m^2=\mathrm{Var}(\varepsilon_m)$ is the residual variance of the biomass of the $m$th compartment and $\zeta_{ml}=\zeta_{lm}$ is the residual covariance between the biomass of the $m$th compartment and that of the $l$th compartment. Like in the univariate case, two residuals corresponding to two different observations are assumed to be independent: $\boldsymbol{\varepsilon}_i$ is independent of $\boldsymbol{\varepsilon}_j$ pour $i\neq j$. The difference arises from the fact that the different compartments are no longer assumed to be independent one from another.

A multivariate model such as \@ref(eq:mvar) is fitted based on the same principles as univariate models \@ref(eq:uvar). If the variance-covariance matrix $\boldsymbol{\Sigma}$ is diagonal (i.e. $\zeta_{ml}=0$, $\forall m$, $l$), then the fitting of the multivariate model \@ref(eq:mvar) is equivalent to the separate fitting of the $M$ univariate models \@ref(eq:uvar). In the case of a linear model, estimated values for coefficients $\theta_1$, $\theta_2$, $\ldots$, $\theta_M$ resulting from fitting the $M$-variate linear model are the same as the values obtained by the separate fitting of the $M$ univariate linear models (on condition that the same effect variables $X_1$, $\ldots$, $X_p$ are used in all cases) [@muller06, chapter 3]. But the significance tests associated with the coefficients do not give the same results in the two cases. If the different compartments are sufficiently correlated one with the other, the simultaneous fitting of all the compartments by the multivariate model \@ref(eq:mvar) will yield a more precise estimation of model coefficients, and therefore more precise biomass predictions.


#### Harmonizing a model {-}

In certain cases, particularly energy wood, we are looking to predict the dry biomass of the trunk at different cross-cut diameters. For instance, we want to predict simultaneously the total biomass $B$ of the trunk, the biomass $B_{7}$ of the trunk to the small-end diameter of 7~cm, and the biomass $B_{10}$ of the trunk to the small-end diameter of 10~cm. We could consider the entire trunk, the trunk to a cross-cut of 7~cm, and the trunk to a cross-cut of 10~cm as three different compartments and apply the same fitting principles as presented in the previous section. In fact, the problem is more complex because, unlike the trunk and leaf compartments which are separate, the compartments defined by different cross-cut diameters are nested one within the others: $B=B_{7}+$ biomass of the section to a small-end diameter of 7~cm, and $B_{7}=B_{10}+$ biomass of the section from the 10~cm to the 7~cm small-end cross-cuts. Thus, the multivariate model that predicts vector ($B$, $B_7$, $B_{10}$) must ensure that $B>B_{7}>B_{10}$ across the entire valid range of the model. The process consisting of constraining the multivariate model in such a manner that it predicts the biomasses of the different compartments while checking the logic of their nesting is called model *harmonization* [@parresol99]. @jacobs80 and @cunia85 have put forward solutions to this problem in the form of equations relating the coefficients in the models used for the different compartments. In this case we must fit an $M$-variate model (if there are $M$ cross-cut diameters) while ensuring that the coefficients $\theta_1$, $\ldots$, $\theta_M$ corresponding to the $M$ cross-cut diameters satisfy a number of equations linking them together. When the coefficients in the multivariate model are estimated by maximum likelihood, their numerical estimation is in fact a problem of constrained optimization.

If predicting the volume or biomass of a stem, an alternative to the volume or biomass model is to use stem profile integration [@parresol89; @parresol99]. Let $P(h)$ be a stem profile, i.e. a plot of trunk transversal section area against height $h$ from the ground ($h$ also represents the length when a stem is followed from its large to its small end) [@maguire96; @dean06; @metcalf09]. If the section through the stem is roughly circular, the diameter of the tree at height $h$ may be calculated as: $D(h)=\sqrt{4P(h)/\pi}$. The biomass of the trunk to a cross-cut diameter $D$ is calculated by integrating the stem profile from the ground ($h=0$) to height $P^{-1}(\frac{\pi}{4}D^2)$ corresponds to this diameter:
\[
B_D=\int_0^{P^{-1}(\frac{\pi}{4}D^2)}\rho(h)\;P(h)\ \mathrm{d}h
\]
where $\rho(h)$ is wood density at height $h$. Stem volume to cross-cut diameter $D$ is calculated in the same manner, except that $\rho$ is replaced by 1. The stem profile approach has the advantage that model harmonization here is automatic. But it is an approach that is conceptually different from volume and biomass models, with specific fitting problems [@fang99; @parresol99], and is outside the scope of this guide. It should be noted that when dealing with very large trees, for which it is almost impossible to measure biomass directly, the stem profile approach is a relevant alternative [@vanpelt01; @dean03; @dean03b; @dean06; @sillett10].


