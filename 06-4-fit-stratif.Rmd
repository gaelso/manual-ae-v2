

## Stratification and aggregation factors

Up until now we have considered that the dataset used to fit the volume or biomass model is homogeneous. In reality, the dataset may be drawn from measurements made under different conditions, or may have been generated by merging several separate datasets. Covariables are generally used to describe this dataset heterogeneity. For example, a covariable may indicate the type of forest in which the measurements were made (deciduous, semi-deciduous, evergreen, etc.), the type of soil, or year planted (if a plantation), etc. A crucial covariable for multispecific data is tree species. Initially, all these covariables that can explain the heterogeneity of a dataset were considered as qualitative variables (or factors). The various modalities of these factors define strata, and a well constituted dataset is one that is sampled in relation to previously identified strata (see \S\ \@ref(stratif)). How can these qualitative covariables be taken into account in a volume or biomass model? Is it valid to analyze the dataset in an overall manner? Or should we analyze the data subsets corresponding to each stratum separately? These are the questions we will be addressing here (\S\ \@ref(stdat)).

Also, biomass measurements are made separately for each compartment in the tree (see chapter \@ref(ter)). Therefore, in addition to an estimation of its total biomass, we also have, for each tree in the sample, an estimation of the biomass of its foliage, the biomass of its truck, of its large branches, of its small branches, etc. How can we take account of these different compartments when establishing biomass models? We will also be addressing this question (\S\ \ref{cmpt}).




### Stratifying data {#stdat}

Let us now consider that there are qualitative covariables that stratify the dataset into $S$ strata. As each stratum corresponds to a cross between the modalities of the qualitative covariables (in the context of experimental plans we speak of *treatment* rather than stratum) we will not consider each covariable separately. For example, if one covariable indicates the type of forest with three modalities (let us say: deciduous forest, semi-deciduous forest and evergreen forest) and another covariable indicates the type of soil with three modalities (let us say: sandy soil, clay soil, loamy soil), the cross between these two covariables gives $S=3\times 3=9$ strata (deciduous forest on sandy soil, deciduous forest on clay soil, etc.). We are not looking to analyze the forest type effect separately, or the soil type effect separately. Also, if certain combinations of the covariable modalities are not represented in the dataset, this reduces the number of strata. For example, if there are no evergreen forests on loamy soil, the number of strata $S=8$ instead of 9.

Therefore, when stratifying a dataset, one strategy consists in fitting a model separately for each stratum. In the case of a multiple regression, this would be written:
\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon_s
\]
where
\[
\varepsilon_s\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma_s)
\]
where ($Y_s$, $X_{1s}$, \ldots, $X_{ps}$) designates an observation relative to stratum $s$, for $s=1$, \ldots, $S$. There are now $S\times(p+1)$ coefficients to be estimated. An alternative strategy consists in analyzing the dataset as a whole, by fitting a model such as:

\begin{equation}
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
(\#eq:ancov)
\end{equation}

where
\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma)
\]
This model differs only in the structure of the error. This type of model is called an analysis of *covariance*. It assumes that all the residuals have the same variance, not only within each stratum, but also from one stratum to another. An analysis of covariance can test whether there is a stratum effect on the response variable, alone or in interaction with each of the effect variables $X_1$, \ldots, $X_p$. Testing the main effect of the stratification is equivalent to testing the null hypothesis $a_{01}=a_{02}=\ldots=a_{0S}$. The test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher's distribution. Testing the effect of the interaction between the stratification and the $j$th effect variable is equivalent to testing the null hypothesis $a_{j1}=a_{j2}=\ldots=a_{jS}$. As previously, the test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher's distribution.

The advantage of testing these effects is that each time one proves not to be significant, the $S$ coefficients $a_{j1}$, $a_{j2}$, \ldots, $a_{jS}$ to be estimated can be replaced by a single common coefficient $a_j$. Let us imagine, for example, that in analysis of covariance \@ref(eq:ancov), the main effect of the stratum is not significant, and neither is the interaction between the stratum and the first $p'$ effect variables (where $p'<p$). In this case the model to be fitted is:
\[
Y_s=a_0+a_1X_{1s}+\ldots+a_{p'}X_{p's}+a_{p'+1,s}X_{p'+1,s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]
where $\varepsilon\sim\mathcal{N}(0,\ \sigma)$. This model now includes "only" $p'+1+(p-p')S$ coefficients to be estimated, instead of $(p+1)S$ coefficients if we fit the model separately for each stratum. As all the observations serve to estimate common coefficients $a_0$, \ldots, $a_{p'}$, these are estimated more precisely than if we fit the model separately for each stratum.

This principle of an analysis of covariance can be immediately extended to the case of a non-linear model. Here again, we can test whether or not the coefficients are significantly different between the strata, and if necessary estimate a common coefficient for all the strata.


::::::{.filrouge data-latex=""}

(@eq-fmspe) (ref:mfspe)

:::{.exercise #mfspe name="(ref:mfspe)"}
\  
:::

In red line \@ref(exr:rllnBvD2H), we used simple linear regression to fit a power model with $D^2H$ as effect variable to log-transformed data: $\ln(B)=a+b\ln(D^2H)$. We can now include species-related information in this model to test whether the coefficients $a$ and $b$ differ from one species to another. The model corresponds to an analysis of covariance:
\[
\ln(B_s)=a_s+b_s\ln(D_s^2H_s)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
where index $s$ designates the species. This model is fitted by the command:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ species*I(log(dbh^2*heig)),data=dat[dat$Btot>0,])
```

To test whether the coefficients $a$ and $b$ differ from one species to another, we can use the command:

```{r, echo=T, results='hide'}
anova(m)
```

which yields:

```{r}
printCoefmat(anova(m), signif.legend = FALSE, na.print = "")
```

The first line in the table tests for a species effect, i.e. whether the y-intercept $a_s$ differs from one species to another. The null hypothesis of this test is that there is no difference between species: $a_1=a_2=\ldots=a_S$, where $S=16$ is the number of species. The test statistic is given in the "F value" column. As the p-value of the test is less than 5\ \%, it may be concluded that the y-intercept of the model is significantly different from one species to another. The second line in the table tests for an effect of the $D^2H$, variable, i.e. whether the mean slope associated with this variable is significantly different from zero. The third line in the table tests whether the slope-species interaction is significant, i.e. whether slope $b_s$ differs from one species to another. The null hypothesis is that there is no difference between species: $b_1=b_2=\ldots=b_S$. The p-value here is 0.1785, therefore greater than 5\ \%: there is therefore no significant slope difference between the species.

This leads us to fit the following model:

\begin{equation}
\ln(B_s)=a_s+b\ln(D_s^2H_s)+\varepsilon(\#eq:mspe)
\end{equation}

which considers that slope $b$ is the same for all species. The relevant command is:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ species+I(log(dbh^2*heig)),data=dat[dat$Btot>0,])
anova(m)
```

and yields:

```{r}
printCoefmat(anova(m), signif.legend = F, na.print = "")
```

Model coefficients may be obtained by the command:

```{r, echo=T, results='hide'}
summary(m)
```

which yields:

```{r}
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

The last line in this table gives the value of the slope: $b=0.89985$. The other lines give y-intercept values for the 16 species. By convention, R proceeds in the following manner when specifying these values: the first line in the table gives the y-intercept for the first species in alphabetical order. As the first species in alphabetical order is *Afzelia bella*, the y-intercept for *Afzelia bella* is therefore $a_1=-9.00359$. The subsequent lines give the *difference* $a_s-a_1$ between the y-intercepts of the species indicated and the y-intercept of *Afzelia bella*. Thus, the y-intercept of *Aubrevillea kerstingii* is: $a_2=a_1-0.54634=-9.00359-0.54634=-9.54993$. Finally, the expression for the specific table corresponds to:
\[
\ln(B)=0.89985\ln(D^2H)-\left\{
\begin{array}{lcl}
9.00359 && \textrm{for } \textit{Afzelia bella}   \\ %
9.54993 && \textrm{for } \textit{Aubrevillea kerstingii}   \\ %
9.78047 && \textrm{for } \textit{Cecropia peltata}         \\ %
9.71200 && \textrm{for } \textit{Ceiba pentandra}          \\ %
9.46786 && \textrm{for } \textit{Cola nitida}              \\ %
8.95674 && \textrm{for } \textit{Daniellia thurifera}      \\ %
9.15985 && \textrm{for } \textit{Dialium aubrevilliei}     \\ %
8.95406 && \textrm{for } \textit{Drypetes chevalieri}      \\ %
7.90713 && \textrm{for } \textit{Garcinia epunctata}       \\ %
9.45614 && \textrm{for } \textit{Guarea cedrata}           \\ %
9.27223 && \textrm{for } \textit{Heritiera utilis}         \\ %
9.55823 && \textrm{for } \textit{Nauclea diderrichii}      \\ %
9.48176 && \textrm{for } \textit{Nesogordonia papaverifera}\\ %
9.18315 && \textrm{for } \textit{Piptadeniastrum africanum}\\ %
8.94026 && \textrm{for } \textit{Strombosia glaucescens}   \\ %
9.09462 && \textrm{for } \textit{Tieghemella heckelii}     \\ %
\end{array}
\right.
\]

For a given diameter and height, the species with the highest biomass is *Garcinia epunctata* while that with the lowest biomass is *Cecropia peltata*. The model's residual standard deviation is $\hat{\sigma}=0.3093$ and $R^2=0.9901$.

::::::


#### Case of a numerical covariable {-}

Up until now we have considered that the covariables defining the stratification are qualitative factors. But in certain cases these covariables can also be interpreted as numerical variables. For instance, let us consider a biomass model for plantations [@saintandre05]. The year the plantation was set up (or, which comes to the same thing, the age of the trees), may be used as a stratification covariable. This year or age may be seen indifferently as a qualitative variable (cohort of trees of the same age) or as a numerical value. More generally, any numerical variable may be seen as a qualitative variable if it is divided into classes. In the case of age, we could also consider plantations aged 0 to 5 years as a stratum, plantations aged 5 to 10 years as another stratum, plantations aged 10 to 20 years as a third stratum, etc.. The advantage of dividing covariable $Z$ into classes and considering it as a qualitative variable is that this allows us to model the relation between $Z$ and response variable $Y$ without any *a priori* constraint on the form of this relation. By contrast, if we consider $Z$ to be a numerical variable, we are obliged *a priori* to set the form of the relation between $Y$ and $Z$ (linear relation, polynomial relation, exponential relation, power relation, etc.). The disadvantage of dividing $Z$ into classes and considering this covariable as being qualitative is that the division introduces an element of randomness. Also, the covariance model that uses classes of $Z$ (qualitative covariables) will generally have more parameters that need to be estimated than the model that considers $Z$ to be a numerical covariable.

It is customary in modeling to play on this dual interpretation of numerical variables. When covariable $Z$ is numerical (e.g. tree age), it is advisable to proceed in two steps (as explained in \S\ \@ref(plus)):

1. consider $Z$ as a qualitative variable (if necessary after division into classes) and fit a covariance model which will provide a picture of the form of the relation between Z and the model's coefficients;
2. model this relation using an appropriate expression then return to the fitting of a linear or non-linear model, but considering $Z$ to be a numerical variable.

If we return to the example of the age of the trees in a plantation: let us assume that age $Z$ has been divided into $S$ age classes. The first step consists typically of an analysis of covariance (assuming that the model has been rendered linear):
\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]
where $s=1$, \ldots, $S$. Let $Z_s$ be the median age of age class $s$. We then draw scatter plots of $a_{0s}$ against $Z_s$, $a_{1s}$ against
$Z_s$, \ldots, $a_{ps}$ against $Z_s$ and for each scatter plot we look for the form of the relation that fits the plot. Let us imagine that $a_{0s}$ varies in a linear manner with $Z_s$, that $a_{1s}$ varies in a exponential manner with $Z_s$, that $a_{2s}$ varies in a power manner with $Z_s$, and that the coefficients $a_{3s}$ to $a_{ps}$ do not vary in relation to $Z_s$ (which besides can be formally tested). In this particular case we will then, as the second step, fit the following non-linear model:
\[
Y=\underbrace{b_0+b_1Z}_{a_{0s}}+\underbrace{b_2\exp(-b_3Z)}_{a_{1s}}X_1
+\underbrace{b_4Z^{b_5}}_{a_{2s}}X_2+a_3X_3+\ldots+a_pX_p+\varepsilon
\]
where age $Z$ is now considered as a numerical variable. Such a model involving a numerical effect covariable is called a *parameterized* model (here by age).

*Ordinal* covariables warrant a special remark. An ordinal variable is a qualitative variable that establishes an order. For example, the month of the year is a qualitative variable that establishes a chronological order. The type of soil along a soil fertility gradient is also an ordinal variable. Ordinal variables are generally treated as fully fledged qualitative variables, but in this case the order information they contain is lost. An alternative consists in numbering the ordered modalities of the ordinal variable using integers, then considering the ordinal variable as a numerical variable. For example, in the case of the months of the year, we can set January = 1, February = 2, etc. This approach is meaningful only if the differences between the integers correctly reflect the differences between the modalities of the ordinal variable. For instance, if we set 1 = January 2011 up to 12 = December 2011, we will set 1 = January 2012 if the response is cyclically seasonal, whereas we will set 13 = January 2012 if the response presents as a continuous trend. In the case of three soil types along a fertility gradient, we will set 1 = poorest soil, 2 = soil of intermediate fertility, and 3 = richest soil if we consider the fertility difference between two soils induces a response that is proportional to this difference, but we will set 1 = poorest soil, 4 = soil of intermediate fertility, and 9 = richest soil if we consider that the response is proportional to the square of the fertility difference.


#### Special case of the species {-}

In the case of multispecific datasets, the species is a stratification covariable that warrants special attention. If the dataset includes only a few species (less than about 10), and there are sufficient observations per species (see \S\ \@ref(size)), then species may be considered to be a stratification covariable like any other. The model in this case would be divided into $S$ specific models, or the models could be grouped together based on the allometric resemblance of the species.

If the dataset contains many species, or if only a few observations are available for certain species, it is difficult to treat the species as a stratification covariable. A solution in this case consists in using species *functional traits*. Functional traits are defined here, a little abusively, as the numerical variables that characterize the species [@diaz97; @rosch97; @lavorel02; see @violle07 for a more rigorous definition]. The most widely used trait in biomass models is wood density. If we decide to use functional traits to represent species, these traits intervene as effect variables in the model in the same way as the effect variables that characterize the tree, for example its dbh or height. The single-entry (dbh) monospecific biomass model of the power type, which in its linear form may be written:
\[
\ln(B)=a_0+a_1\ln(D)+\varepsilon
\]
will thus, in the multispecific case, become a double-entry biomass model:
\[
\ln(B)=a_0+a_1\ln(D)+a_2\ln(\rho)+\varepsilon
\]
if we decide to use wood density $\rho$ to represent the specific effect.


::::::{.filrouge data-latex=""}

(@eq-fdens) (ref:fdens)

:::{.exercise #fdens name="(ref:fdens)"}
\  
:::

In red line \@ref(exr:fmspe), species-related information was taken into account in the model $\ln(B)=a+b\ln(D^2H)$ through a qualitative covariable. We can now capture this information through specific wood density $\rho$. The fitted model is therefore:

\begin{equation}
\ln(B)=a_0+a_1\ln(D^2H)+a_2\ln(\rho)+\varepsilon(\#eq:dens)
\end{equation}

where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
As wood density in the dataset was measured for each individual, we must start by calculating mean wood density for each species:

```{r, echo=T, results='hide'}
dm <- tapply(dat$dens,dat$species,mean)
dat <- cbind(dat,dmoy=dm[as.character(dat$species)])
```

The `dat` dataset now contains an additional variable `dmoy` that gives specific wood density. The model is fitted by the command:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ I(log(dbh^2*heig)) + I(log(dmoy)), data=dat[dat$Btot>0,])
summary(m)
```

which yields:
\Rout{\begin{tabular}{lrrrrl}%
                      & Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)           & -8.38900 &    0.26452 & -31.714 &  < 2e-16 & ***\\ %
I(log(dbh\^{}2*heig)) &  0.85715 &    0.02031 &  42.205 &  < 2e-16 & ***\\ %
I(log(dmoy))          &  0.72864 &    0.17720 &   4.112 & 0.000202 & ***\\ %
\end{tabular}}%
with a residual standard deviation of \decimal{0}{3442} and $R^2=\decimal{0}{9806}$. The model is therefore written:
$\ln(B)=-\decimal{8}{38900}+\decimal{0}{85715}\ln(D^2H)+\decimal{0}{72864}\ln(\rho)$.
Is it better to take account of the species through wood density, as we have just done, or by constructing specific models as in red line \@ref(exr:fmspe)? To answer this question we need to compare model (\ref{mspe}) to the model (\ref{dens}) using the AIC:
\R{AIC(m)}%
which yields $\mathrm{AIC}=\decimal{34}{17859}$ for specific model (\ref{mspe}) and $\mathrm{AIC}=\decimal{33}{78733}$ for the model (\ref{dens}) that uses wood density. The latter therefore is slightly better, but the difference in AIC is nevertheless minor.



::::::








