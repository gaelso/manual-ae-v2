
# Sampling and stratification {#samp}

Sampling consists in predicting the characteristics of a set from one part (a sample) of this set. Typically, we want to estimate the volume of wood in a forest, but it is impossible to measure the volume of all the trees one by one. We therefore measure the volume of a sample of trees in the forest, then extrapolate to obtain an estimate for all the trees in the entire forest [p.252 @ctft89]. As volume is measured only in a sample of trees, the estimated total volume we obtain suffers from a *sampling error*^[The terms in italics are those used in sampling theory; a definition of these terms may be found, for example, in appendix 2 of @bellefontaine01.].
Sampling in the strict sense consists in:

- selecting appropriate trees for the measured set (we tend to speak of a *sampling plan*),
- selecting a method used to calculate total volume from the measurements (we tend to speak of an *estimator*),

in such a manner to minimize the sampling error.

In conventional sampling theory, the volumes of $N$ trees in the stand are fixed data: the only source of variation in the estimations is the sampling, such that exhaustive sampling would always give the same estimation. Here in this guide we will adopt the so-called super-population approach that was developed in the 1970s [@cochran77]. This consists in considering that the volumes of the $N$ trees that make up the stand are random variables, such that the observed stand is only one among others drawn from a super-population. This approach means that we can do away with certain approximations and draw up an optimal sampling plan (something that is often impossible in the conventional approach). It does, however, have the disadvantage of leading to incorrect solutions if the super-population model adopted does not comply with reality.

The sampling method depends on the goal. Therefore, in principle, the question must first be asked as to the purpose of the volume or biomass tables we are seeking to construct. Are they to serve to predict the characteristics of a particular tree that has known entry variables? Are they to serve to predict the characteristics of an average tree from data on entry variables? Are they to serve to predict the total volume of the stand that yielded the trees used to construct the tables, or the total volume of another stand? In both these latter cases, are the tables' entry variables measured in all the trees of the stand, or again on a sample of trees? Etc. We can therefore construct a chain that runs from the studied stand to the variable we are looking to predict (Figure \@ref(fig:cha)).

```{r cha, fig.align='center', out.width='100%', fig.cap="Chain running from the studied stand to the variables we are looking to predict."}

grViz("
  
  digraph cha {
  
  graph [layout = dot, rankdir = LR]
  
  ## Add node statements
  node [shape = plaintext]
  'Stand \nstudied'
  'Trees \nmeasured'
  Model
  'Volume of an \nindividual tree'
  'Mean tree volume'
  'Volume of the \nstudied stand'
  'Volume of \nanother stand'
  
  ## Add edge statements
  'Stand \nstudied' -> 'Trees \nmeasured' [label = 'Sampling \nplan']
  'Trees \nmeasured' -> Model [label = 'Model \nconstruction']
  Model -> 'Volume of an \nindividual tree' [label = 'Prediction']
  Model -> 'Mean tree volume'
  Model -> 'Volume of the \nstudied stand'
  Model -> 'Volume of \nanother stand' [label = 'Extrapolation', align='r']
  
  }
")

```

```{r test, fig.align='center', out.width='100%', fig.cap="another diagram test with mermaid."}

mermaid('
  
  graph LR
  
  A("Stand \n; studied") -- "Sampling plan" --> B("Trees measured")
  B -- "Model \nconstruction" --> C(Model)
  C -- Prediction --> D("Volume of an \nindividual tree")
  C --> E(Mean tree volume)
  C --> F(Volume of the studied stand)
  C -- Extrapolation --> G(Volume of another stand)
  ')


```


By following this chain backward, it can be seen that the precision of the predicted variable depends on the precision of the tables' parameters, which itself depends on the sampling plan (number and selection of trees measured) and the variability within the studied stand [@cunia87c]. We can therefore set a target for the precision of our predictions which, by a retroactive effect for a given table and a given sampling type, determines the minimum number of trees to be measured. We can also apply an optimization procedure to determine, for a given target precision and a given table, the sampling method that minimizes measurement time or cost [@cunia87; @cunia87d]. In certain cases the cost of the measurements is the limiting factor. This in particular is the case for biomass measurements of root systems. Here in this case we are not looking so much to reach a given target precision in our prediction but to remain within reasonable cost limits. We can therefore, for a given measurement cost and a given type of table, determine the sampling plan that maximizes the precision of our predictions.

This reasoning, which is often too complex to be followed in a rigorous manner, should be applied on a case-by-case basis as it depends (*i*) on what we are looking to predict, (*ii*) on the type of tables used and (*iii*) on the type of sampling adopted. The fact that we are using a table is in itself a constraint on the sampling method: the total volume of a stand can be estimated from the volume of a sample of trees, without using volume tables. By using volume tables to estimate the total volume of a stand, we are already restricting ourselves to one type of total volume *estimator*.

What is more, the reasoning used to determine the sampling plan that complies with the target precision for our predictions assumes that we know the relation between the precision of our predictions and the precision of the parameters used to construct the tables, and the relation between the precision of the parameters used to construct the tables and sample size, etc. In certain simple cases these relations are explicitly known. But in most cases, as soon as the tables take a more complex form, these relations are not explicit. In this case, the above reasoning is no longer tractable.

This reasoning is delivered a knock-out blow when we realize that: (*i*) volume/biomass tables generally have multiple purposes or unspecified purposes and (*ii*) the form of these tables is generally unknown in advance. This is due to the simple fact that in most cases we wish to use these tables for different ends: to estimate the volume of a particular tree, an average tree, an entire stand, etc. Constructing the tables is ultimately an end in itself, without any reference to a quantity we wish to predict. Also, the form of the table is most often selected based on the results of an exploratory data analysis, and is therefore not known in advance. Obviously, some relationships such as the power function or second-order polynomials often recur, but no rule can be established *a priori*. It is therefore illusory to look to optimize a sampling plan.

Ultimately, the sampling used to construct volume/biomass tables is generally based on empirical considerations relating to the sampling plan. The choice of the *estimator*, which in fact determines which tables are chosen is made *a posteriori*, depending on the data collected, and independently of the sampling plan.

## Sampling for a simple linear regression {#simple}

Let us begin with a simple example that clearly illustrates the notions developed above. Let us assume that the trees in a given stand are described by their dbh $D$, their height $H$ and their volume $V$. We use volume tables that predict volume $V$ from the variable $D^2H$. The super-population model we adopt to describe the stand assumes that the relation between $V$ and $D^2H$ is linear, with white noise $\varepsilon$ of variance $\sigma^2$:

\begin{equation}
V=\alpha+\beta D^2H+\varepsilon(\#eq:MLex)
\end{equation}

where $\varepsilon$ follows a normal distribution of zero expectation and standard deviation $\sigma$. Let us also assume that the quantity $D^2H$ follows a normal distribution with a mean of $\mu$ and a standard deviation of $\tau$. White noise $\varepsilon$ incorporates all the factors that cause two trees of the same dbh and the same height not to have necessarily the same volume. Parameters
$\alpha$ and $\beta$ are unknown. To estimate them, we will measure 
$n$ trees; and will thus obtain a sample of $n$ doublets
$(D_1^2H_1,\ V_1)$, \ldots, $(D_n^2H_n,\ V_n)$, then construct the following linear regression: 

\begin{equation}
V_i=a+b D_i^2H_i+\varepsilon_i(\#eq:reg)
\end{equation}

In sampling theory jargon, the entry variables for the volume/biomass tables (diameter, height\ldots) are called *auxiliary* variables. It is important to distinguish these tree-related variables from those such as age which are stand-related. These latter variables are considered as parameters [p.106 @parde88]. Also, the sampling *unit* is the tree. Let us now see how to draw up a sampling plan on the basis of the goal set.

### Predicting the volume of a particular tree

Let us assume that the goal is to predict the volume of a tree in a stand of dbh $D^*$ and height $H^*$. Its predicted volume is of course:

\[
V^*=a+b D^{*2}H^*
\]

The super-population model stipulates that because of white noise 
$\varepsilon$, two trees selected at random and with the same dbh $D$ and the same height $H$ do not necessarily have the same volume. This means we are faced with intrinsic variability when we measure a particular tree, equal to $\sigma^2$. When attempting to predict volumes, this intrinsic variability is supplemented by the variability due to the imprecision of the $\alpha$ and $\beta$ parameter estimations. We will return to these notions later (in Chapter \@ref(util)). Thus, for a linear regression, the semi-amplitude of the confidence interval at the threshold $\alpha$ (typically 5\ \%) of $V^*$ is equal to [p.374 @saporta90]:

\[
t_{n-2}\;\hat{\sigma} \sqrt{1+\frac{1}{n}+
\frac{(D^{*2}H^*-\overline{D^2H}_e)^2}{\sum_{i=1}^n(D_i^2H_i
-\overline{D^2H}_e)^2}}
\]

where $t_{n-2}$ is the quantile $1-\alpha/2$ of a Student's distribution with 
$n-2$ degrees of freedom, $\overline{D^2H}_e$ is the empirical mean of the $D^2H$ values measured in the sample:

\[
\overline{D^2H}_e=\frac{1}{n}\sum_{i=1}^nD_i^2H_i
\]

and $\hat{\sigma}$ is an estimate of the standard deviation of the residuals:

\[
\hat{\sigma}^2=\frac{1}{n-2}\sum_{i=1}^n[V_i-(a+b D_i^2H_i)]^2
\]

The lowest value of this semi-amplitude (when $n\rightarrow\infty$) is $1.96\ \sigma$. We will now set our precision target for the estimation as a deviation of $E\ \%$
from this incompressible minimum, i.e. in an approximate manner we are looking for sample size $n$
such that:

\begin{equation}
1 + E \approx \sqrt{
1 + \frac{1}{n} + \frac{(D^{*2}H^* - \overline{D^2H}_e)^2}{\sum_{i=1}^n(D_i^2H_i - \overline{D^2H}_e)^2}
}(\#eq:E1)
\end{equation}


#### Random sampling

Let us first examine the case where we do not look to optimize the sampling plan, for example by selecting sample trees at random. The empirical mean of $D^2H$ in the sample is therefore an estimation of $\mu$, while the empirical variance of $D^2H$ in the sample is an estimation of $\tau^2$. Thus:

\[
(1+E)^2-1\approx\frac{1}{n}\left[
1+\frac{(D^{*2}H^*-\mu)^2}{\tau^2}\right]
\]

By way of a numerical example, let us take $\mu=5$ m^3^ as the mean value of $D^2H$ in the entire stand and $\tau=1$ m^3^
as its standard deviation. If we want to predict the volume of a tree that has a $D^2H$ of 2 m^3^ with a precision deviation of $E=5\ \%$, we will have to measure approximately $n=98$ trees.
We can see here that the expression of $n$ in relation to $D^{*2}H^*$
is symmetrical around $\mu$ and passes through a minimum for $D^{*2}H^*=\mu$. As $\mu-2=3$ m^3^ and $\mu+3=8$ m^3^, we will also need $n=98$ trees to predict the volume of a tree whose $D^2H$ is 8 m^3^ with a precision deviation of 5\ \%. The $n=98$ sample size may also be interpreted as that which ensures a precision deviation of at most 5\ \% (at $\alpha=5\ \%$) for all predictions in the 2--8 m^3^ range.


#### Optimized sampling

Let us now examine the case where we are looking to optimize the sampling plan in relation to the value of $D^{*2}H^*$. Equation \@ref(eq:E1) shows that precision deviation $E$ is smallest when $\overline{D^2H}_e=D^{*2}H^*$. It is therefore advantageous to select sample trees in such a manner that their empirical mean $D^2H$ value is $D^{*2}H^*$. In practice, as the empirical mean of the sample's $D^2H$ values will never be exactly $D^{*2}H^*$, it is advantageous to maximize the denominator $\sum_i(D_i^2H_i-\overline{D^2H}_e)^2$, i.e. to maximize the empirical variance of the sample's $D^2H$ values. Ultimately, the sampling plan that maximizes the precision of the volume prediction of trees with $D^2H$ values of $D^{*2}H^*$ consists in selecting $n/2$ trees with $D^2H$ values of $D^{*2}H^*-\Delta$ and $n/2$ trees with $D^2H$ values of $D^{*2}H^*+\Delta$, with $\Delta$ being as large as possible (Figure \@ref(fig:plan)).

```{r plan, fig.align='center', out.width='100%', fig.scap="Sampling plan optimizing volume prediction precision for a particular tree", fig.cap="(ref:plan)"}

plan

```


This sampling plan means that we can ignore the term dependent upon $D^{*2}H^*$ in Equation \@ref(eq:E1), thus simplifying the relationship to:

\[
(1+E)^2-1\approx\frac{1}{n}
\]

For $E=5\ \%$, this gives us $n=10$ trees. Optimizing the sampling plan has therefore "saved" us measuring 88 trees compared to the plan that consisted in selecting trees at random. However, the optimized plan is restricted to estimating the volume of a $D^{*2}H^*$sized tree. It is not optimized for estimating the volume of other sized trees. This reasoning therefore clearly has its limitations as volume tables are not generally (or even never) constructed to predict the volume of only one size of tree.

Even more seriously, the optimized sampling plan is also reliant upon the super-population model and may yield erroneous estimations if it does not correspond to reality. This is illustrated in Figure \@ref(fig:bad). The optimized sampling plan for a given $D^{*2}H^*$ size leads to the selection of extreme points (in black in Figure \@ref(fig:bad)) for the sample. This situation is critical for a linear regression as two groups of points far removed one from the other will give an elevated $R^2$ value without us really knowing what is happening between the two. If the linear relation assumed by the super-population model is correct (Figure \@ref(fig:bad) left), then there is no problem: the volume predicted by the tables (shown by a star) will indeed be close to the actual value (gray point). By contrast, if the super-population model is wrong, the predicted volume will be wrong: this is illustrated in Figure \@ref(fig:bad) on the right (where the sample points, in black, are exactly the same as those in Figure \@ref(fig:bad) on the left), where the size-volume relationship is in fact parabolic, not linear. In practice, the shape of the size-volume relationship (and therefore the volume tables) is not known in advance, and it is therefore best to sample trees across the entire size variation interval so as to visualize the nature of the size-volume relationship.

```{r badplan, fig.scap="Volume prediction based on a linear regression constructed using extreme points when the size-volume relationship is indeed linear and when it is not", fig.cap="(ref:badplan)"}

knitr::include_graphics(paste0(fig_path, "/badplan.pdf"))

```
