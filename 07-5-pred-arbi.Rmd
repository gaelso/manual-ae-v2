

## Arbitrating between different models {#arbi}

If we want to predict the volume or biomass of given trees, we often have a choice between different models. For instance, several models may have been fitted in different places for a given species. Or we may have a local model and a pan-tropical model. Arbitrating between the different models available is not always an easy task [@henry11]. For example, should we choose a specific, local model fitted using little data (and therefore unbiased *a priori* but with high prediction variability) or a pan-tropical, multispecific model fitted with a host of data (therefore potentially biased but with low prediction variability)? This shows how many criteria are potentially involved when making a choice: model quality (the range of its validity, its capacity to extrapolate predictions, etc.), its specificity (with local, monospecific models at one extreme and pan-tropical, multispecific models at the other), the size of the dataset used for model fitting (and therefore implicitly the variability of its predictions). Arbitrating between different models should not be confused with model selection, as described in section \@ref(selmod) for when selecting models their coefficients are not yet known, and when we estimate these coefficients we are looking for the model that best fits the data. When arbitrating between models we are dealing with models already fitted and whose coefficients are known.

Arbitration between different models often takes place without any biomass or volume data. But the case we will focus on here is when we have a reference dataset $\mathcal{S}_n$, with $n$ observations of the response variable (volume of biomass) and the effect variables.


### Comparing using validation criteria

When we have a dataset $\mathcal{S}_n$, we can compare the different models available on the basis of the validation criteria described in section \@ref(Ival), by using $\mathcal{S}_n$ as the validation dataset. Given that the models do not necessarily have the same number $p$ of parameters, and in line with the parsimony principle, we should prefer the validation criteria that depend on $p$ in such a manner as to penalize those models with many parameters.

When comparing a particular, supposedly "best" candidate model with different competitors, we can compare its predictions with those of its competitors. To do this we look to see whether or not the predictions made by its competitors are contained within the confidence interval at a significance level of $\alpha$ of the candidate model.


### Choosing a model

A model may be chosen with respect to a "true" model $f$ that although unknown may be assumed to exist. Let $M$ be the number of models available. Let us note as $\hat{f}_m$ the function of the $p$ effect variables that predicts volume or biomass based on the $m$th model. This function is random as it depends on coefficients that are estimated and therefore have their own distribution. The distribution of $\hat{f}_m$ therefore describes the variability of the predictions based on the model $m$, as described in section \@ref(BVpred). The $M$ models may have very different forms: the $\hat{f}_1$ model may correspond to a power function, the $\hat{f}_2$ model to a polynomial function, etc. We will also assume that there is a function $f$ of the $p$ effect variables that describes the "true" relation between the response variable (volume or biomass) and these effect variables. We do not know this "true" relation. We do not know what form it has. But each of the $M$ models may be considered to be an approximation of the "true" relation $f$.

In the models selection theory [@massart07], the difference between the relation $f$ and a model $\hat{f}_m$ is quantified by a function $\gamma$ that we call the *loss* function. For example, the loss function may be the norm $L^2$ of the difference between $f$ and $\hat{f}_m$:
\[
\gamma(f,\ \hat{f}_m)=\int_{x_1}\ldots\int_{x_p}
[f(x_1,\ \ldots,\ x_p)-\hat{f}_m(x_1,\ \ldots,\ x_p)]^2
\ \mathrm{d}x_1\ldots\mathrm{d}x_p
\]
The expectation of the loss with respect to the distribution of $\hat{f}_m$ is called the *risk* (written $R$) i.e. when integrating on the variability of model predictions:
\[
R=\mathrm{E}[\gamma(f,\ \hat{f}_m)]
\]
The best of the $M$ models available is that which minimizes the risk. The problem is that we do not know the true relation $f$, therefore, this "best" model is also unknown. In the models selection theory this best model is called an *oracle*. The model finally chosen will be that such that the risk of the oracle is bounded for a vast family of functions $f$. Intuitively, the model chosen is that where the difference between it and the "true" relation is limited whatever this "true" relation may be (within the limits of a range of realistic possibilities). We will not go into further detail here concerning this topic as it is outside the scope of this guide.


### Bayesian model averaging

Rather than choose one model from the $M$ available, with the danger of not choosing the "best", an alternative consists in combining the $M$ competitor models to form a new model. This is called "Bayesian model averaging" and although it has been very widely used for weather forecasting models [@raftery05; @furrer07; @berliner08; @smith09] it is little used for forestry models [@li08; @picard12]. Let $\mathcal{S}_n=\{(Y_i,\ X_{i1},\ \ldots,\ X_{ip}),\ i=1,\ldots,p\}$ be a reference dataset with $n$ observations of the response variable $Y$ and the $p$ effect variables. Bayesian model averaging considers that the distribution of the response variable $Y$ is a mixture of $M$ distributions:
\[
g(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ g_m(Y|X_1,\ \ldots,\ X_p)
\]
where $g$ is the distribution density of $Y$, $g_m$ is the conditional distribution density of $Y$ with model $m$ being the "best", and $w_m$ is the weight of the $m$th model in the mixture, that we may interpret as the *a posteriori* probability that the $m$th model is the "best". The *a posteriori* $w_m$ probabilities are reflections of the quality of the fitting of the models to the data, and their sum is one: $\sum_{m=1}^Mw_m=1$.

In the same manner as for the model selection mentioned in the previous section, Bayesian model averaging assumes that there is a "true" (though unknown) relation between the response variable and the $p$ effect variables, and that each model differs from this "true" relation according to a normal distribution of standard deviation $\sigma_m$. In other words, density $g_m$
is the density of the normal distribution of mean $f_m(x_1,\ \ldots,\ x_p)$ and standard deviation $\sigma_m$, where $f_m$ is the function of $p$ variables corresponding to the $m$th model. Thus, 
\[
g(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ \phi(Y;f_m(x_1,\ \ldots,\ x_p),\;\sigma_m)
\]
where $\phi(\cdot;\mu,\ \sigma)$ is the probability density of the normal distribution of expectation $\mu$ and standard deviation $\sigma$. The model $f_{\mathrm{mean}}$ resulting from the combination of the $M$ competitor models is defined as the expectation of the mixture model, i.e.:
\[
f_{\mathrm{mean}}(X_1,\ \ldots,\ X_p)=\mbox{E}(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ 
f_m(X_1,\ \ldots,\ X_p)
\]
Thus, the model resulting from the combination of the $M$ competitor models is a weighted mean of these $M$ models, with the weight of model $m$ being the *a posteriori* probability that model $m$ is the best. We can also calculate the variance of the predictions made by model $f_{\mathrm{mean}}$ that is a combination of the $M$ competitor models:

\begin{eqnarray*}
\mbox{Var}(Y|X_1,\ \ldots,\ X_p) &=& \sum_{m=1}^Mw_m\Big[
f_m(X_1,\ \ldots,\ X_p)-\sum_{l=1}^Mw_l\ f_l(X_1,\ \ldots,\ X_p)\Big]^2
\\ && +\sum_{m=1}^Mw_m\ \sigma_m^2
\end{eqnarray*}

The first term corresponds to between-model variance and expresses prediction variability from one model to another; the second term corresponds to within-model variance, and expresses the conditional error of the prediction, with this model being the best.

If we want to use model $f_{\mathrm{mean}}$ instead of the $M$ models $f_1,\ \ldots,\ f_M$, we now have to estimate the weights
$w_1,\ \ldots,\ w_M$ and the within-model standard deviations $\sigma_1,\ \ldots,\ \sigma_M$. These $2M$ parameters are estimated from reference dataset $\mathcal{S}_n$ using an EM algorithm [@dempster77; @mclachlan08]. The EM algorithm introduces latent variables $z_{im}$ such that $z_{im}$ is the *a posteriori* probability that model $m$ is the best model for observation $i$ of $\mathcal{S}_n$. The latent variables $z_{im}$ take values of between 0 and 1. The EM algorithm is iterative and alternates between two steps at each iteration: step E (as in "expectation") and step M (as "maximisation"). The EM algorithms is as follows:

1. Choose start values $w^{(0)}_1,\ \ldots,\ w^{(0)}_M$, $\sigma^{(0)}_1,\ \ldots,\ \sigma^{(0)}_M$ for the $2M$ parameters to be estimated.
2. Alternate the two steps:
    + 2.1. step E: calculate the value of $z_{im}$ at iteration 
    $j$ sbased on the values of the parameters at iteration $j-1$:
    \[
    z_{im}^{(j)}=\frac{w_m^{(j-1)}\ \phi[Y_i;f_m(X_{i1},\ \ldots,\ X_{ip}),
    \ \sigma_m^{(j-1)}]}{\sum_{k=1}^Mw_k^{(j-1)}\ \phi[Y_i;f_k(X_{i1},\ \ldots,\ X_{ip}),
    \ \sigma_k^{(j-1)}]}
    \]
    + 2.2. step M: estimate the parameters at iteration $j$ using current values of $z_{im}$ as weight, i.e.:
    \begin{eqnarray*}
    w_m^{(j)} &=& \frac{1}{n}\sum_{i=1}^nz_{im}^{(j)}\\ %
    {\sigma_m^{(j)}}^2 &=&
    \frac{\sum_{i=1}^nz_{im}^{(j)}[Y_i-f_m(X_{i1},\ \ldots,\ X_{ip})]^2}
    {\sum_{i=1}^nz_{im}^{(j)}}
    \end{eqnarray*}

    such as $\sum_{m=1}^M|w_m^{(j)}-w_m^{(j-1)}|+\sum_{m=1}^M|\sigma_m^{(j)}-\sigma_m^{(j-1)}|$ is larger than a fixed infitesimal threshold (for example $10^{-6}$).

3. The estimated value of $w_m$ is $w_m^{(j)}$ and the estimated value of $\sigma_m$ is $\sigma_m^{(j)}$.
