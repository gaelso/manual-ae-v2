

## Selecting variables and models {#select}

When we look to construct a volume or biomass table, the graphical exploration of the data (chapter \@ref(explo)) generally yields several possible model forms. We could fit all these potentially interesting models, but ultimately, which of all these fitted models should be recommended to the user? Selecting variables and selecting models aims to determine which is the "best" possible expression of the model among all those fitted.


### Selecting variables

Let us take the example of a biomass table we are looking to construct from a dataset that includes tree diameter (dbh) and height, and wood specific density. By working on log-transformed data, and given the variables they include, we may fit the following models:

\begin{eqnarray*}
\ln(B) &=& a_0+a_1\ln(D)+\varepsilon\\ %
\ln(B) &=& a_0+a_2\ln(H)+\varepsilon\\ %
\ln(B) &=& a_0+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_2\ln(H)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+a_3\ln(\rho)+\varepsilon
\end{eqnarray*}

The *complete* model (the last in the above list) is that which includes all the effect variables available. All the other models may be considered to be subsets of the complete model, but where certain effect variables have been employed and other set aside. Selecting the variables aims to choose --- from among the effect variables of the complete model --- those that should be kept and those that may be set aside as they contribute little to the prediction of the response variable. In other words, in this example, selecting the variables would consist in choosing the best model from among the seven models envisaged for $\ln(B)$.

Given that there are $p$ effect variables $X_1$, $X_2$, \ldots, $X_p$, there are $2^p-1$ models that include all or some of these effect variables. Selecting the variables consists in choosing the "best" combination of effect variables from all those available. This means firstly that we must have criterion that can be used to evaluate the quality of a model. We have already seen `r ifelse(book_format == "latex", "(p.\\pageref{irm}) ", "")`that $R^2$ is a poor criterion for evaluating the quality of one model in comparison with that of another as it increases automatically with the number of effect variables, and this regardless of whether these provide information useful for predicting the response variable or not. A better criterion for selecting effect variables is the residual variance estimator, which is linked to $R^2$ by the relation:
\[
\hat{\sigma}^2=\frac{n}{n-p-1}(1-R^2)\ S_Y^2
\]
where $S_Y^2$ is the empirical variance of the response variable.

Several methods may be used to select the best combination of effect variables. If $p$ is not too high, we can review all the $2^p-1$ possible models exhaustively. When $p$ is too high, a step by step method may be used to select the variables. Step-by-step methods proceed by the successive elimination or successive addition of effect variables. The descending method consists in eliminating the least significant of the $p$ variables. The regression is then recalculated and the process repeated until a stop criterion is satisfied (e.g. when all model coefficients are significantly different from zero). The ascending method proceeds in the opposite direction: we start with the best single-variable regression and add the variable that increases $R^2$ the most until the stop criterion is satisfied.

The so-called *stepwise* method is a further improvement upon the previous algorithm that consists in performing an additional Fisher's significance test at each step such as not to introduce a non-significant variable and possibly eliminate variables that have already been introduced but are no longer informative given the last variable selected. The algorithm stops when no more variables can be added or withdrawn. The different step-by-step selection methods do not necessarily give the same result, but the "stepwise" method would appear to be the best. They do not, however, safeguard from the untimely removal of variables that are actually significant, which may well bias the result. And in connection with this, it should be recalled that if we know (for biological reasons) why a variable should be included in a model (e.g. wood specific density), it is not because a statistical test declares it non-significant that it should be rejected (because of the test's type II error).

:::::::{.filrouge data-latex=""}

(@eq-selvar) (ref:selvar)

:::{.exercise #selvar name="(ref:selvar)"}
\  
:::

Let us select the variables $\ln(D)$, $[\ln(D)]^2$, $[\ln(D)]^3$, $\ln(H)$ to predict the log of the biomass. The complete model is therefore:
\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(H)
+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
Variables are selected in R using the `step` command applied to the fitted complete model:

```{r, echo=T, results='hide'}
m <- lm(
  log(Btot) ~ I(log(dbh)) + I(log(dbh)^2) + I(log(dbh)^3) + I(log(heig)),
  data = dat[dat$Btot>0,]
  )
summary(step(m))
```

which yields:

```{r}
m <- lm(
  log(Btot) ~ I(log(dbh)^2) + I(log(heig)),
  data = dat[dat$Btot>0,]
  )
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

The variables selected are therefore  $[\ln(D)]^2$ and $\ln(H)$. The model finally retained is therefore:
$\ln(B)=-6.50202+0.23756[\ln(D)]^2+1.01874\ln(H)$, with a residual standard deviation of `r print(summary(m)$sigma, digits = 4)` and $R^2=$ `r print(summary(m)$r.squared, digits = 4)`.

::::::


### Selecting models {#selmod}

Given two competitor models that predict the same response variable to within one transformation of a variable, which should we choose? Let us look at a few different cases before answering this question.

#### Nested models {-}

The simplest case is where the two models to be compared are nested. A model is *nested* in another if the two predict the same response variable and if we can move from the second to the first by removing one or several effect variables. For example, the biomass table $B=a_0+a_1D+\varepsilon$ is nested in $B=a_0+a_1D+a_2D^2H+\varepsilon$ since we can move from the second to the first by deleting $D^2H$ from the effect variables. Likewise, the model $B=a_0+a_2D^2H+\varepsilon$ is nested in $B=a_0+a_1D+a_2D^2H+\varepsilon$ since we can move from the second to the first by deleting $D$ from the effect variables. By contrast, the model $B=a_0+a_1D+\varepsilon$ is not nested in $B=a_0+a_2D^2H+\varepsilon$. Let $p$ be the number of effect variables in the complete model and $p'<p$ be the number of effect variables in the nested model. Without loss of generality, we can write the complete model as:

\begin{equation}
Y=f(X_1,\ \ldots,\ X_{p'},\ X_{p'+1},\ \ldots,\ X_p;
\theta_0,\ \theta_1)+\varepsilon(\#eq:embnl)
\end{equation}

where ($\theta_0$, $\theta_1$) is the vector of the coefficients associated with the complete model and $\theta_0$ is the vector of the coefficients associated with the nested model, which may be obtained by setting 
$\theta_1=\mathbf{0}$. In the particular case of the linear model, the complete model is obtained as the sum of the nested model and additional terms:

\begin{equation}
\underbrace{\underbrace{Y=a_0+a_1X_1+\ldots+a_{p'}X_{p'}}_{\mbox{\scriptsize
nested model}}+a_{p'+1}X_{p'+1}+\ldots+a_pX_p}_{\mbox{
\scriptsize complete model}}+\varepsilon(\#eq:emblm)
\end{equation}

where $\theta_0=(a_0,\ \ldots,\ a_{p'})$ and $\theta_1=(a_{p'+1},\ \ldots,\ a_p)$.

In the case of nested models, a statistical test can be used to test one of the models against the other. The null hypothesis of this test is that $\theta_1=\mathbf{0}$, i.e. the additional terms are not significant, which can also be expressed as: the nested model is better than the complete model. If the p-value of this test proves to be below the significance level (typically 5\ \%), then the null hypothesis is rejected, i.e. the complete model is best. Conversely, if the p-value is above the significance threshold, the nested model is considered to be the best.

In the case of the linear model \@ref(eq:emblm), the test statistic is a ratio of the mean squares, which under the null hypothesis follows Fisher's distribution. This is the same type of test as that used to test the overall significance of a multiple regression, or that used in the "stepwise" method of selecting variables. In the general case of the non-linear model \@ref(eq:embnl), the test statistic is a likelihood ratio, such that $-2\log$(likelihood ratio) under the null hypothesis follows a $\chi^2$ distribution.

::::::{.filrouge data-latex=""}

(@eq-fboite) (ref:fboite)

:::{.exercise #fboite name="(ref:fboite)"}
\  
:::

In red line \@ref(exr:selvar) the variable $[\ln(D)]^2$ was selected with $\ln(H)$ as effect variable of $\ln(B)$ but not $\ln(D)$. Model $\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_4\ln(H)$, which includes the additional term $\ln(D)$, can be compared with the model $\ln(B)=a_0+a_2[\ln(D)]^2+a_4\ln(H)$ using the nested models test. In R, the `anova` command can be used to test a nested model, with the first argument being the nested model and the second being the complete model:

```{r, echo=T, results='hide'}
comp <- lm(
  log(Btot) ~ I(log(dbh)) + I(log(dbh)^2) + I(log(heig)), 
  data=dat[dat$Btot > 0,]
  )
nest <- lm(
  log(Btot) ~ I(log(dbh)^2) + I(log(heig)), 
  data=dat[dat$Btot > 0,]
  )
anova(nest,comp)
```

The test gives the following result:

```{r}
res <- anova(nest, comp)
attr(res, "heading") <- NULL
res
```

The p-value is  `r print(res[2, "Pr(>F)"], digits = 4)`, therefore greater than 5\ \%. The nested model (without $\ln(D)$) is therefore selected rather than the complete model.

::::::


::::::{.filrouge data-latex=""}

(@eq-fboite2) (ref:fboite2)

:::{.exercise #fboite2 name="(ref:fboite2)"}
\  
:::

The model $\ln(B)=-8.42722+2.36104\ln(D)$ was obtained in red line \@ref(exr:rllnBvD) whereas red line \@ref(exr:flnDlnH) gave the model $\ln(B)=-8.9050+1.8654\ln(D)+0.7083\ln(H)$. As the first is nested in the second, we can test for which is the best. The command 

```{r, echo=T, results='hide'}
comp <- lm(log(Btot) ~ I(log(dbh)) + I(log(heig)), data = dat[dat$Btot>0,])
nest <- lm(log(Btot) ~ I(log(dbh)), data = dat[dat$Btot>0,])
anova(nest,comp)
```

yields:

```{r}
res <- anova(nest, comp)
attr(res, "heading") <- NULL
res
```
As the p-value is less than 5\ \%, the complete model (including $\ln(H)$ as effect variable) is selected rather than the nested model.

::::::


#### Models with the same response variable {-}

When we want to compare two models that have the same response variable but are not nested, we can no longer use a statistical test. For example, we cannot use the above-mentioned test to compare $B=a_0+a_1D+\varepsilon$ and $B=a_0+a_2D^2H+\varepsilon$. In this case, we must use an information criterion [@bozdogan87; @burnham02; @burnham04]. There are several, suited to different contexts. The most widespread are the "Bayesian information criterion", or BIC, and above all the @akaike74 information criterion (or AIC). The AIC is expressed as:
\[
\mathrm{AIC}=-2\ln\ell(\hat{\theta})+2q
\]
where $\ell(\hat{\theta})$ if the model's likelihood, i.e. the likelihood of the sample for the values estimated from model parameters, see equation \@ref(eq:vrais), and $q$ is the number of free parameters estimated. In particular, in the case of a multiple regression against $p$ effect variables, $q=p+1$ (i.e. the $p$ coefficients associated with the $p$ effect variables plus the y-intercept). The coefficient $-2$ in front of the log-likelihood in the AIC expression is identical to that in the test statistic on the likelihood ratio in the case of nested models. Given two models with the same number of parameters, the best model is that with the highest likelihood, therefore that with the smallest AIC. At equal likelihoods, the best model is that with the fewest parameters (in accordance with the principle of Occam's razor), and therefore is once more that with the smallest AIC. When all is said and done, the best model is that with the smallest value of AIC.

The BIC is similar in expression to the AIC, but with a term that penalizes more strongly the number of parameters:
\[
\mathrm{BIC}=-2\ln\ell(\hat{\theta})+q\ln(n)
\]
where $n$ is the number of observations. Here again, the best model is that with the smallest value of BIC. When fitting volume or biomass tables, AIC is used rather than BIC as a model selection criterion.

::::::{.filrouge data-latex=""}

(@eq-selB) (ref:selB)

:::{.exercise #selB name="(ref:selB)"}
\  
:::

The following models with B as response variable were fitted:

- red line \@ref(exr:fDpara) or \@ref(exr:finvD): $B=-3.840\times10^{-3}D+1.124\times10^{-3}D^2$
- red line \@ref(exr:fDD2var): $B=-3.319456\times10^{-3}D+1.067068\times10^{-3}D^2$
- red line \@ref(exr:fnlsD): $B=2.492\times10^{-4}D^{2.346}$
- red line \@ref(exr:fnlmD): $B=2.445\times10^{-4}D^{2.35105}$
- red line \@ref(exr:frlpD2H) or \@ref(exr:fH): $B=2.747\times10^{-5}D^2H$
- red line \@ref(exr:fD2Hvar): $B=2.740688\times10^{-5}D^2H$
- red line \@ref(exr:fnlsD2H): $B=7.885\times10^{-5}(D^2H)^{0.9154}$
- red line \@ref(exr:fnlmD2H): $B=8.19\times10^{-5}(D^2H)^{0.9122144}$
- red line \@ref(exr:fnlsDH): $B=1.003\times10^{-4}D^{1.923}H^{0.7435}$
- red line \@ref(exr:fnlmDH): $B=1.109\times10^{-4}D^{1.9434876}H^{0.6926256}$

The models in red lines \@ref(exr:fDpara), \@ref(exr:fDD2var), \@ref(exr:frlpD2H) and \@ref(exr:fD2Hvar) are fitted by linear regression while the others are fitted by non-linear regression. These models have five distinct forms, with two fitting methods for each: using a weighted regression by the weighted least squares method (red lines \@ref(exr:fDpar), \@ref(exr:fnlsD), \@ref(exr:frlpD2H), \@ref(exr:fnlsD2H) and \@ref(exr:fnlsDH)) or using a regression with variance model by the maximum likelihood method (red lines \@ref(exr:fDD2var), \@ref(exr:fnlmD), \@ref(exr:fD2Hvar), \@ref(exr:fnlmD2H) and \@ref(exr:fnlmDH)). The predictions made by these different models are shown in  Figure \@ref(fig:fpredB). Let `m` be one of the fitted models with dbh as the only entry. A plot of the predictions made by this model may be obtained as follows:

```{r, echo=T, eval=F}
m <-  lm(Btot âˆ¼ dbh + I(dbh^2), data = dat, weights = 1/dat$dbh^4) ## ex: red line 14
with(dat, plot(dbh, Btot, xlab = "Dbh (cm)", ylab = "Biomass (t)"))
D <- seq(par("usr")[1], par("usr")[2], length=200)
lines(D, predict(m, newdata = data.frame(dbh = D)), col = "red")
```

For a model `m` that has dbh and height as entries, its predictions may be obtained as follows:

```{r, echo=T, eval=F}
## model red line 22
library(nlme)
start <- coef(lm(log(Btot) ~ I(log(dbh)) + I(log(heig)), data = dat[dat$Btot>0,]))
start[1] <- exp(start[1])
names(start) <- c("a","b1","b2")
m <- nlme(
  model   = Btot ~ a * dbh^b1 * heig^b2,
  data    = cbind(dat, g = "a"),
  fixed   = a + b1 + b2 ~ 1,
  start   = start,
  groups  = ~g,
  weights = varPower(form = ~dbh)
  )

## Predictions
D <- seq(0,180,length=20)
H <- seq(0,61,length=20)
newdata <- data.frame(expand.grid(dbh=D, heig=H), g = "a")
B <- matrix(predict(m, newdata), length(D))
```

and a plot of the biomass response surface against diameter and height may be obtained by:

```{r, echo=T, eval=F}
M <- persp(D,H,B,xlab="Dbh (cm)",ylab="Height (m)",
           zlab="Biomass (t)", ticktype="detailed", theta = -30, phi = 20)
points(trans3d(dat$dbh,dat$heig,dat$Btot,M))
```

Given a fitted model `m`, its AIC may be calculated by the command: 

```{r, echo=T, results='hide'}
AIC(m)
```

AIC values for the 10 models listed above are given in Table \@ref(tab:fAICD). This table illustrates a problem that arises with several statistical software packages, including R: when we maximize the log-likelihood \@ref(eq:ll), the constants (such as $-n\ln(2\pi)/2$) no longer play any role. The constant we use to calculate the log-likelihood, and by consequence AIC, is therefore a matter of convention, and different constants are used depending on the calculation. Thus, in Table \@ref(tab:fAICD), we can see that the values of AIC in models fitted by the `nls` command are substantially higher than those in the other models: it is not that these models are substantially worse than the others, it is simply that the `nls` command uses a constant different from the others to calculate the log-likelihood. Therefore, with R, it should be remembered that the values of AIC should be compared only for models that have been fitted using the same command. In our present case, if we compare the two models that were fitted with the `lm` command, the best (i.e. that with the smallest AIC) is that which has $D^2H$ as effect variable (red line \@ref(exr:frlpD2H)). If we compare the five models fitted with the `nlme` command, the best is again that with $D^2H$ as effect variable (red line \@ref(exr:fD2Hvar)). And if we compare the three models fitted with the `nls` command, the best is yet again that with $D^2H$ as effect variable (red line \@ref(exr:fnlsD2H)). It may therefore be concluded that whatever fitting method is used, the biomass table that uses $D^2H$ as effect variable is the best.

::::::

```{r fpredB, fig.ncol=2, fig.subcap = c('(A)', '(B)', '(C)', '(D)'), out.extra="", fig.show='hold', out.width="50%", out.height="50%", fig.scap="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by @henry10", fig.cap="(ref:fpredB)"}

knitr::include_graphics(c(paste0(fig_path, "/faicD1.png"), paste0(fig_path, "/faicD2.png"), paste0(fig_path, "/faicD3.png"), paste0(fig_path, "/faicD4.png")), auto_pdf = TRUE)

```

```{r fAICD}

kab_opt <- list(
  x = kab_fAICD,
  caption = "(ref:fAICD)",
  caption.short = "AIC values for 10 biomass tables fitted to data from 42 trees measured in Ghana by @henry10",
  col.names = linebreak(c("Red \nline", "Entry", "Fitting$^*$\nmethod", "R \ncommand", "AIC")),
  #align = "ccccccccccccc",
  booktabs = TRUE,
  escape = F
  )

display_table() %>%
  footnote(symbol = "WLS = weighted least squares, ML = maximum likelihood")

```

::::::{.filrouge data-latex=""}

(@eq-sellnB) (ref:sellnB)

:::{.exercise #sellnB name="(ref:sellnB)"}
\  
:::

The following models with $\ln(B)$ as response variable were fitted:

- red line \@ref(exr:rllnBvD) or \@ref(exr:flnDpol): $\ln(B)=-8.42722+2.36104\ln(D)$
- red line \@ref(exr:rllnBvD2H): $\ln(B)=-8.99427+0.87238\ln(D^2H)$
- red line \@ref(exr:flnDlnH): $\ln(B)=-8.9050+1.8654\ln(D)+0.7083\ln(H)$
- red line \@ref(exr:selvar): $\ln(B)=-6.50202+0.23756[\ln(D)]^2+1.01874\ln(H)$

All these models were fitted using linear regression by the ordinary least squares method. A plot of the predictions on a log scale for model `m` with dbh as only entry may be obtained by the following command:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ I(log(dbh)),data=dat[dat$Btot>0,]) ## red line 7
with(dat, plot(dbh, Btot, xlab="Dbh (cm)", ylab="Biomass (t)", log="xy"))
D <- 10^par("usr")[1:2]
lines(D, exp(predict(m, newdata = data.frame(dbh = D))))
```

For a model that uses both dbh and height as entries, a plot on a log scale may be obtained by the command:

```{r, echo=T, results='hide'}
m <- lm(log(Btot) ~ I(log(dbh)) + I(log(heig)), data = dat[dat$Btot>0,]) ## red line 10
D <- exp(seq(log(1), log(180), length = 20))
H <- exp(seq(log(1), log(61), length=20))
B <- matrix(predict(m, newdata=expand.grid(dbh=D,heig=H)), length(D))
M <- persp(log(D), log(H), B, xlab="log(Diameter) (cm)",ylab="log(height) (m)",
           zlab="log(Biomass) (t)", ticktype = "detailed", theta = -30, phi = 20)
points(trans3d(log(dat$dbh), log(dat$heig), log(dat$Btot), M))
```

Predictions of $\ln(B)$ by the four models are given in  Figure \@ref(fig:fpredlnB). Given a fitted model `m`, its AIC may be calculated by the command:

```{r, echo=T, results='hide'}
AIC(m)
```

Table \@ref(tab:fAIClnD) gives AIC values for the four models. As all four models were fitted by the same `lm` command, the AIC values are directly comparable. The best model, i.e. that with the smallest AIC, is the fourth (red line \@ref(exr:selvar)). It should also be noted that an AIC-based classification of these models is entirely consistent with the results of the nested models tests performed previously (red lines \@ref(exr:fboite) and \@ref(exr:fboite2)).
::::::

```{r fpredlnB, fig.ncol=2, fig.subcap = c('', '', '', ''), out.extra="", fig.show='hold', out.width="50%", out.height="50%", fig.scap="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry *et al.* (2010)", fig.cap="(ref:fpredlnB)"}

knitr::include_graphics(c(paste0(fig_path, "/faiclnD1.png"), paste0(fig_path, "/faiclnD2.png"), paste0(fig_path, "/faiclnD3.png"), paste0(fig_path, "/faiclnD4.png")), auto_pdf = TRUE)

```

```{r fAIClnD}
kab_opt <- list(
  x = kab_fAIClnD,
  caption = "(ref:fAIClnD)",
  caption.short = "AIC values for four biomass tables fitted to data from 42 trees measured in Ghana by 
Henry et al. (2010)",
  col.names = linebreak(c("Red \nline", "Entry", "Fitting \nmethod", "R \ncommand", "AIC")),
  #align = "ccccccccccccc",
  booktabs = TRUE,
  escape = F
  )

display_table()
```



\subsubsection{Models with different response variables\label{furni}}

The more general case is when we want to compare two models that do not have the same response variable because one is a transform of the other. For example, the models $B=aD^b+\varepsilon$ and $\ln(B)=a+b\ln(D)+\varepsilon$ both predict biomass, but the response variable is $B$ in one case and $\ln(B)$ in the other. In this case, we cannot use the AIC or BIC to compare the models. By contrast, @furnival61 index can in this case be used to compare the models. The model with the smallest Furnival's index is considered to be the best [@parresol99].

Furnival's index is defined only for a model whose residual error $\varepsilon$ has a variance that is assumed to be constant: $\mbox{Var}(\varepsilon)=\sigma^2$. By contrast, no constraint is imposed on the form of the transformation of the variable linking the modeled response variable $Y$ to the variable of interest (volume or biomass). Let us consider the case of a biomass model (that can immediately be transposed into a volume model) and let $\psi$ be this variable transformation: $Y=\psi(B)$. Furnival's index is defined by:
\[
F=\frac{\hat{\sigma}}{\sqrt[n]{\prod_{i=1}^n\psi'(B_i)}}
=\exp\Big(-\frac{1}{n}\sum_{i=1}^n\ln[\psi'(B_i)]\Big)\ \hat{\sigma}
\]
where $\hat{\sigma}$ if the estimation of the residual standard deviation of the fitted model and $B_i$ is the biomass of the $i$th tree measured. When there is no transformation of variables, $\psi$ is the identity function and Furnival's index $F$ is then equal to the residual standard deviation $\hat{\sigma}$. The most common variable transformation is the log transformation:
$\psi(B)=\ln(B)$ and $\psi'(B)=1/B$, in which case Furnival's index is:

\[
F_{\ln}=\hat{\sigma}\sqrt[n]{\textstyle\prod_{i=1}^nB_i}
=\exp\Big(\frac{1}{n}\sum_{i=1}^n\ln(B_i)\Big)\ \hat{\sigma}
\]

For linear regressions where the residual variance is assumed to be proportional to a power of an effect variable $X_1$, a trick can nevertheless be used to define Furnival's index. The linear regression:

\begin{equation}
Y=a_0+a_1X_1+a_2X_2+\ldots+a_pX_p+\varepsilon(\#eq:F1)
\end{equation}
where $\mbox{Var}(\varepsilon)=(kX_1^c)^2$; is strictly identical to the linear regression (see p.\pageref{apart}):
\begin{equation}
Y'=a_0X_1^{-c}+a_1X_1^{1-c}+a_2X_2X_1^{-c}+\ldots+
a_pX_pX_1^{-c}+\varepsilon'(\#eq:F2)
\end{equation}
where $Y'=YX_1^{-c}$, $\varepsilon'=\varepsilon X_1^{-c}$ and
$\mbox{Var}(\varepsilon')=k^2$. As model (\ref{F2}) has constant residual variance, its Furnival's index is defined.
By extension, we can define the Furnival index of model (\ref{F1}) as being the Furnival index of model (\ref{F2}). If $Y=\psi(B)$, then $Y'=X_1^{-c}\psi(B)$, such that Furnival's index is:

\[
F=\frac{\hat{k}}{\sqrt[n]{\prod_{i=1}^nX_{i1}^{-c}\psi'(B_i)}}
=\exp\Big(\frac{1}{n}\sum_{i=1}^n\{c\ln(X_{i1})-\ln[\psi'(B_i)]\}
\Big)\ \hat{k}
\]
Therefore, Furnival's index can be used to select the value of exponent $c$ in a weighted regression (see p.\pageref{chx}).

### Choosing a fitting method

Let us return to the method used to fit a volume or biomass model. Many solutions may be available to fit a model. Let us for instance consider the biomass model:
\[
B=a\rho^{b_1}D^{b_2}H^{b_3}+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ kD^c)
\]
This model could be adjusted as a non-linear model (*i*) by the weighted least squares method ($c$ fixed in advance) or (*ii*) by the maximum likelihood method ($c$ not fixed in advance). If we apply a log transformation to the data, we could (*iii*) fit the multiple regression:
\[
\ln(B)=a'+b_1\ln(\rho)+b_2\ln(D)+b_3\ln(H)+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ \sigma)
\]
Thus, for a given model that predicts biomass as a power of effect variables, we have three possible fitting methods. Obviously, methods (*i*)--(*ii*) and (*iii*) are based on different hypotheses concerning the structure of the residual errors: additive error with regard to $B$ in cases (*i*) and (*ii*), multiplicative error with regard to $B$ in case (*iii*). But both these error types reflect data heteroscedasticity, and therefore fitting methods (*i*), (*ii*) and (*iii*) all have a chance of being valid.

As another example, let us consider the biomass table:
\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)\}+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ kD^c)
\]
Here again, we can (*i*) fit a non-linear model by the least squares method (specifying $c$ in advance), (*ii*) fit a non-linear model by the maximum likelihood method (estimating $c$), or (*ii*) fit a multiple regression on log-transformed data:
\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)+\varepsilon
\]
where
\[
\varepsilon\sim\mathcal{N}(0,\ \sigma)
\]
Here again, the structure of the errors is not the same in the three cases, but all can correctly reflect the heteroscedasticity of the biomass.
In most cases, the different fitting methods give very similar results in terms of predictions. Should any doubt persist regarding the most appropriate fitting method, then model selection methods may be employed to decide. But in practice the choice of a particular fitting method results rather from the importance given to the respective advantages and drawbacks of each method. Multiple regression has the drawback of imposing constraints on the form of the residuals, and has less flexibility for the mean model. By contrast, it has the advantage of offering an explicit expression for the estimators of the model's coefficients: there is therefore no risk of having erroneous estimators for the coefficients. The non-linear model has the advantage of not imposing any constraints on the mean model or the variance model. Its drawback is that it does not have an explicit expression for parameter estimators: there is therefore the risk of having erroneous parameter estimators.

\begin{filrouge}{Power model fitting methods}{fmeth}%
We have already taken a look at three methods for fitting the power model $B=aD^b$:
\begin{enumerate}
\item using a simple linear regression on log-transformed data (red line \@ref(exr:rllnBvD)):
$\ln(B)=-\decimal{8}{42722}\bidouille{\newline}%
+\decimal{2}{36104}\ln(D)$, if we "naively" apply the exponential inverse transformation;
\item using a weighted non-linear regression (red line \@ref(exr:fnlsD)):
$B=\decimal{2}{492}\times10^{-4}D^{2.346}$;
\item using a non-linear regression with variance model (red line \@ref(exr:fnlmD)): $B=\decimal{2}{445}\bidouille{\newline}%
\times10^{-4}D^{2.35105}$.
\end{enumerate}
The predictions given by these three fittings of the same model are illustrated in  Figure \@ref(fig:fselm). The differences can be seen to be minimal and well below prediction precision, as we will see later (\S\ \ref{BVpred}).
\end{filrouge}

\begin{figure}[htb]
\begin{center}
\ifincludegraphics{width=\textwidth}{fselm}
\end{center}
\caption[Biomass predictions by the same power table fitted in three different ways to data from 42 trees measured in Ghana by @henry10]{Biomass predictions by the same power table fitted in three different ways to data from 42 trees measured in Ghana by @henry10. The data are shown as points. The \ifelsecoul{red}{solid} line is the linear regression on log-transformed data (red line \@ref(exr:rllnBvD)). The \ifelsecoul{green line}{dashed line} (practically superimposed by the \ifelsecoul{blue line}{dotted line}) is the fitting by weighted non-linear regression (red line \@ref(exr:fnlsD)). The \ifelsecoul{blue color}{dotted line} shows the fitting by non-linear regression with variance model (red line \@ref(exr:fnlmD)). (A) No data transformation. (B) On a log scale.\label{fselm}}
\end{figure}%