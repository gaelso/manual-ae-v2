

## Selecting variables and models {#select}

When we look to construct a volume or biomass table, the graphical exploration of the data (chapter \@ref(explo)) generally yields several possible model forms. We could fit all these potentially interesting models, but ultimately, which of all these fitted models should be recommended to the user? Selecting variables and selecting models aims to determine which is the "best" possible expression of the model among all those fitted.


### Selecting variables

Let us take the example of a biomass table we are looking to construct from a dataset that includes tree diameter (dbh) and height, and wood specific density. By working on log-transformed data, and given the variables they include, we may fit the following models:

\begin{eqnarray*}
\ln(B) &=& a_0+a_1\ln(D)+\varepsilon\\ %
\ln(B) &=& a_0+a_2\ln(H)+\varepsilon\\ %
\ln(B) &=& a_0+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_2\ln(H)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+a_3\ln(\rho)+\varepsilon
\end{eqnarray*}

The *complete* model (the last in the above list) is that which includes all the effect variables available. All the other models may be considered to be subsets of the complete model, but where certain effect variables have been employed and other set aside. Selecting the variables aims to choose --- from among the effect variables of the complete model --- those that should be kept and those that may be set aside as they contribute little to the prediction of the response variable. In other words, in this example, selecting the variables would consist in choosing the best model from among the seven models envisaged for $\ln(B)$.

Given that there are $p$ effect variables $X_1$, $X_2$, \ldots, $X_p$, there are $2^p-1$ models that include all or some of these effect variables. Selecting the variables consists in choosing the "best" combination of effect variables from all those available. This means firstly that we must have criterion that can be used to evaluate the quality of a model. We have already seen `r ifelse(book_format == "latex", "(p.\\pageref{irm}) ", "")`that $R^2$ is a poor criterion for evaluating the quality of one model in comparison with that of another as it increases automatically with the number of effect variables, and this regardless of whether these provide information useful for predicting the response variable or not. A better criterion for selecting effect variables is the residual variance estimator, which is linked to $R^2$ by the relation:
\[
\hat{\sigma}^2=\frac{n}{n-p-1}(1-R^2)\ S_Y^2
\]
where $S_Y^2$ is the empirical variance of the response variable.

Several methods may be used to select the best combination of effect variables. If $p$ is not too high, we can review all the $2^p-1$ possible models exhaustively. When $p$ is too high, a step by step method may be used to select the variables. Step-by-step methods proceed by the successive elimination or successive addition of effect variables. The descending method consists in eliminating the least significant of the $p$ variables. The regression is then recalculated and the process repeated until a stop criterion is satisfied (e.g. when all model coefficients are significantly different from zero). The ascending method proceeds in the opposite direction: we start with the best single-variable regression and add the variable that increases $R^2$ the most until the stop criterion is satisfied.

The so-called *stepwise* method is a further improvement upon the previous algorithm that consists in performing an additional Fisher's significance test at each step such as not to introduce a non-significant variable and possibly eliminate variables that have already been introduced but are no longer informative given the last variable selected. The algorithm stops when no more variables can be added or withdrawn. The different step-by-step selection methods do not necessarily give the same result, but the "stepwise" method would appear to be the best. They do not, however, safeguard from the untimely removal of variables that are actually significant, which may well bias the result. And in connection with this, it should be recalled that if we know (for biological reasons) why a variable should be included in a model (e.g. wood specific density), it is not because a statistical test declares it non-significant that it should be rejected (because of the test's type II error).

:::::::{.filrouge data-latex=""}

(@eq-selvar) (ref:selvar)

:::{.exercise #selvar name="(ref:selvar)"}
\  
:::

Let us select the variables $\ln(D)$, $[\ln(D)]^2$, $[\ln(D)]^3$, $\ln(H)$ to predict the log of the biomass. The complete model is therefore:
\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(H)
+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
Variables are selected in R using the `step` command applied to the fitted complete model:

```{r, echo=T, results='hide'}
m <- lm(
  log(Btot) ~ I(log(dbh)) + I(log(dbh)^2) + I(log(dbh)^3) + I(log(heig)),
  data = dat[dat$Btot>0,]
  )
summary(step(m))
```

which yields:

```{r}
m <- lm(
  log(Btot) ~ I(log(dbh)^2) + I(log(heig)),
  data = dat[dat$Btot>0,]
  )
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

The variables selected are therefore  $[\ln(D)]^2$ and $\ln(H)$. The model finally retained is therefore:
$\ln(B)=-6.50202+0.23756[\ln(D)]^2+1.01874\ln(H)$, with a residual standard deviation of `r print(summary(m)$sigma, digits = 4)` and $R^2=$ `r print(summary(m)$r.squared, digits = 4)`.

::::::


### Selecting models {#selmod}

Given two competitor models that predict the same response variable to within one transformation of a variable, which should we choose? Let us look at a few different cases before answering this question.

#### Nested models {-}

The simplest case is where the two models to be compared are nested. A model is *nested* in another if the two predict the same response variable and if we can move from the second to the first by removing one or several effect variables. For example, the biomass table $B=a_0+a_1D+\varepsilon$ is nested in $B=a_0+a_1D+a_2D^2H+\varepsilon$ since we can move from the second to the first by deleting $D^2H$ from the effect variables. Likewise, the model $B=a_0+a_2D^2H+\varepsilon$ is nested in $B=a_0+a_1D+a_2D^2H+\varepsilon$ since we can move from the second to the first by deleting $D$ from the effect variables. By contrast, the model $B=a_0+a_1D+\varepsilon$ is not nested in $B=a_0+a_2D^2H+\varepsilon$. Let $p$ be the number of effect variables in the complete model and $p'<p$ be the number of effect variables in the nested model. Without loss of generality, we can write the complete model as:

\begin{equation}
Y=f(X_1,\ \ldots,\ X_{p'},\ X_{p'+1},\ \ldots,\ X_p;
\theta_0,\ \theta_1)+\varepsilon(\#eq:embnl)
\end{equation}

where ($\theta_0$, $\theta_1$) is the vector of the coefficients associated with the complete model and $\theta_0$ is the vector of the coefficients associated with the nested model, which may be obtained by setting 
$\theta_1=\mathbf{0}$. In the particular case of the linear model, the complete model is obtained as the sum of the nested model and additional terms:

\begin{equation}
\underbrace{\underbrace{Y=a_0+a_1X_1+\ldots+a_{p'}X_{p'}}_{\mbox{\scriptsize
nested model}}+a_{p'+1}X_{p'+1}+\ldots+a_pX_p}_{\mbox{
\scriptsize complete model}}+\varepsilon(\#eq:emblm)
\end{equation}

where $\theta_0=(a_0,\ \ldots,\ a_{p'})$ and $\theta_1=(a_{p'+1},\ \ldots,\ a_p)$.

In the case of nested models, a statistical test can be used to test one of the models against the other. The null hypothesis of this test is that $\theta_1=\mathbf{0}$, i.e. the additional terms are not significant, which can also be expressed as: the nested model is better than the complete model. If the p-value of this test proves to be below the significance level (typically 5\ \%), then the null hypothesis is rejected, i.e. the complete model is best. Conversely, if the p-value is above the significance threshold, the nested model is considered to be the best.

In the case of the linear model \@ref(eq:emblm), the test statistic is a ratio of the mean squares, which under the null hypothesis follows Fisher's distribution. This is the same type of test as that used to test the overall significance of a multiple regression, or that used in the "stepwise" method of selecting variables. In the general case of the non-linear model \@ref(eq:embnl), the test statistic is a likelihood ratio, such that $-2\log$(likelihood ratio) under the null hypothesis follows a $\chi^2$ distribution.

::::::{.filrouge data-latex=""}

(@eq-fboite) (ref:fboite)

:::{.exercise #fboite name="(ref:fboite)"}
\  
:::

In red line \@ref(exr:selvar) the variable $[\ln(D)]^2$ was selected with $\ln(H)$ as effect variable of $\ln(B)$ but not $\ln(D)$. Model $\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_4\ln(H)$, which includes the additional term $\ln(D)$, can be compared with the model $\ln(B)=a_0+a_2[\ln(D)]^2+a_4\ln(H)$ using the nested models test. In R, the `anova` command can be used to test a nested model, with the first argument being the nested model and the second being the complete model:

```{r, echo=T, results='hide'}
comp <- lm(
  log(Btot) ~ I(log(dbh)) + I(log(dbh)^2) + I(log(heig)), 
  data=dat[dat$Btot > 0,]
  )
nest <- lm(
  log(Btot) ~ I(log(dbh)^2) + I(log(heig)), 
  data=dat[dat$Btot > 0,]
  )
anova(nest,comp)
```

The test gives the following result:

```{r}
res <- anova(nest, comp)
attr(res, "heading") <- NULL
res
```

The p-value is  `r print(res[2, "Pr(>F)"], digits = 4)`, therefore greater than 5\ \%. The nested model (without $\ln(D)$) is therefore selected rather than the complete model.

::::::


::::::{.filrouge data-latex=""}

(@eq-fboite2) (ref:fboite2)

:::{.exercise #fboite2 name="(ref:fboite2)"}
\  
:::

The model $\ln(B)=-8.42722+2.36104\ln(D)$ was obtained in red line \@ref(exr:rllnBvD) whereas red line \@ref(exr:flnDlnH) gave the model $\ln(B)=-8.9050+1.8654\ln(D)+0.7083\ln(H)$. As the first is nested in the second, we can test for which is the best. The command 

```{r, echo=T, results='hide'}
comp <- lm(log(Btot) ~ I(log(dbh)) + I(log(heig)), data = dat[dat$Btot>0,])
nest <- lm(log(Btot) ~ I(log(dbh)), data = dat[dat$Btot>0,])
anova(nest,comp)
```

yields:

```{r}
res <- anova(nest, comp)
attr(res, "heading") <- NULL
res
```
As the p-value is less than 5\ \%, the complete model (including $\ln(H)$ as effect variable) is selected rather than the nested model.

::::::


#### Models with the same response variable {-}

When we want to compare two models that have the same response variable but are not nested, we can no longer use a statistical test. For example, we cannot use the above-mentioned test to compare $B=a_0+a_1D+\varepsilon$ and $B=a_0+a_2D^2H+\varepsilon$. In this case, we must use an information criterion [@bozdogan87; @burnham02; @burnham04]. There are several, suited to different contexts. The most widespread are the "Bayesian information criterion", or BIC, and above all the @akaike74 information criterion (or AIC). The AIC is expressed as:
\[
\mathrm{AIC}=-2\ln\ell(\hat{\theta})+2q
\]
where $\ell(\hat{\theta})$ if the model's likelihood, i.e. the likelihood of the sample for the values estimated from model parameters, see equation \@ref(eq:vrais), and $q$ is the number of free parameters estimated. In particular, in the case of a multiple regression against $p$ effect variables, $q=p+1$ (i.e. the $p$ coefficients associated with the $p$ effect variables plus the y-intercept). The coefficient $-2$ in front of the log-likelihood in the AIC expression is identical to that in the test statistic on the likelihood ratio in the case of nested models. Given two models with the same number of parameters, the best model is that with the highest likelihood, therefore that with the smallest AIC. At equal likelihoods, the best model is that with the fewest parameters (in accordance with the principle of Occam's razor), and therefore is once more that with the smallest AIC. When all is said and done, the best model is that with the smallest value of AIC.

The BIC is similar in expression to the AIC, but with a term that penalizes more strongly the number of parameters:
\[
\mathrm{BIC}=-2\ln\ell(\hat{\theta})+q\ln(n)
\]
where $n$ is the number of observations. Here again, the best model is that with the smallest value of BIC. When fitting volume or biomass tables, AIC is used rather than BIC as a model selection criterion.

::::::{.filrouge data-latex=""}

(@eq-selB) (ref:selB)

:::{.exercise #selB name="(ref:selB)"}
\  
:::

The following models with B as response variable were fitted:

- red line \@ref(exr:fDpara) or \@ref(exr:finvD): $B=-3.840\times10^{-3}D+1.124\times10^{-3}D^2$
- red line \@ref(exr:fDD2var): $B=-3.319456\times10^{-3}D+1.067068\times10^{-3}D^2$
- red line \@ref(exr:fnlsD): $B=2.492\times10^{-4}D^{2.346}$
- red line \@ref(exr:fnlmD): $B=2.445\times10^{-4}D^{2.35105}$
- red line \@ref(exr:frlpD2H) or \@ref(exr:fH): $B=2.747\times10^{-5}D^2H$
- red line \@ref(exr:fD2Hvar): $B=2.740688\times10^{-5}D^2H$
- red line \@ref(exr:fnlsD2H): $B=7.885\times10^{-5}(D^2H)^{0.9154}$
- red line \@ref(exr:fnlmD2H): $B=8.19\times10^{-5}(D^2H)^{0.9122144}$
- red line \@ref(exr:fnlsDH): $B=1.003\times10^{-4}D^{1.923}H^{0.7435}$
- red line \@ref(exr:fnlmDH): $B=1.109\times10^{-4}D^{1.9434876}H^{0.6926256}$

The models in red lines \@ref(exr:fDpara), \@ref(exr:fDD2var), \@ref(exr:frlpD2H) and \@ref(exr:fD2Hvar) are fitted by linear regression while the others are fitted by non-linear regression. These models have five distinct forms, with two fitting methods for each: using a weighted regression by the weighted least squares method (red lines \@ref(exr:fDpar), \@ref(exr:fnlsD), \@ref(exr:frlpD2H), \@ref(exr:fnlsD2H) and \@ref(exr:fnlsDH)) or using a regression with variance model by the maximum likelihood method (red lines \@ref(exr:fDD2var), \@ref(exr:fnlmD), \@ref(exr:fD2Hvar), \@ref(exr:fnlmD2H) and \@ref(exr:fnlmDH)). The predictions made by these different models are shown in  Figure \@ref(fig:fpredB). Let `m` be one of the fitted models with dbh as the only entry. A plot of the predictions made by this model may be obtained as follows:

```{r, echo=T, eval=F}
#m <-  lm(Btot ∼ dbh + I(dbh^2), data = dat, weights = 1/dat$dbh^4) ## ex: red line 14
with(dat, plot(dbh, Btot, xlab = "Dbh (cm)", ylab = "Biomass (t)"))
D <- seq(par("usr")[1], par("usr")[2], length=200)
lines(D, predict(m, newdata = data.frame(dbh = D)), col = "red")
```

For a model `m` that has dbh and height as entries, its predictions may be obtained as follows:
\R{D <- seq(0,180,length=20)%
\\ H <- seq(0,61,length=20)%
\\ B <- matrix(predict(m,newdata=expand.grid(dbh=D,height=H)),length(D))}%
and a plot of the biomass response surface against diameter and height may be obtained by:
\R{M <- persp(D,H,B,xlab="Dbh (cm)",ylab="Height (m)",%
zlab="Biomass (t)",\bidouille{\newline}ticktype="detailed")
\\ points(trans3d(dat\$dbh,dat\$heig,dat\$Btot,M))}%
Given a fitted model `m`, its AIC may be calculated by the command: 
\R{AIC(m)}%
AIC values for the 10 models listed above are given in Table \@ref(tab:fAICD). This table illustrates a problem that arises with several statistical software packages, including R: when we maximize the log-likelihood (\ref{ll}), the constants (such as $-n\ln(2\pi)/2$) no longer play any role. The constant we use to calculate the log-likelihood, and by consequence AIC, is therefore a matter of convention, and different constants are used depending on the calculation. Thus, in Table \@ref(tab:fAICD), we can see that the values of AIC in models fitted by the `nls` command are substantially higher than those in the other models: it is not that these models are substantially worse than the others, it is simply that the `nls` command uses a constant different from the others to calculate the log-likelihood. Therefore, with R, it should be remembered that the values of AIC should be compared only for models that have been fitted using the same command.
In our present case, if we compare the two models that were fitted with the `lm` command, the best (i.e. that with the smallest AIC) is that which has $D^2H$ as effect variable (red line \@ref(exr:frlpD2H)). If we compare the five models fitted with the `nlme` command, the best is again that with $D^2H$ as effect variable (red line \@ref(exr:fD2Hvar)). And if we compare the three models fitted with the `nls` command, the best is yet again that with $D^2H$ as effect variable (red line \@ref(exr:fnlsD2H)). It may therefore be concluded that whatever fitting method is used, the biomass table that uses $D^2H$ as effect variable is the best.

::::::

\begin{filrouge}{}{selB}%
\end{filrouge}



