
# Model fitting {#fit}

Fitting a model consists in estimating its parameters from data. This assumes, therefore, that data are already available and formatted, and that the mathematical expression of the model to be fitted is known. For example, fitting the power model $B=aD^b$ consists in estimating coefficients $a$ and $b$ from a dataset that gives the values of $B_i$ and $D_i$ from the biomass and dbh of $n$ trees ($i=1$, \ldots, $n$). The response variable (also in the literature called the output variable or the dependent variable) is the variable fitted by the model. There is only one. In this guide, the response variable is always a volume or a biomass. Effect variables are the variables used to predict the response variable. There may be several, and their number is denoted by $p$. Care must be taken not to confuse effect variables and model entry data. The model $B=a(D^2H)^b$ possesses a single effect variable ($D^2H$) but two entries (dbh $D$ and height $H$). Conversely, the model $B=a_0+a_1D+a_2D^2$ possesses two effect variables ($D$ and $D^2$) but only a single entry (dbh $D$). Each effect variable is associated with a coefficient to be calculated. To this must be added, if necessary, the y-intercept or a multiplier such that the total number of coefficients to be calculated in a model with $p$ effect variables is $p$ or $p+1$.

An observation consists of the data forming the response variable (volume or biomass) and the effect variables for a tree. If we again consider the model $B=aD^b$, an observation consists of the doublet ($B_i$, $D_i$). The number of observations is therefore $n$. An observation stems from a measurement in the field. The prediction provided by the model is the value it predicts for the response variable from the data available for the effect variables. A prediction stems from a calculation. For example, the prediction provided by the model $B=aD^b$ for a tree of dbh $D_i$ is $\hat{B}_i=aD_i^b$. There are as many predictions as there are observations. A key concept in model fitting is the residual. The residual, or residual error, is the difference between the observed value of the response variable and its prediction. Again for the same example, the residual of the $i$th observation is: $\varepsilon_i=B_i-\hat{B}_i=B_i-aD_i^b$. There are as many residuals as there are observations. The lower the residuals, the better the fit. Also, the statistical properties of the model stem from the properties that the residuals are assumed to check *a priori*, in particular the form of their distribution. The type of a model's fitting is therefore directly dependent upon the properties of its residuals.

In all the models we will be considering here, the observations will be assumed to be independent or, which comes to the same thing, the residuals will be assumed to be independent: for each $i\neq j$, $\varepsilon_i$ is assumed to be independent of $\varepsilon_j$. This independence property is relatively easy to ensure through the sampling protocol. Typically, care must be taken to ensure that the characteristics of a tree measured in a given place do not affect the characteristics of another tree in the sample. Selecting trees that are sufficiently far apart is generally enough to ensure this independence. If the residuals are not independent, the model can be modified to take account of this. For example, a spatial dependency structure could be introduced into the residuals to take account of a spatial auto-correlation between the measurements. We will not be considering these models here as they are far more complex to use.

In all the models we will be looking at, we assume also that the residuals have a normal distribution with zero expectation. The zero mean of the residuals is in fact a property that stems automatically from model fitting, and ensures that the model's predictions are not biased. It is the residuals that are assumed to have a normal distribution, not the observations. This hypothesis in fact causes little constraint for volume or biomass data. In the unlikely case where the distribution of the residuals is far from normal, efforts could be made to fit other model types, e.g. the generalized linear model, but this will not be addressed here in this guide. The hypotheses that the residuals are independent and follow a normal distribution are the first two hypotheses underlying model fitting. We will see a third hypothesis later. It should be checked that these two hypotheses are actually valid. To the extent that these hypotheses concern model residuals, not the observations, they cannot be tested until they have been calculated, i.e. until the model has been fitted. These hypotheses are therefore checked a posteriori, after model fitting. The models we will look at here are also robust with regard to these hypotheses, i.e. the predictive quality of the fitted models is acceptable even when the independence and the normal distribution of the residuals are not perfectly satisfied. For this reason, we will not look to test these two hypotheses very formally. In practice we will simply perform a visual verification of the plots.


## Fitting a linear model

The linear model is the simplest of all models to fit. The word *linear* means here that the model is *linearly* dependent on its coefficients. For example, $Y=a+bX^2$ and $Y=a+b\ln(X)$ are linear models because the response variable $Y$ is linearly dependent upon coefficients $a$ and $b$, even if $Y$ is not linearly dependent on the effect variable $X $. Conversely, $Y=aX^b$ is not a linear model because $Y$ is not linearly dependent on coefficient $b$. Another property of the linear model is that the residual is additive. This is underlined by explicitly including the residual $\varepsilon$ in the model's formula. For example, a linear regression of $Y$ against $X$ will be written: $Y=a+bX+\varepsilon$.


### Simple linear regression

A simple linear regression is the simplest of the linear models. It assumes (*i*) that there is only one effect variable $X$, (*ii*) that the relation between the response variable $Y$ and $X$ is a straight line:
\[
Y=a+bX+\varepsilon
\]
where $a$ is the y-intercept of the line and $b$ its slope, and (*iii*) that the residuals are of constant variance: $\mathrm{Var}(\varepsilon)=\sigma^2$. For example, the model

\begin{equation}
\ln(B)=a+b\ln(D)+\varepsilon(\#eq:pow)
\end{equation}

is a typical simple linear regression, with response variable $Y=\ln(B)$ and effect variable $X=\ln(D)$. It corresponds to a power model for biomass: $B=\exp(a)D^b$. This model is often used to fit a single-entry biomass model. Another example is the double-entry biomass model:

\begin{equation}
\ln(B)=a+b\ln(D^2H)+\varepsilon(\#eq:powH)
\end{equation}

The hypothesis whereby the residuals are of constant variance is added to the two independence and normal distribution hypotheses (we also speak of homoscedasticity). These three hypotheses may be summarized by writing:
\[
\varepsilon\;\mathop{\sim}_{\mathrm{i.i.d.}}\;\mathcal{N}(0,\ \sigma)
\]
where $\mathcal{N}(\mu,\ \sigma)$ designates a normal distribution of expectation $\mu$ and standard deviation $\sigma$, the tilde "$\sim$" means "is distributed in accordance with", and "i.i.d." is the abbreviation of "independently and identically distributed".

```{r reglin, fig.cap="(ref:reglin)"}

display_fig("reglin")

```


#### Estimating coefficients {-}

Figure \@ref(fig:reglin) shows the observations and the plot of predicted values. The best fit is that which minimizes the residual error. There are several ways of quantifying this residual error. From a mathematical standpoint, this is equivalent with choosing a norm to measure $\varepsilon$ and various norms could be used. The norm that is commonly used is the $L_2$ norm, which is equivalent with quantifying the residual difference between the actual observations and the predictions by summing the squares of the residuals, which is also called the sum of squares (SS):
\[
\mathrm{SSE}(a,b)=\sum_{i=1}^n\varepsilon_i^2=\sum_{i=1}^n(Y_i-\hat{Y}_i)^2
=\sum_{i=1}^n(Y_i-a-bX_i)^2
\]
The best fit is therefore that which minimizes SS. In other words, the estimations $\hat{a}$ and $\hat{b}$ of the coefficients $a$ and $b$ are values of $a$ and $b$ that minimize the sum of squares:
\[
(\hat{a},\ \hat{b})=\arg\min_{(a,\ b)}\mathrm{SSE}(a,b)
\]
This minimum may be obtained by calculating the partial derivatives of SS in relation to $a$ and $b$, and by looking for the values of $a$ and $b$ that cancel these partial derivatives. Simple calculations yield the following results: $\hat{b}=\widehat{\mathrm{Cov}}(X,\ Y)/S_X^2$ and $\hat{a}=\bar{Y}-\hat{b}\bar{X}$, where $\bar{X}=(\sum_{i=1}^nX_i)/n$ is the empirical mean of the effect variable, $\bar{Y}=(\sum_{i=1}^nY_i)/n$ is the empirical mean of the response variable,
\[
S_X^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2
\]
is the empirical variance of the effect variable, and
\[
\widehat{\mathrm{Cov}}(X,\ Y)=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})
\]
is the empirical covariance between the effect variable and the response variable. The estimation of the residual variance is:
\[
\hat{\sigma}^2=\frac{1}{n-2}\sum_{i=1}^n(Y_i-\hat{a}-\hat{b}X_i)^2
=\frac{\mathrm{SSE}(\hat{a},\ \hat{b})}{n-2}
\]
Because this method of estimating the coefficients is based on minimizing the sum of squares, it is called the *least squares* method (sometimes it is called the "ordinary least squares" method to differentiate it from the weighted least squares method we will see in \S\ \@ref(pond)). This estimation method has the advantage of providing an explicit expression of the estimated coefficients.


#### Interpreting the results of a regression {-}

When fitting a simple linear regression, several outputs need to be analyzed. The determination coefficient, more commonly called $R^2$, measures the quality of the fit. $R^2$ is directly related to the residual variance since:
\[
R^2=1-\frac{\hat{\sigma}^2(n-2)/n}{S_Y^2}
\]
where $S_Y^2=[\sum_{i=1}^n(Y_i-\bar{Y})^2]/n$ is the empirical variance of $Y$. The difference $S_Y^2-\hat{\sigma}^2(n-2)/n$ between the variance of $Y$ and the residual variance corresponds to the variance explained by the model. The determination coefficient $R^2$ can be interpreted as being the ratio between the variance explained by the model and the total variance. It is between 0 and 1, and the closer it is to 1, the better the quality of the fit. In the case of a simple linear regression, and only in this case, $R^2$ is also the square of the linear correlation coefficient (also called Pearson's coefficient) between $X$ and $Y$. We have already seen in chapter \@ref(explo) (particularly in Figure \@ref(fig:badR2)) that the interpretation of $R^2$ has its limitations.

In addition to estimating values for coefficients $a$ and $b$, model fitting also provides the standard deviations of these estimations (i.e. the standard deviations of estimators $\hat{a}$ and $\hat{b}$), and the results of significance tests on these coefficients. A test is performed on the y-intercept $a$, which tests the null hypothesis that $a=0$, and likewise a test is performed on the slope $b$, which tests the hypothesis that $b=0$.

Finally, the result given by the overall significance test for the model is also analyzed. This test is based on breaking down the total variance of $Y$ into the sum of the variance explained by the model and the residual variance. Like in an analysis of variance, the test used is Fisher's test which, as test statistic, uses a weighted ratio of the explained variance over the residual variance. In the case of a simple linear regression, and only in this case, the test for the overall significance of the model gives the same result as the test on the null hypothesis $b=0$. This can be grasped intuitively: a line linking $X$ to $Y$ is significant only if its slope is not zero.


#### Checking hypotheses {-}

Model fitting may be brought to a conclusion by checking that the hypotheses put forward for the residuals are in fact satisfied. We will not consider here the hypothesis that the residuals are independent for this has already been satisfied thanks to the sampling plan adopted. If there is a natural order in the observations, we could possibly use the Durbin-Watson test to test if the residuals are indeed independent [@durbin71]. The hypothesis that the residuals are normally distributed can be checked visually by inspecting the quantile--quantile graph. This graph plots the empirical quantiles of the residuals against the theoretical quantiles of the standard normal distribution. If the hypothesis that the residuals are normally distributed is satisfied, then the points are approximately aligned along a straight line, as in  Figure \@ref(fig:res1) (right plot).

```{r res1, fig.cap="(ref:res1)"}

display_fig("resid1")

```

In the case of fitting volume or biomass models, the most important hypothesis to satisfy is that of the constant variance of the residuals. This can be checked visually by plotting the cluster of points for the residuals $\varepsilon_i=Y_i-\hat{Y}_i$ in function to predicted values $\hat{Y}_i=\hat{a}+\hat{b}X_i$.

 If the variance of the residuals is indeed constant, this cluster of points should not show any particular trend and no particular structure. This for instance is the case for the plot shown on the left in  Figure \@ref(fig:res1). By contrast, if the cluster of points shows some form of structure, the hypothesis should be questioned. This for instance is the case in  Figure \@ref(fig:res2) where the cluster of points for the residuals plotted against fitted values forms a funnel shape. This shape is typical of a residual variance that increases with the effect variable (which we call heteroscedasticity). If this is the case, a model other than a simple linear regression must be fitted.

```{r res2, fig.cap="(ref:res2)"}

display_fig("resid2")

```

In the case of biological data such as tree volume or biomass, heteroscedasticity is the rule and homoscedasticity the exception. This means simply that the greater tree biomass (or volume), the greater the variability of this biomass (or volume). This increase in the variability of biomass with increasing size is a general principle in biology. Thus, when fitting biomass or volume models, simple linear regression using biomass as response variable ($Y=B$) is generally of little use. The log transformation (i.e. $Y=\ln(B)$) resolves this problem and therefore the linear regressions we use for adjusting models nearly always use log-transformed data. We will return at length to this fundamental point later.

::::::{.filrouge data-latex=""}

:::{.exercise #rllnBvD name="(ref:rllnBvD)}
\  
:::

The exploratory analysis (red line \@ref(exr:feln)) showed that the relation between the log of the biomass and the log of the dbh was linear, with a variance of $\ln(B)$ that was approximately constant. A simple linear regression may therefore be fitted to predict $\ln(B)$from $\ln(D)$:
\[
\ln(B)=a+b\ln(D)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
The regression is fitted using the ordinary least squares method. Given that we cannot apply the log transformation to a value of zero, zero biomass data (see red line \@ref(exr:read)) must first be withdrawn from the dataset:

```{r, echo=T, eval=F}
m <- lm(log(Btot) ~ I(log(dbh)),data = dat[dat$Btot>0,])
summary(m)
```

The residual standard deviation is $\hat{\sigma}=0.462$, $R^2$ is 0.9642 and the model is highly significant (Fisher's test: $F_{1,39}=1051$, p-value $<2.2\times10^{-16}$). The values of the coefficients are given in the table below:

```{r}
summary(m)$coef
```

The first column in the table gives the values of the coefficients. The model is therefore: $\ln(B)=-8.42722+2.36104\ln(D)$. The second column gives the standard deviations of the coefficient estimators. The third column gives the result of the test on the null hypothesis that the coefficient is zero. Finally, the fourth column gives the p-value of this test. In the present case, both the slope and the y-intercept are significantly different from zero. We must now check graphically that the hypotheses of the linear regression are satisfied:

```{r, echo=T, eval=F}
plot(m,which=1:2)
```

The result is shown in Figure \@ref(fig:fBvDres). Even though the quantile--quantile plot of the residuals appears to have a slight structure, we will consider that the hypotheses of the simple linear regression are suitably satisfied.

::::::


```{r fBvDres, fig.cap="(ref:fBvDres)"}

display_fig("fBvDres")

```


::::::{.filrouge data-latex=""}

:::{.exercise #rllnBvD2H name="(ref:rllnBvD2H)"}
\  
:::

The exploratory analysis (red line \@ref(exr:feln2)) showed that the relation between the log of the biomass and the log of $D^2H$ was linear, with a variance of $\ln(B)$ that was approximately constant. We can therefore fit a simple linear regression to predict $\ln(B)$ from $\ln(D^2H)$:
\[
\ln(B)=a+b\ln(D^2H)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
The regression is fitted by the ordinary least squares method. Given that we cannot apply the log transformation to a value of zero, zero biomass data (see red line \@ref(exr:read)) must first be withdrawn from the dataset:

```{r, echo=T, eval=F}
m <- lm(log(Btot) ~ I(log(dbh^2*heig)),data=dat[dat$Btot>0,])
summary(m)
```

The residual standard deviation is $\hat{\sigma}=0.4084$, $R^2$ is 0.972 and the model is highly significant (Fisher's test: $F_{1,39}=1356$, p-value $<2.2\times10^{-16}$). The values of the coefficients are as follows:

```{r}
summary(m)$coef
```

The first column in the table gives the values of the coefficients. The model is therefore: $\ln(B)=-8.99427+0.87238\ln(D^2H)$. The second column gives the standard deviations of the coefficient estimators. The third column gives the result of the test on the null hypothesis that "the coefficient is zero". Finally, the fourth column gives the p-value of this test. In the present case, both the slope and the y-intercept are significantly different from zero.

We must now check graphically that the hypotheses of the linear regression are satisfied:

```{r, echo=T, eval=F}
plot(m,which=1:2)
```

The result is shown in  Figure \@ref(fig:fBvD2Hres). Even though the plot of the residuals against the fitted values appears to have a slight structure, we will consider that the hypotheses of the simple linear regression are suitably satisfied.

::::::


```{r fBvD2Hres, fig.cap="(ref:fBvD2Hres)"}

display_fig("fBvD2Hres")

```


### Multiple regression

Multiple regression is the extension of simple linear regression to the case where there are several effect variables, and is written:

\begin{equation}
Y=a_0+a_1X_1+a_2X_2+\ldots+a_pX_p+\varepsilon(\#eq:regmul)
\end{equation}

where $Y$ is the response variable, $X_1$, \ldots, $X_p$ the $p$ effect variables, $a_0$, \ldots, $a_p$ the coefficients to be estimated, and $\varepsilon$ the residual error. Counting the y-intercept $a_0$, there are $p+1$ coefficients to be estimated. Like for simple linear regression, the variance of the residuals is assumed to be constant and equal to $\sigma^2$:
\[
\varepsilon\;\mathop{\sim}_{\mathrm{i.i.d.}}\;\mathcal{N}(0,\ \sigma)
\]
The following biomass models are examples of multiple regressions:

\begin{eqnarray}
\ln(B) &=& a_0+a_1\ln(D^2H)+a_2\ln(\rho)+\varepsilon(\#eq:powHs)\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+\varepsilon(\#eq:powHd)\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2\ln(H)+a_3\ln(\rho)+\varepsilon(\#eq:powHsd)\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+\varepsilon(\#eq:powd)\\ %
\ln(B) &=& a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)+\varepsilon(\#eq:powsd)%
\end{eqnarray}

where $\rho$ is wood density. In all these examples the response variable is the log of the biomass: $Y=\ln(B)$. As model \@ref(eq:powHs) generalizes \@ref(eq:powH) by adding dependency on wood specific density: typically \@ref(eq:powHs) should be preferred to \@ref(eq:powH) when the dataset is multispecific. Model \@ref(eq:powHd) generalizes \@ref(eq:powH) by considering that the exponent associated with height $H$ is not necessarily half the exponent associated with dbh. It therefore introduces a little more flexibility into the form of the relation between biomass and $D^2H$. Model \@ref(eq:powHsd) generalizes \@ref(eq:powH) by considering that there are both several species and that biomass is not quite a power of $D^2H$. Model \@ref(eq:powd) generalizes \@ref(eq:pow) by considering that the relation between $\ln(B)$ and $\ln(D)$ is not exactly linear. It therefore offers a little more flexibility in the form of this relation. Model \@ref(eq:powsd) is the extension of \@ref(eq:powd) to take account of the presence of several species in a dataset.


#### Estimating coefficients {-}

In the same manner as for simple linear regression, the estimation of the coefficients is based on the least squares method. Estimators $\hat{a}_0$, $\hat{a}_1$, \ldots, $\hat{a}_p$ are the values of coefficients $a_0$, $a_1$, \ldots, $a_p$ that minimize the sum of squares:
\[
\mathrm{SSE}(a_0,\ a_1,\ \ldots,\ a_p)=\sum_{i=1}^n\varepsilon_i^2
=\sum_{i=1}^n(Y_i-\hat{Y}_i)^2
=\sum_{i=1}^n(Y_i-a_0-a_1X_{i1}-\ldots-a_pX_{ip})^2
\]
where $X_{ij}$ is the value of the $j$th effect variable for the $i$th observation ($i=1$, \ldots, $n$ and $j=1$\  \ldots, $p$). Once again, estimations of the coefficients may be obtained by calculating the partial derivatives of SS in relation to the coefficients, and by looking for the values of the coefficients that cancel these partial derivatives. These computations are barely more complex than for simple linear regression, on condition that they are arranged in a matrix form. 
Let $\matx{X}$ be the matrix with $n$ lines and $p$ columns, called the design matrix, containing the values observed for the effect variables:
\[
\matx{X}=\left[
\begin{array}{cccc}
1      & X_{11} & \cdots & X_{1p}\\ %
\vdots & \vdots &        & \vdots\\ %
1      & X_{n1} & \cdots & X_{np}\\ %
\end{array}
\right]
\]
Let $\vect{Y}=\tr{[Y_1,\ \ldots,\ Y_n]}$ be the vector of the $n$ values observed for the response variable, and $\vect{a}=\tr{[a_0,\ \ldots,\ a_p]}$ be the vector of the $p+1$ coefficients to be estimated. Thus
\[
\matx{X}\vect{a}=\left[
\begin{array}{c}
a_0+a_1X_{11}+\ldots+a_pX_{1p}\\ %
\vdots\\ %
a_0+a_1X_{n1}+\ldots+a_pX_{np}\\ %
\end{array}
\right]
\]
is none other than the vector $\hat{\vect{Y}}$  of the $n$ values fitted by the response variable model. Using these matrix notations, the sum of squares is written:
\[
\mathrm{SSE}(\vect{a})=\tr{(\vect{Y}-\hat{\vect{Y}})}
(\vect{Y}-\hat{\vect{Y}})=\tr{(\vect{Y}-\matx{X}\vect{a})}
(\vect{Y}-\matx{X}\vect{a})
\]
And using matrix differential calculus [@magnus07], we finally obtain:
\[
\hat{\vect{a}}=\arg\min_{\vect{a}}\mathrm{SSE}(\vect{a})
=(\tr{\matx{X}}\matx{X})^{-1}\tr{\matx{X}}\vect{Y}
\]
The estimation of the residual variance is:
\[
\hat{\sigma}^2=\frac{\mathrm{SSE}(\hat{\vect{a}})}{n-p-1}
\]
Like for simple linear regression, this estimation method has the advantage of providing an explicit expression of the coefficients estimated. As simple linear regression is a special case of multiple regression (case where $p=1$), we can check that the matrix expressions for estimating the coefficients and $\hat{\sigma}$ actually --- when $p=1$ --- give again the expressions given previously with a simple linear regression.


#### Interpreting the results of a multiple regression {.unnumbered #irm}

In the same manner as for simple linear regression, fitting a multiple regression provides a determination coefficient $R^2$ corresponding to the proportion of the variance explained by the model, values $\hat{\vect{a}}$ for model coefficients $a_0$, $a_1$, \ldots, $a_p$, standard deviations for these estimations, the results of significance tests on these coefficients (there are $p+1$ --- one for each coefficient --- null hypotheses $a_i=0$ for $i=0$, \ldots, $p$), and the result of the test on the overall significance of the model.

As previously, $R^2$ has a value of between 0 and 1. The higher the value, the better the fit. However, it should be remembered that the value of $R^2$ increases automatically with the number of effect variables used. For instance, if we are predicting $Y$ using a polynomial with $p$ orders in $X$,
\[
Y=a_0+a_1X+a_2X^2+\ldots+a_pX^p
\]
$R^2$ will automatically increase with the number of orders $p$. This may give the illusion that the higher the number of orders $p$ in a polynomial, the better its fit. This of course is not the case. If the number $p$ of orders is too high, this will result in model over-parameterization. In other words, $R^2$ is not a valid criterion on which model selection may be based. We will return to this point in section \@ref(select).


#### Checking hypotheses {-}

Like simple linear regression, multiple regression is based on three hypotheses: that the residuals are independent, that they follow a normal distribution and that their variance is constant. These hypotheses may be checked in exactly the same manner as for the simple linear regression. To check that the residuals follow a normal distribution, we can plot a quantile--quantile graph and verify visually that the cluster of points forms a straight line. To check that the variance of the residuals is constant, we can plot the residuals against the fitted values and verify visually that the cluster of points does not show any particular trend. The same restriction as for the simple linear regression applies to volume or biomass data that nearly always (or always) show heteroscedasticity. For this reason, multiple regression is generally applicable for fitting models only on log-transformed data.

::::::{.filrouge data-latex=""}

:::{.exercise #flnDpol name="(ref:flnDpol)"}
\  
:::

::::::

\begin{filrouge}{}{flnDpol}%
The exploratory analysis (red line \ref{feln}) showed that the relation between the log of the biomass and the log of the dbh was linear. We can ask ourselves the question of whether this relation is truly linear, or has a more complex shape. To do this, we must construct a polynomial regression with $p$ orders, i.e. a multiple regression of $\ln(B)$ against $\ln(D)$, $[\ln(D)]^2$, \ldots, $[\ln(D)]^p$:

\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+\ldots+a_p[\ln(D)]^p+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
The regression is fitted by the ordinary least squares method. As the log transformation stabilizes the residual variance, the hypotheses on which the multiple regression is based are in principle satisfied. For a second-order polynomial, the polynomial regression is fitted by the following code:

\R{m2 <- lm(log(Btot)$\sim$I(log(dbh))+I(log(dbh)\^{}2),data=dat[dat\$Btot>0,])%
\\ print(summary(m2))}%
This yields:
\Rout{\begin{tabular}{lrrrrl}%
                 &  Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)      & -8.322190 &   1.031359 &  -8.069 & 9.25e-10 & ***\\ %
I(log(dbh))      &  2.294456 &   0.633072 &   3.624 & 0.000846 & ***\\ %
I(log(dbh)\^{}2) &  0.009631 &   0.090954 &   0.106 & 0.916225 &    \\ %
\end{tabular}}%
with $R^2=\decimal{0}{9642}$. And as for a third-order polynomial regression:
\R{m3 <- lm(log(Btot)$\sim$I(log(dbh))+I(log(dbh)\^{}2)+I(log(dbh)\^{}3),data=dat[dat\$Btot>0,])%
\\ print(summary(m3))}%
it yields:
\Rout{\begin{tabular}{lrrrr}%
                 & Estimate & Std. Error & t value & Pr(>|t|)\\ %
(Intercept)      & -5.46413 &    3.80855 &  -1.435 &    0.160\\ %
I(log(dbh))      & -0.42448 &    3.54394 &  -0.120 &    0.905\\ %
I(log(dbh)\^{}2) &  0.82073 &    1.04404 &   0.786 &    0.437\\ %
I(log(dbh)\^{}3) & -0.07693 &    0.09865 &  -0.780 &    0.440\\ %
\end{tabular}}%
with $R^2=\decimal{0}{9648}$. Finally, a fourth-order polynomial regression:
\R{m4 <-
lm(log(Btot)$\sim$I(log(dbh))+I(log(dbh)\^{}2)+I(log(dbh)\^{}3)+I(log(dbh)\^{}4),data=dat[\bidouille{\newline}
dat\$Btot>0,])%
\\ print(summary(m4))}%
yields:
\Rout{\begin{tabular}{lrrrrl}%
                 & Estimate & Std. Error & t value & Pr(>|t|) &  \\ %
(Intercept)      & -26.7953 &    15.7399 &  -1.702 &   0.0973 & .\\ %
I(log(dbh))      &  26.3990 &    19.5353 &   1.351 &   0.1850 &  \\ %
I(log(dbh)\^{}2) & -11.2782 &     8.7301 &  -1.292 &   0.2046 &  \\ %
I(log(dbh)\^{}3) &   2.2543 &     1.6732 &   1.347 &   0.1863 &  \\ %
I(log(dbh)\^{}4) &  -0.1628 &     0.1166 &  -1.396 &   0.1714 &  \\ %
\end{tabular}}%
with $R^2=\decimal{0}{9666}$. Adding higher order terms above 1 therefore does not improve the model. The coefficients associated with these terms were not significantly different from zero. But the model's $R^2$ continued to increase with the number $p$ of orders in the polynomial. $R^2$ is not therefore a reliable criterion for selecting the order of the polynomial. The plots fitted by these different polynomials may be superimposed on the biomass-dbh cluster of points: with object \texttt{m} indicating the linear regression of $\ln(B)$ against $\ln(D)$ fitted in red line \ref{rllnBvD},

\R{with(dat,plot(dbh,Btot,xlab="Dbh (cm)",ylab="Biomass (t)",log="xy"))%
\\ D <- 10\^{}seq(par("usr")[1],par("usr")[2],length=200)%
\\ lines(D,exp(predict(m,newdata=data.frame(dbh=D))))%
\\ lines(D,exp(predict(m2,newdata=data.frame(dbh=D))))%
\\ lines(D,exp(predict(m3,newdata=data.frame(dbh=D))))%
\\ lines(D,exp(predict(m4,newdata=data.frame(dbh=D))))}%

The plots are shown in  Figure \@ref(fig:fDpol): the higher the order of the polynomial, the more deformed the plot in order to fit the data, with increasingly unrealistic extrapolations outside the range of the data (typical of model over-parameterization).

\end{filrouge}

\begin{figure}[tbhp]
\includegraphics[width=\textwidth]{fDpol}
\caption[Biomass against dbh for 42 trees in Ghana measured by @henry10, and predictions by a polynomial regression of $\ln(B)$ against $\ln(D)$: (A) first-order polynomial; (B) second-order polynomial; (C) third-order polynomial; (D) fourth-order polynomial.]{Biomass against dbh for 42 trees in Ghana measured by @henry10 (points), and predictions (plots) by a polynomial regression of $\ln(B)$ against $\ln(D)$: (A) first-order polynomial (straight line); (B) second-order polynomial (parabolic); (C) third-order polynomial; (D) fourth-order polynomial.\label{fDpol}}
\end{figure}

\begin{filrouge}{Multiple regression between $\ln(B)$, $\ln(D)$ and $\ln(H)$}{flnDlnH}%

The graphical exploration (red lines \ref{feBvD2H} and \ref{feln2}) showed that the combined variable $D^2H$ was linked to biomass by a power relation (i.e. a linear relation on a log scale): $B=a(D^2H)^b$. We can, however, wonder whether the variables $D^2$ and $H$ have the same exponent $b$, or whether they have different exponents: $B=a\times(D^2)^{b_1}H^{b_2}$. Working on the log-transformed data (which in passing stabilizes the residual variance), means fitting a multiple regression of $\ln(B)$ against $\ln(D)$ and $\ln(H)$:
\[
\ln(B)=a+b_1\ln(D)+b_2\ln(H)+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]
The regression is fitted by the ordinary least squares method. Fitting this multiple regression:

\R{m <- lm(log(Btot)$\sim$I(log(dbh))+I(log(haut)),data=dat[dat\$Btot>0,])%
\\ summary(m)}%
yields:
\Rout{\begin{tabular}{lrrrrl}%
             & Estimate & Std. Error & t value & Pr(>|t|) &    \\ %
(Intercept)  &  -8.9050 &     0.2855 & -31.190 &  < 2e-16 & ***\\ %
I(log(dbh))  &   1.8654 &     0.1604 &  11.632 & 4.35e-14 & ***\\ %
I(log(haut)) &   0.7083 &     0.2097 &   3.378 &  0.00170 & ** \\ %
\end{tabular}}%
with a residual standard deviation of  \decimal{0}{4104} and $R^2=\decimal{0}{9725}$. 
The model is highly significant (Fisher's test: $F_{2,38}=\decimal{671}{5}$, p-value $<\decimal{2}{2}\times10^{-16}$). The model, all of whose coefficients are significantly different from zero, is written: $\ln(B)=-\decimal{8}{9050}+\decimal{1}{8654}\ln(D)+\decimal{0}{7083}\ln(H)$. By applying the exponential function to return to the starting data, the model becomes: $B=\decimal{1}{357}\times10^{-4}D^{1.8654}H^{0.7083}$. The exponent associated with height is a little less than half that associated with dbh, and is a little less than the exponent of \decimal{0}{87238} found for the combined variable $D^2H$ (see red line \ref{rllnBvD2H}). An examination of the residuals:

\R{plot(m,which=1:2)}%
shows nothing in particular (Figure \@ref(fig:flnDHres)).
\end{filrouge}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{flnDlnHres}
\caption[Residuals plotted against fitted values and quantile--quantile plot for residuals of the multiple regression of $\ln(B)$ against $\ln(D)$ and $\ln(H)$ fitted for the 42 trees measured by @henry10 in Ghana]{Residuals plotted against fitted values (left) and quantile--quantile plot (right) for residuals of the multiple regression of $\ln(B)$ against $\ln(D)$ and $\ln(H)$ fitted for the 42 trees measured by @henry10 in Ghana.\label{flnDHres}}
\end{figure}


\subsection{Weighted regression\label{pond}}

