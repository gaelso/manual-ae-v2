

# Entering and formatting data {#don}

Once the field measurements phase has been completed, and before any analysis, the data collected need to be given structure, which includes data entry, cleansing and formatting.




## Data entry


This consists in transferring the data recorded on the field forms into a computer file. The first step is to choose the software. For a small dataset, a spreadsheet such as Microsoft Excel or OpenOffice Calc may be sufficient. For more substantial campaigns, a database management system such as Microsoft Access or MySQL (www.mysql.com) should be used.



### Data entry errors

The data must be entered as carefully as possible to minimize errors. One of the methods used to achieve this is double data entry: one operator enters the data, a second operator (if possible different from the first) repeats the data entry in a manner entirely independent from the first. The two files can then simply be compared to detect any data entry errors. As it is unlikely that the two operators will make the same data entry error, this method guarantees good quality data entry. But it is time consuming and tedious.

Certain details, that have their importance, must also be respected during data entry. First, numbers must be distinguished from character strings. For the statistical software that will subsequently process the data, a number does not have the same role as a character string. This distinction must therefore be made during data entry. A number will be interpreted as the value of a numerical variable, whereas a character string will be interpreted as the category of a qualitative variable. The difference between the two is generally obvious, but not always. Let us, for instance, consider latitudes and longitudes. If we want to calculate the correlation between the latitude or the longitude and another variable (to identify a north-south or east-west gradient), the software must perceive the geographic coordinates as being numbers. These geographic coordinates must not therefore be entered as $7^\circ28'55.1\text{"}$ or $13^\circ41'25.9\text{"}$ as they would be interpreted as qualitative variables, and no calculations would be possible. A possible solution consists in converting geographic coordinates into decimal values. Another solution consists in entering geographic coordinated in three columns (one for degrees, one for minutes, and the third for seconds).

When entering qualitative variables, care must be taken to avoid entering strings of great length as this enhances the risk of making data entry errors. It is far better to enter a short code and in the meta-information (see below) give the meaning of the code.

Another important detail is the decimal symbol used. Practically all statistical software packages can switch from the comma (symbol used in French) to the period (symbol used in English), therefore one or the other may be used. By contrast, if you choose to use a comma or a period as the decimal symbol, you must stick to this throughout the data entry operation. If you use one, then the other, some of the normally numerical data will be interpreted by the statistical software as character strings.



### Meta-information

When entering data, some thought must be given to the meta-information, i.e. the information that accompanies the data, without in itself being measured data. Meta-information will, for instance, give the date on which measurements were taken, and by whom. If codes are used during data entry, the meta-information will specify what these codes mean. For example, it is by no means rare for the names of species to be entered in a shortened form. A species code such as ANO in dry, west African savannah for instance is ambiguous: it could mean *Annona senegalensis* or *Anogeissus leiocarpus*. The meta-information is there to clear up this ambiguity. This meta-information must also specify the nature of the variables measured. For example, if tree girth is measured, then noting "girth" in the data table is in itself insufficient. The meta-information must specify at what height tree girth was measured (at the base, at 20 cm, at 1.30\ m...) and, an extremely important point, in which units this girth is expressed (in cm, dm...). It is essential that the units in which each variable was measured must be specified in the meta-information. All too often, data tables fail to specify in which units the data are expressed, and this gives rise to extremely hazardous guessing games.

For the people who designed the measuring system and supervised the actual taking of measurements, these details given in the meta-information are often obvious and they do not see the need to spend time providing this information. But let us imagine the situation facing persons who were not involved in the measurements and who review the data 10 years later. If the meta-information has been correctly prepared, these persons should be able to work on the dataset as if they had constituted it themselves.



### Nested levels

The data should be entered in the spreadsheets as one line per individual. If the data include several nested levels, then as many data tables as there are levels should be prepared. Let us imagine that we are looking to construct a table for high forest coppice formations on a regional scale. For multistemmed individuals, the volume of each stem is calculated separately. Individuals should be selected within the land plots, themselves selected within the forests distributed across the study area. This therefore gives rise to four nested levels: the forest, which includes several plots; the plot, which includes several trees; the tree, which includes several stems; and finally the stem. Four data tables should therefore be prepared (Figure \@ref(fig:niv)). Each data table will include the data describing the individuals of the corresponding level, with one worksheet line per individual. For example, this first data table will give the area of each forest. The second will give the geographic coordinates of each plot. The third data table will give the species and its coordinates for each tree in the plot. Finally, the fourth data table will give the volume and size of each stem. Therefore, to each line in a data table will correspond several lines in the lower level data table. A code should be used to establish the correspondence between the lines in the different data tables. For instance, the number of the forest should be repeated in the "forest" and "plot", data tables, the number of the plot should be repeated in the "plot" and "tree", data tables, and the number of the tree should be repeated in the "tree" and "stem" data tables (Figure \@ref(fig:niv)).

```{r niv, out.width='70%', fig.cap="(ref:niv)"}

display_fig("nested")

```


```{r one}

kab_opt <- list(
  x = kab_one,
  caption = "(ref:one)",
  col.names = c("forest", "area", "plot", "longitude", "latitude", "tree", "species", "$x$", "$y$", "stem", "$D$", "$H$", "$V$"),
  booktabs = TRUE,
  escape = F,
  align = "ccccccccccccc"
)

display_table()

```

Giving the data this structure minimizes the amount of information entered, and therefore minimizes data entry errors. An alternative consists in entering all the data in a single data table, as indicated for instance in Table \@ref(tab:one). This is not recommended as data is unnecessarily repeated and therefore the risk of making data entry errors is greatly increased. For instance, in Table \@ref(tab:one) we have deliberately introduced a data entry error in the second line of the data table, where the area of forest 1, which should be 400 ha, here is 401 ha. By unnecessarily repeating the information, we find ourselves multiplying this type of inconsistency that must subsequently be corrected.

An effective method that can be used to resolve these nested level problems is to build a relational database. This type of database is specifically intended to manage different inter-related data tables. It removes all inconsistencies, such as that illustrated in Table \@ref(tab:one) by systematically checking relational integrity between the tables. But building a relational database requires technical knowledge, and a person with skills in this field may need to be called upon.

In brief, when entering data, it is advisable to:

- avoid repeating the same information,

- prefer relational databases,

- provide additional information (meta-information),

- take great care with units,

- distinguish between qualitative and quantitative information,

- check the data,

- reduce or correct missing data.




## Data cleansing


Data cleansing requires toing-and-froing between the data record forms and the statistical software (or use of specially designed data cleansing software). This step aims to remove all data inconsistencies. And, if the measurement system is still in place, it may require certain measurements to be repeated. Data cleansing removes:

- aberrant data. For example, a tree 50 meters in diameter.

- inconsistent data. For example, a tree with a trunk biomass of 755 kg and a total biomass of 440 kg, or a tree that is 5 cm in diameter and 40 m in height.

- false categories for qualitative variables. For example, software that differentiates between upper case and lower case will interpret "yes" and "Yes" as two different categories, whereas in fact they are the same.

The difficulty inherent to detecting aberrant data lies in choosing the threshold between what is a normal measurement and what is an aberrant measurement. It may happen that aberrant data stem from a change in unit during data entry. If the field data record form mentions 1.2 kg then 900 g for leaf biomass measurements, care must be taken to enter 1.2 and 0.9 (en kg), or 1200 and 900 (in\ g), not 1.2 and 900. Inconsistent data are more difficult to detect as they require the comparison of several variables one with another. In the example given above, it is perfectly normal for a tree to have a trunk biomass of 755 kg, as it is for a tree to have a total biomass of 440 kg, but obviously the two values cannot be simultaneously correct for the same tree. Likewise, there is nothing abnormal about a tree having a diameter of 5 cm, or a tree being 40 m high. It is simply that a tree with a diameter of 5 cm and a height of 40 m is abnormal.

Aberrant data and data inconsistencies can be detected using descriptive statistics and by plotting two variables one against the other: an examination of means, quantiles, and maximum and minimum values, will often detect aberrant data; plots of two variables one against the other can be used to detect inconsistent data. For instance, in the above example, we could plot total biomass against trunk biomass, and check that all the points are located above the line $y=x$. Plots of height against diameter, volume against diameter, etc. can also be used to detect abnormal data. The categories describing qualitative variables could also be inspected by counting the number of observations per category. Two qualitative variables could then be crossed by building the corresponding contingency table. It should be assumed, during this inspection, that the statistical software has correctly interpreted numerical data as numerical data and qualitative data as qualitative data.

False categories often stem from sloppy data entry. Involuntary spelling mistakes are often made when entering the names of species in full, and this generates false categories. False categories may also be very ambiguous and difficult to correct. Let us consider the example of a dataset of trees in Central African wet tropical forest which, among other things, contains two species: alombi (*Julbernardia seretii*) and ilomba (*Picnanthus angolensis*). Let us assume that, by mistake, the false category "alomba" was entered. This false category differs from the true alombi and ilomba categories by only one letter. But which is the right category? Accents are also often the source of false categories, depending on whether text was entered with or without accents. Let us for example consider the entering of a color: for the person entering the data (in French) it may be perfectly clear that "vert foncÃ©" and "vert fonce" are the same thing, but the software will consider these as two different categories. Masculines and feminines (again in French) may also pose a problem. "Feuille vert clair" and "feuille verte clair" will be understood by the software as two different categories. A false category often found is generated by a space. The "green" category (no space) and the "green`r ifelse(book_format=="latex", "\\textvisiblespace", "&blank;")`" category (with a space, shown here by "`r ifelse(book_format=="latex", "\\textvisiblespace", "&blank;")`") will be understand by the software as two different categories. This false category is particularly difficult to detect as the space is invisible on the screen and the user therefore has the impression that the two categories are the same. All invisible characters (carriage return, tabulation, etc.) and characters that appear in the same fashion on the screen but have different ASCII codes, may generate the same type of confusing error.

False categories can be avoided by using data entry templates that restrict the entering of qualitative variable to a choice among admissible categories. Automatic data cleansing scripts, that remove incorrect spaces, check accents or letter upper or lower case, and check that qualitative variables take their value from a list of admissible categories, are a necessity for large datasets.




## Data formatting


This formatting consists in organizing the data into a format suitable for the calculations needed to construct the model. Typically, this consists of a model with one line per statistical individual (a tree for an individual model, a plot for a stand table) and as many columns as there are descriptive variables (i.e. variables predicting biomass, volume...), and as there are effect variables (dbh, height...). This formatting phase may require advanced data handling techniques. In some cases the data will need to be aggregated from one descriptive level to another. For example, if we are looking to construct an individual model for a coppice, and the measurements were made on stems, the data concerning the stems of a given stump will need to be aggregated: add together the volumes and masses, and calculate the equivalent diameter (i.e. the quadratic mean) of the stump from the diameters of its stems. Another example is the construction of a stand model from measurements in individual trees. In this case the data regarding the trees need to be aggregated into data characterizing the stand (volume per hectare, dominant height, etc.). In the opposite case, datasets need to be split. For example, we have calculated the volume of trees selected at random in a multispecific stand, and we are looking to construct a separate model for the five dominant species: the dataset needs to be split on the basis of tree species.

This data formatting will be greatly facilitated if the data were initially entered in an appropriate format. In this matter, relational databases have the advantage of offering a search language that can be used to construct the appropriate tables easily. In Microsoft Excel, the notion of "pivot table" can also usefully be employed to format data.


::::: {.filrouge data-latex=""}

(@eq-read) (ref:read) *(\@eq- counter: not full red / 1 to n / cross ref but no link and no list)*

:::{.exercise #read name="(ref:read)"}
\ 
*(hacking exercise environmment:not full red / section based number / cross ref and listing available)* 
:::

To illustrate specific points in this guide we will be using a dataset collected in Ghana by @henry10. This dataset gives the dry biomass of 42 trees belonging to 16 different species in a wet tropical forest. The diameter at breast height of each tree was measured, together with its height, the diameter of its crown, the mean density of its wood, its volume, and its dry biomass in five compartments: branches, leaves, trunk, buttresses, and total biomass.

Table \@ref(tab:data) gives the data @henry10 as they should be presented in a spreadsheet. The data table in the spreadsheet takes the form of a data rectangle; it should not contain any empty lines or empty columns, and its layout must not deviate from this matrix-like format. Decorative effects, indentations, cells left empty to "lighten" its appearance are to be avoided as the statistical software will be unable to read a dataset that deviates from a matrix format. Column headings should be reduced to short words, or even abbreviations. Information on the significance of these variables, and their units, should be presented separately in the meta-information.

If information needs to be entered about species, this should be entered in a second table as this involves two nested levels: the species level, with several trees per species, and the tree level nested in the species level. Thus, if we are looking to enter the ecological guild and the vernacular name of the species, this will generate a second Table \@ref(tab:datasp) specific to the species where the scientific name of the species is used to create the link between Table \@ref(tab:data) and Table \@ref(tab:datasp).


#### Data reading. {-}

Let us suppose that the data formatted in the matrix format have been saved in the Excel file
`Henry_et_al2010.xls`, the first worksheet of which, called `biomass`, contains Table \@ref(tab:data). In R software, these data are read using the commands:

```{r, eval=FALSE, echo=TRUE}
library(RODBC)%
ch <- odbcConnectExcel("Henry_et_al2010.xls")
dat <- sqlFetch(ch,"biomass")
odbcClose(ch)
```

```{r}
## Load the data
dat <- read_csv("source/data/henry_et_al2010.csv", col_types = cols())

```

The data are then stored in the object `dat`.


#### Data cleansing. {-}

A few checks may be performed to verify data quality. In R, the command `summary` generates basic descriptive statistics for the variables in a data table:

```{r, echo=T, eval=F}
summary(dat)
```

In particular for diameter at breast height, the result is:

```{r}
summary(dat$dbh)
```

Thus tree dbh varies from 2.6\ cm to 180\ cm, with a mean of 58.59\ cm and a median of 59.25\ cm. Basic descriptive statistics for total dry biomass `Btot` are as follows:

```{r}
summary(dat$Btot)
```

The total dry biomass of the largest tree is 70.24\ tonnes. The dry biomass of the smallest tree in the dataset is zero. As biomass is expressed in tonnes to two decimal places, this zero value is not aberrant, it means simply that the dry biomass of this tree is less than 0.01\ tonnes = 10 kg. This zero value will nevertheless pose a problem in the future when we want to log-transform the data.

Finally, we can check that the total dry biomass is indeed the sum of the biomasses of the four other compartments:

```{r, echo=T, eval=F}
max(abs(dat$Btot-rowSums(dat[,c("Bbran","Bfol","Btronc","Bctf")])))
```

The greatest difference in absolute value is 0.01\ tonnes, which indeed corresponds to the precision of the data (two significant figures). The dataset therefore does not contain any inconsistencies at this level.

::::::


```{r data}

kab_opt <- list(
  x = dat,
  caption = "(ref:data)",
  caption.short = "(ref:datashort)",
  #col.names = c(),
  #align = "ccccccccccccc",
  booktabs = TRUE,
  escape = F
)

if (book_format == "html") {
  display_table() %>%  scroll_box(height = "500px", box_css = "margin-bottom: 1.2em;")
} else {
  display_table(.latex_scaling = "scale_down")
}

```

```{r datasp}

kab_opt <- list(
  x = kab_datasp,
  caption = "(ref:datasp)",
  #caption.short = "(ref:)",
  #col.names = c(),
  #align = "ccccccccccccc",
  booktabs = TRUE,
  escape = F
)

display_table()

```
