

## Fitting a non-linear model {#nlm}


Let us now address the more general case of fitting a non-linear model. This model is written:
\[
Y=f(X_1,\ \ldots,\ X_p;\theta)+\varepsilon
\]
where $Y$ is the response variable, $X_1$, $\ldots$, $X_p$ are the effect variables, $\theta$ is the vector of all the model coefficients, $\varepsilon$ is the residual error, and $f$ is a function. If $f$ is linear in relation to the coefficients $\theta$, this brings us back to the previously studied linear model. We will henceforth no longer make any a priori hypotheses concerning the linearity of $f$ in relation to coefficients $\theta$. As previously, we assume that the residuals are independent and that they follow a centered normal distribution. By contrast, we do not make any a priori hypothesis concerning their variance. $\mathrm{E}(\varepsilon)=0$ means that $\mathrm{E}(Y)=f(X_1,\ \ldots,\ X_p;\theta)$. This is why we can say that $f$ defines the mean model (i.e. for $Y$). Let us write:
\[
\mathrm{Var}(\varepsilon)=g(X_1,\ \ldots,\ X_p;\vartheta)
\]
where $g$ is a function and $\vartheta$ a set of parameters. As $\mathrm{Var}(Y)=\mathrm{Var}(\varepsilon)$, we can say that $g$ defines the variance model. Function $g$ may take various forms, but for biomass or volume data it is generally a power function of a variable that characterizes tree size (typically dbh). Without loss of generality, we can put forward that this effect variable is $X_1$, and therefore:
\[
g(X_1,\ \ldots,\ X_p;\vartheta)\equiv(kX_1^c)^2
\]
where $\vartheta\equiv(k,\ c)$, $k>0$ and $c\geq0$.

Interpreting the results of fitting a non-linear model is fundamentally the same as for the linear model. The difference between the linear model and the non-linear model, in addition to their properties, lies in the manner by which model coefficients are estimated. Two particular approaches are used: (*i*) exponent $c$ is fixed in advance; (*ii*) exponent $c$ is a parameter to be estimated in the same manner as the model's other parameters.



### Exponent known

Let us first consider the case where the exponent $c$ of the variance model is known in advance. Here, the least squares method can again be used to fit the model. The weighted sum of squares corresponds to:
\[
\mathrm{SSE}(\theta)=\sum_{i=1}^nw_i\ \varepsilon_i^2
=\sum_{i=1}^nw_i\ [Y_i-f(X_{i1},\ \ldots,\ X_{ip};\theta)]^2
\]
where the weights are inversely proportional to the variance of the residuals:
\[
w_i=\frac{1}{X_{i1}^{2c}}\propto\frac{1}{\mathrm{Var}(\varepsilon_i)}
\]
As previously, the estimator of the model's coefficients corresponds to the value of $\theta$ that minimizes the weighted sum of squares:
\[
\hat{\theta}=\arg\min_{\theta}\mathrm{SSE}(\theta)
=\arg\min_{\theta}\bigg\{\sum_{i=1}^n\frac{1}{X_{i1}^{2c}}
[Y_i-f(X_{i1},\ \ldots,\ X_{ip};\theta)]^2\bigg\}
\]
In the particular case where the residuals have a constant variance (i.e. $c=0$), the weighted least squares method is simplified to the ordinary least squares method (all weights $w_i$ are 1), but the principle behind the calculations remains the same. The estimator $\theta$ is obtained by resolving
\begin{equation}
\frac{\partial\mathrm{SSE}}{\partial\theta}(\hat{\theta})=0
(\#eq:der)
\end{equation}
with the constraint $(\partial^2\mathrm{SSE}/\partial\theta^2)>0$ to ensure that this is indeed a minimum, not a maximum. In the previous case of the linear model, resolving \@ref(eq:der) yielded an explicit expression for the estimator $\hat{\theta}$. This is not the case for the general case of the non-linear model: there is no explicit expression for $\hat{\theta}$. The sum of squares must therefore be minimized using a numerical algorithm. We will examine this point in depth in section \@ref(algo).


#### *A priori* value for the exponent {-}

The *a priori* value of exponent $c$ is obtained in the non-linear case in the same manner as for the linear case `r ifelse(book_format == "latex", "(see page \\pageref{chx})", "")`: by trial and error, by dividing $X_1$ into classes and estimating the variance of $Y$ for each class, or by minimizing Furnival's index `r ifelse(book_format == "latex", "(see p.\\pageref{furni})", "")`.


::::::{.filrouge data-latex=""}

(@eq-fnlsD)

:::{.exercise #fnlsD}
(ref:fnlsD)
:::

The graphical exploration (red lines \@ref(exr:feBvD) and \@ref(exr:feln)) showed that the relation between biomass $B$ and dbh $D$ was of the power type, with the variance of the biomass increasing with dbh:
\[
B=aD^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]
We saw in red line \@ref(exr:frlpD2H) that the conditional standard deviation of the biomass derived from the dbh was proportional to the square of the dbh: $c=2$. We can therefore fit a non-linear regression by the weighted least squares method using a weighting that is inversely proportional to $D^4$:

```{r, echo=T, results='hide'} 
start <- coef(lm(log(Btot) ~ I(log(dbh)), data =  dat[dat$Btot > 0,]))
start[1] <- exp(start[1])
names(start) <- c("a", "b")
m <- nls(
  formula = Btot ~ a * dbh^b, 
  data = dat, 
  start = start, 
  weights = 1 / dat$dbh^4
  )
summary(m)
```

The non-linear regression is fitted using the `nls` command which calls on the start values of the coefficients. These start values are contained in the `start` object and are calculated by re-transforming the coefficients of the linear regression on the log-transformed data. Fitting the non-linear regression by the weighted least squares method gives:

```{r}
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

with a residual standard deviation $k=$ ``r print(summary(m)$sigma, digits = 4)` tonnes\ cm^-2^. The model is therefore written: $B=2.492\times10^{-4}D^{2.346}$. Let us now return to the linear regression fitted to log-transformed data (red line \@ref(exr:rllnBvD)) which was written: $\ln(B)=-8.42722+2.36104\ln(D)$. If we return naively to the starting data by applying the exponential function (we will see in `r ifelse(book_format == "latex", "\\S", "&sect;")`\ \@ref(invtra) why this is naive), the model becomes: $B=\exp(-8.42722)\times D^{2.36104}=2.188\times10^{-4}D^{2.36104}$. The model fitted by non-linear regression and the model fitted by linear regression on log-transformed data are therefore very similar.

::::::


::::::{.filrouge data-latex=""}

(@eq-pfnlsD2H)

:::{.exercise #fnlsD2H}
(ref:fnlsD2H)
:::

We have already fitted a power model $B=a(D^2H)^b$ by simple linear regression on log-transformed data (red line \@ref(exr:rllnBvD2H)). Let us now fit this model directly by non-linear regression:
\[
B=a(D^2H)^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]
In order to take account of the heteroscedasticity, and considering that the conditional standard deviation of the biomass derived from the diameter is proportional to $D^2$ (red line \@ref(exr:frlpD2H)), we can fit this non-linear model by the weighted least squares method using a weighting inversely proportional to $D^4$:

```{r, echo=T, results='hide'}
start <- coef(lm(
  formula = log(Btot) ~ I(log(dbh^2 * heig)),
  data = dat[dat$Btot > 0,]
  ))
start[1] <- exp(start[1])
names(start) <- c("a", "b")
m <- nls(
  formula = Btot ~ a*(dbh^2 * heig)^b, 
  data = dat, 
  start = start, 
  weights = 1 / dat$dbh^4
  )
summary(m)
```

As previously (red line \@ref(exr:fnlsD)), the `nls` command calls on start values for the coefficients and these are obtained from the coefficients of the multiple regression on log-transformed data. The result of the fitting is as follows:

```{r}
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

with a residual standard deviation $k=$ `r print(summary(m)$sigma, digits = 4)` tonnes\ cm^-2^. The model is therefore written: $B=7.885\times10^{-5}(D^2H)^{0.9154}$. Let us now return to the linear regression fitted to log-transformed data (red line \@ref(exr:rllnBvD2H)), which was written: $\ln(B)=-8.99427+0.87238\ln(D^2H)$. If we return naively to the starting data by applying the exponential function, this model becomes: $B=\exp(-8.99427)\times D^{0.87238}=1.241\times10^{-4}D^{0.87238}$. The model fitted by non-linear regression and the model fitted by linear regression on log-transformed data are therefore very similar.

::::::


::::::{.filrouge data-latex=""}

(@eq-fnlsDH)

:::{.exercise #fnlsDH}
(ref:fnlsDH)
:::

We have already fitted a power model $B=aD^{b_1}H^{b_2}$ by multiple linear regression on log-transformed data (red line \@ref(exr:flnDlnH)). Let us now fit this model directly by non-linear regression:
\[
B=aD^{b_1}H^{b_2}+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]
In order to take account of the heteroscedasticity, and considering that the conditional standard deviation of the biomass derived from dbh is proportional to $D^2$ (red line \@ref(exr:frlpD2H)), we can fit this non-linear model by the weighted least squares method using a weighting inversely proportional to $D^4$:

```{r, echo=T, results='hide'}
start <- coef(lm(
  log(Btot) ~ I(log(dbh)) + I(log(heig)), data = dat[dat$Btot > 0,]
  ))
start[1] <- exp(start[1])
names(start) <- c("a", "b1", "b2")
m <- nls(
  formula = Btot ~ a * dbh^b1 * heig^b2, 
  data = dat, 
  start = start, 
  weights = 1 / dat$dbh^4
  )
summary(m)
```

As previously (red line \@ref(exr:fnlsD)), the `nls` command calls on start values for the coefficients and these are obtained from the coefficients of the multiple regression on log-transformed data. The result of the fitting is as follows:

```{r}
printCoefmat(summary(m)$coef, digits = 4, signif.stars = TRUE, signif.legend = FALSE)
```

with a residual standard deviation $k=$ `r print(summary(m)$sigma, digits = 4)` tonnes\ cm^-2^. The model is therefore written: $B=1.003\times10^{-4}D^{1.923}H^{0.7435}$. The model is similar to that fitted by multiple regression on log-transformed data (red line \@ref(exr:flnDlnH)). But coefficient $a$ is estimated with less precision here than by the multiple regression on log-transformed data.

::::::



### Estimating the exponent

Let us now consider the case where the exponent $c$ needs to be estimated at the same time as the model's other parameters. This case includes the linear regression with variance model that we mentioned in section \@ref(lme). In this case the least squares method is no longer valid. We are therefore obliged to use another fitting method: the maximum likelihood method. The likelihood of an observation ($X_{i1}$, $\ldots$, $X_{ip}$, $Y_i$) is the probability density of observing ($X_{i1}$, $\ldots$, $X_{ip}$, $Y_i$) in the specified model. The probability density of a normal distribution of expectation $\mu$ and standard deviation $\sigma$ is:
\[
\phi(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{x-\mu}{\sigma}\bigg)^2\bigg]
\]
As $Y_i$ follows a normal distribution of expectation $f(X_{i1},\ \ldots,\ X_{ip};\theta)$ and standard deviation $kX_{i1}^c$, the likelihood of the $i$th observation is:
\[
\frac{1}{kX_{i1}^c\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\]
As the observations are independent, their joint likelihood is the product of the likelihoods of each observation. The likelihood of the sample of $n$ observations is therefore:
\begin{eqnarray}
\ell(\theta,\ k,\ c) &=& \prod_{i=1}^n
\frac{1}{kX_{i1}^c\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
(\#eq:vrais)
\\ &=& \frac{1}{(k\sqrt{2\pi})^n}\frac{1}{(\prod_{i=1}^nX_{i1})^c}
\exp\bigg[-\frac{1}{2}\sum_{i=1}^n\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\nonumber
\end{eqnarray}
This likelihood is considered to be a function of parameters $\theta$, $k$ and $c$.

The better the values of parameters $\theta$, $k$ and $c$ the higher the probability of the observations being obtained in the model corresponding to these parameter values. In other words, the best values for parameters $\theta$, $k$ and $c$ are those that maximize the likelihood of the observations. The corresponding estimator is by definition the maximum likelihood estimator, and is written:
\[
(\hat{\theta},\ \hat{k},\ \hat{c})=\arg\max_{(\theta,\ k,\ c)}\;
\ell(\theta,\ k,\ c)=\arg\max_{(\theta,\ k,\ c)}\;
\ln[\ell(\theta,\ k,\ c)]
\]
where the last equality stems from the fact that a function and its logarithm reach their maximum for the same values of their argument. The logarithm of the likelihood, which we call the log-likelihood and which we write $\mathcal{L}$, is easier to calculate than the likelihood, and therefore, for our calculations, it is the log-likelihood we will be seeking to maximize. In the present case, the log-likelihood is written:
\begin{eqnarray}
\mathcal{L}(\theta,\ k,\ c) &=&
\ln[\ell(\theta,\ k,\ c)]
(\#eq:ll)
\\ &=& -n\ln(k\sqrt{2\pi})-c\sum_{i=1}^n\ln(X_{i1})-\frac{1}{2}\sum_{i=1}^n\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\nonumber
\\ &=& -\frac{1}{2}\sum_{i=1}^n\bigg[\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2
+\ln(2\pi)+\ln(k^2X_i^{2c})\bigg]\nonumber
\end{eqnarray}
To obtain the maximum likelihood estimators of the parameters, we would need to calculate the partial derivatives of the log-likelihood with respect to these parameters, and look for the values where they cancel each other out (while ensuring that the second derivatives are indeed negative). In the general case, there is no analytical solution to this problem. In the same manner as previously for the sum of squares, we will need to use a numerical algorithm to maximize the log-likelihood. We can show that the maximum likelihood method yields a coefficients estimator that is asymptotically (i.e. when the number $n$ of observations tends toward infinity) the best. We can also show that in the case of the linear model, the least squares estimator and the maximum likelihood estimator are the same.


::::::{.filrouge data-latex=""}

(@eq-fnlmD)

:::{.exercise #fnlmD}
(ref:fnlmD)
:::

Let us look again at the non-linear regression between biomass and dbh (see red line \@ref(exr:fnlsD)), but this time consider that exponent $c$ in the variance model is a parameter that needs to be estimated like the others. The model is written in the same fashion as before (red line \@ref(exr:fnlsD)):
\[
B=aD^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
but is fitted by the maximum likelihood method:

```{r, echo=T, results='hide'}
start <- coef(lm(log(Btot) ~ I(log(dbh)), data = dat[dat$Btot > 0,]))
start[1] <- exp(start[1])
names(start) <- c("a", "b")
library(nlme)
m <- nlme(
  model = Btot ~ a * dbh^b, 
  data = cbind(dat, g = "a"), 
  fixed = a + b ~ 1,
  start = start, 
  groups = ~g, 
  weights = varPower(form = ~dbh)
  )
summary(m)
```

The model is fitted by the `nlme` command^[The `nlme` command in fact serves to fit mixed effect non-linear models. The `nlreg` command is used to fit non-linear models with variance model, but we have obtained abnormal results with this command (version 3.1-96) which explains why we prefer to use the `nlme` command here, even though there is no mixed effect in the models considered here.] which, like the `nls` command (red line \@ref(exr:fnlsD)), requires `start` values for the coefficients. These start values are calculated as in red line \@ref(exr:fnlsD). The result of the fitting is as follows:

```{r}
summary(m)$tTable
```

with an estimated value of exponent $c=2.090814$. This estimated value is very similar to that evaluated for the weighted non-linear regression ($c=2$, see in red line \@ref(exr:frlpD2H)). The fitted model is therefore: $B=2.445\times10^{-4}D^{2.35105}$, which is very similar to the model fitted by weighted non-linear regression (red line \@ref(exr:fnlsD)).

::::::


::::::{.filrouge data-latex=""}

(@eq-fnlmD2H)

:::{.exercise #fnlmD2H}
(ref:fnlmD2H)
:::

Let us look again at the non-linear regression between biomass and $D^2H$ (see red line \@ref(exr:fnlsD2H)), but this time consider that exponent $c$ in the variance model is a parameter that needs to be estimated like the others. The model is written in the same fashion as before (red line \@ref(exr:fnlsD2H)):
\[
B=a(D^2H)^b+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
but is fitted by the maximum likelihood method:

```{r, echo=T, results='hide'}
start <- coef(lm(
  formula = log(Btot) ~ I(log(dbh^2 * heig)), 
  data = dat[dat$Btot > 0,]
  ))
start[1] <- exp(start[1])
names(start) <- c("a", "b")

library(nlme)
m <- nlme(
  model   = Btot ~ a * (dbh^2 * heig)^b, 
  data    =  cbind(dat, g = "a"),
  fixed   = a + b ~ 1, 
  start   = start, 
  groups  = ~g, 
  weights = varPower(form = ~dbh)
  )
summary(m)
```

The model is fitted by the `nlme` command, which, like the `nls` command (red line \@ref(exr:fnlsD)), requires start values for the coefficients. These `start` values are calculated as in red line \@ref(exr:fnlsD). The result of the fitting is as follows:

```{r}
summary(m)$tTable
```

with an estimated value of exponent $c=2.042586$. This estimated value is very similar to that evaluated for the weighted non-linear regression ($c=2$, see red line \@ref(exr:frlpD2H)). The fitted model is therefore: $B=8.19\times10^{-5}(D^2H)^{0.9122144}$, which is very similar to the model fitted by weighted non-linear regression (red line \@ref(exr:fnlsD2H)).

::::::


::::::{.filrouge data-latex=""}

(@eq-fnlmDH)

:::{.exercise #fnlmDH}
(ref:fnlmDH)
:::

Let us look again at the non-linear regression between biomass, dbh and height (see red line \@ref(exr:fnlsDH)):
\[
B=aD^{b_1}H^{b_2}+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
but this time consider that exponent $c$ in the variance model is a parameter that needs to be estimated like the others. The fitting by maximum likelihood is as follows:

```{r, echo=T, results='hide'}
library(nlme)
start <- coef(lm(
  log(Btot) ~ I(log(dbh)) + I(log(heig)), data = dat[dat$Btot > 0,]
  ))
start[1] <- exp(start[1])
names(start) <- c("a", "b1", "b2")
m <- nlme(
  model   = Btot ~ a * dbh^b1 * heig^b2, 
  data    = cbind(dat, g = "a"), 
  fixed   = a + b1 + b2 ~ 1,
  start   = start, 
  groups  = ~g, 
  weights = varPower(form = ~dbh)
  )
summary(m)
```

and, as previously, requires the provision of `start` values for the coefficients. The fitting yields:

```{r}
summary(m)$tTable
```

with an estimated value of exponent $c=$ `r summary(m)$modelStruct$varStruct[1]`. This estimated value is very similar to that evaluated for the weighted non-linear regression ($c=2$, see red line \@ref(exr:frlpD2H)). The fitted model is therefore: $B=1.109\times10^{-4}D^{1.9434876}H^{0.6926256}$, which is very similar to the model fitted by weighted non-linear regression (red line \@ref(exr:fnlsDH)).

::::::


::::::{.filrouge data-latex=""}

(@eq-fexpDpol)

:::{.exercise #fexpDpol}
(ref:fexpDpol)
:::

Previously (red line \@ref(exr:flnDpol)), we used multiple regression to fit a model between $\ln(B)$ and a polynomial of $\ln(D)$. If we look again at the start variable, the model is written:
\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+\ldots+a_p[\ln(D)]^p\}+\varepsilon
\]
where
\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]
We will now directly fit this non-linear model by maximum likelihood (such that exponent $c$ is estimated at the same time as the model's other parameters). For a third-order polynomial, the fitting is obtained by:

```{r, echo=T, results='hide'}
library(nlme)
start <- coef(lm(
  log(Btot) ~ I(log(dbh)) + I(log(dbh)^2) + I(log(dbh)^3), 
  data = dat[dat$Btot > 0,]
  ))
start[1] <- exp(start[1])
names(start) <- paste("a", 0:3, sep = "")
m <- nlme(
  model = Btot ~ exp(a0 + a1 * log(dbh) + a2 * log(dbh)^2 + a3 * log(dbh)^3),
  data = cbind(dat, g = "a"), 
  fixed = a0 + a1 + a2 + a3 ~ 1,
  start = start, 
  groups = ~g, 
  weights = varPower(form = ~dbh)
  )
summary(m)
```

and results in:

```{r}
summary(m)$tTable
```

with an estimated value of exponent $c=$ `r summary(m)$modelStruct$varStruct[1]`. This result is very similar to that obtained by multiple regression on log-transformed data (red line \@ref(exr:flnDpol)).

::::::



### Numerical optimization {#algo}

If, in the case of a non-linear model, we are looking to minimize the sum of squares (when exponent $c$ is known) or maximize the log-likelihood (when exponent $c$ needs to be estimated) we need to use a numerical optimization algorithm. As maximizing the log-likelihood is equivalent to minimizing the opposite of the log-likelihood, we shall, in all that follows, consider only the problem of minimizing a function in a multidimensional space. A multitude of optimization algorithms have been developed [@press07, chapitre 10] but the aim here is not to present a review of them all. What simply needs to be recognized at this stage is that these algorithms are iterative and require parameter start values. Based on this start value and each iteration, the algorithm shifts in the parameter space while looking to minimize the target function (i.e. the sum of squares or minus the log-likelihood). The target function may be represented as a hypersurface in the parameter space (Figure \@ref(fig:sp)). Each position in this space corresponds to a value of the parameters. A lump in this surface corresponds to a local maximum of the target function, whereas a dip in the surface corresponds to a local minimum. The aim is to determine the overall minimum, i.e. the deepest dip. The position of this dip corresponds to the estimated value of the parameters. If the algorithm gives the position of a dip that is not the deepest, the estimation of the parameters is false.

```{r sp, out.height='50%', out.width="60%", out.extra="", fig.show='hold', fig.scap="Representation of the target function", fig.cap="(ref:sp)", fig.subcap=c("",""), fig.ncol=1}

knitr::include_graphics(c(paste0(fig_path, "/numopt1.png"), paste0(fig_path, "/numopt2.png")), auto_pdf = TRUE)

```


#### Descending algorithm {-}

The simplest optimization algorithm consists in calculating successive positions, i.e. successive parameter values, by descending the surface defined by the target function along the line that has the steepest slope (Figure \@ref(fig:sp)A). This algorithm leads to one dip in the surface, but nothing says that this dip is the deepest for the surface may have several watersheds with several dips. Depending on the starting position, the algorithm will converge toward one dip or another (Figure \@ref(fig:sp)B). Also, even two starting positions very close together may be located on either side of a crest separating the two watersheds, and will therefore result in different dips, i.e. in different parameter estimations. The only case where this algorithm gives the correct parameter estimation regardless of starting value is when the surface has only a single dip, i.e. when the target function is convex. This in particular is the case for the linear model, but generally not for the non-linear model.


#### Improving algorithms in the case of local minima {-}

More subtle algorithms have been developed than those descending along the steepest slope. For example, these may include the possibility to climb out of a dip into which the algorithm has temporarily converged in order to determine whether there might not be a deeper dip in the neighborhood. But no algorithm, even the most subtle, offers the certainty that it has converged to the deepest dip. Thus, any numerical optimization algorithm (*i*) may be trapped by a local minimum instead of converging to the overall minimum, and (*ii*) is sensitive to the indicated start position, which in part determines the final position toward which the algorithm will converge.

If we now return to the issue at hand, this means (*i*) fitting a non-linear model could yield erroneous parameter estimations, and (*ii*) selecting parameter start values for the algorithm is a sensitive issue. Herein lies the main drawback of fitting a non-linear model, and if it is to be avoided, the parameter start values must be carefully selected and, above all, several values must be tested.


#### Selecting parameter start values {-}

When mean model $f$ can be transformed into a linear relation between the response variable $Y$ and the effect variables $X_1$, $\ldots$, $X_p$, a start value for the coefficients may be obtained by fitting a linear regression on the transformed variables without considering the possible heteroscedasticity of the residuals. Let us for example consider a power type biomass table:
\begin{equation}
B=aD^{b_1}H^{b_2}\rho^{b_3}+\varepsilon(\#eq:powea)
\end{equation}
where
\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\;kD^c)
\]
The power model for the expectation of $B$ can be rendered linear by log-transforming the variables: $\ln(B)=a'+b_1\ln(D)+b_2\ln(H)+b_3\ln(\rho)$. But this transformation is incompatible with the additivity of the errors in the model \@ref(eq:powea). In other words, the multiple regression of the response variable $\ln(B)$ against the effect variables $\ln(D)$, $\ln(H)$ and $\ln(\rho)$:
\begin{equation}
\ln(B)=a'+b_1\ln(D)+b_2\ln(H)+b_3\ln(\rho)+\varepsilon'(\#eq:poweb)
\end{equation}
where $\varepsilon'\sim\mathcal{N}(0,\ \sigma)$, is not a model equivalent to \@ref(eq:powea), even when the residuals $\varepsilon'$ of this model have a constant variance. Even though models \@ref(eq:powea) and \@ref(eq:poweb) are not mathematically equivalent, the coefficients of \@ref(eq:poweb) --- estimated by multiple regression --- may serve as start values for the numerical algorithm that estimates the coefficients of \@ref(eq:powea). If we write as $x^{(0)}$ the start value of parameter $x$ for the numerical optimization algorithm, we thus obtain:
\[
a^{(0)}=\exp(\hat{a}'),\quad b_i^{(0)}=\hat{b}_i,\quad
k^{(0)}=\hat{\sigma},\quad c^{(0)}=0
\]
Sometimes, the mean model cannot be rendered linear. An example of this is the following biomass table used for trees in a plantation [@saintandre05]:
\[
B=a+[b_0+b_1T+b_2\exp(-b_3T)]D^2H+\varepsilon
\]
where $T$ is the age of the plantation and $\varepsilon\sim\mathcal{N}(0,\ kD^c)$, which has a mean model that cannot be rendered linear. In this case, parameter start values must be selected empirically. For this current example we could for instance take:
\[
a^{(0)}=\hat{a},\quad b_0^{(0)}+b_2^{(0)}=\hat{b}_0,\quad
b_1^{(0)}=\hat{b}_1,\quad b_3^{(0)}=0,\quad
k^{(0)}=\hat{\sigma},\quad c^{(0)}=0
\]
where $\hat{a}$, $\hat{b}_0$, $\hat{b}_1$ and $\hat{\sigma}$ are estimated values for the coefficients and residual standard deviations of the multiple regression of $B$ against $D^2H$ and $D^2HT$.
Selecting parameter start values does not mean that several start values should not be tested. Therefore, when fitting a non-linear model with a numerical optimization algorithm, it is essential to test several parameter start values to ensure that the estimations are stable.


